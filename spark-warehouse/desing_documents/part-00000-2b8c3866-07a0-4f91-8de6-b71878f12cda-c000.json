{"_c1":"Add Hadoop native library to java library path  compression","document":"Add Hadoop native library to java library path  compression","words":["add","hadoop","native","library","to","java","library","path","","compression"],"filtered":["add","hadoop","native","library","java","library","path","","compression"],"features":{"type":0,"size":1000,"indices":[181,187,372,388,432,446,668,812,967],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Add Java API for trackStateByKey","document":"Add Java API for trackStateByKey","words":["add","java","api","for","trackstatebykey"],"filtered":["add","java","api","trackstatebykey"],"features":{"type":0,"size":1000,"indices":[36,415,432,644,967],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Add R API for stddev variance","document":"Add R API for stddev variance","words":["add","r","api","for","stddev","variance"],"filtered":["add","r","api","stddev","variance"],"features":{"type":0,"size":1000,"indices":[36,113,432,570,644,939],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Break SQLQuerySuite out into smaller test suites","document":"Break SQLQuerySuite out into smaller test suites","words":["break","sqlquerysuite","out","into","smaller","test","suites"],"filtered":["break","sqlquerysuite","smaller","test","suites"],"features":{"type":0,"size":1000,"indices":[301,586,648,654,891,931,974],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Change numPartitions   in RDD to be  getNumPartitions  to be consistent with pyspark scala","document":"Change numPartitions   in RDD to be  getNumPartitions  to be consistent with pyspark scala","words":["change","numpartitions","","","in","rdd","to","be","","getnumpartitions","","to","be","consistent","with","pyspark","scala"],"filtered":["change","numpartitions","","","rdd","","getnumpartitions","","consistent","pyspark","scala"],"features":{"type":0,"size":1000,"indices":[64,158,372,388,433,445,490,509,650,656,870,954],"values":[1.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c1":"RPC Layer improvements to support protocol compatibility","document":"RPC Layer improvements to support protocol compatibility","words":["rpc","layer","improvements","to","support","protocol","compatibility"],"filtered":["rpc","layer","improvements","support","protocol","compatibility"],"features":{"type":0,"size":1000,"indices":[21,181,388,695,722,780,834],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Refactor R mllib for easier ml implementations","document":"Refactor R mllib for easier ml implementations","words":["refactor","r","mllib","for","easier","ml","implementations"],"filtered":["refactor","r","mllib","easier","ml","implementations"],"features":{"type":0,"size":1000,"indices":[36,324,474,521,570,623,925],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove OpenHashSet for the old aggregate","document":"Remove OpenHashSet for the old aggregate","words":["remove","openhashset","for","the","old","aggregate"],"filtered":["remove","openhashset","old","aggregate"],"features":{"type":0,"size":1000,"indices":[36,288,420,438,672,710],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove UnsafeRowConverter in favor of UnsafeProjection","document":"Remove UnsafeRowConverter in favor of UnsafeProjection","words":["remove","unsaferowconverter","in","favor","of","unsafeprojection"],"filtered":["remove","unsaferowconverter","favor","unsafeprojection"],"features":{"type":0,"size":1000,"indices":[30,236,288,343,445,631],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove implicit conversion from Expression to Column","document":"Remove implicit conversion from Expression to Column","words":["remove","implicit","conversion","from","expression","to","column"],"filtered":["remove","implicit","conversion","expression","column"],"features":{"type":0,"size":1000,"indices":[288,388,420,577,601,804,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove unneeded staging repositories from build","document":"Remove unneeded staging repositories from build","words":["remove","unneeded","staging","repositories","from","build"],"filtered":["remove","unneeded","staging","repositories","build"],"features":{"type":0,"size":1000,"indices":[288,520,536,807,872,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Removed calling size  length in while condition to avoid extra JVM call","document":"Removed calling size  length in while condition to avoid extra JVM call","words":["removed","calling","size","","length","in","while","condition","to","avoid","extra","jvm","call"],"filtered":["removed","calling","size","","length","condition","avoid","extra","jvm","call"],"features":{"type":0,"size":1000,"indices":[8,109,146,192,300,372,388,389,445,512,583,707,813],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Removed diffSum which is theoretical zero in LinearRegression and coding formating","document":"Removed diffSum which is theoretical zero in LinearRegression and coding formating","words":["removed","diffsum","which","is","theoretical","zero","in","linearregression","and","coding","formating"],"filtered":["removed","diffsum","theoretical","zero","linearregression","coding","formating"],"features":{"type":0,"size":1000,"indices":[234,281,333,445,512,575,597,637,837,863,927],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Rename upstreams      inputRDDs   in WholeStageCodegen","document":"Rename upstreams      inputRDDs   in WholeStageCodegen","words":["rename","upstreams","","","","","","inputrdds","","","in","wholestagecodegen"],"filtered":["rename","upstreams","","","","","","inputrdds","","","wholestagecodegen"],"features":{"type":0,"size":1000,"indices":[248,340,372,445,867,946],"values":[1.0,1.0,7.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c1":"Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver","document":"Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver","words":["replace","shufflemanagerclass","with","shortshufflemgrnames","in","externalshuffleblockresolver"],"filtered":["replace","shufflemanagerclass","shortshufflemgrnames","externalshuffleblockresolver"],"features":{"type":0,"size":1000,"indices":[32,72,445,650,787,907],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support UnsafeRow in Coalesce Except Intersect","document":"Support UnsafeRow in Coalesce Except Intersect","words":["support","unsaferow","in","coalesce","except","intersect"],"filtered":["support","unsaferow","coalesce","except","intersect"],"features":{"type":0,"size":1000,"indices":[175,179,373,445,513,695],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support UnsafeRow in LocalTableScan","document":"Support UnsafeRow in LocalTableScan","words":["support","unsaferow","in","localtablescan"],"filtered":["support","unsaferow","localtablescan"],"features":{"type":0,"size":1000,"indices":[175,445,494,695],"values":[1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support UnsafeRow in MapPartitions MapGroups CoGroup","document":"Support UnsafeRow in MapPartitions MapGroups CoGroup","words":["support","unsaferow","in","mappartitions","mapgroups","cogroup"],"filtered":["support","unsaferow","mappartitions","mapgroups","cogroup"],"features":{"type":0,"size":1000,"indices":[155,175,445,553,695,997],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support window functions in SQLContext","document":"Support window functions in SQLContext","words":["support","window","functions","in","sqlcontext"],"filtered":["support","window","functions","sqlcontext"],"features":{"type":0,"size":1000,"indices":[445,451,511,587,695],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Update maven enforcer plugin version to","document":"Update maven enforcer plugin version to","words":["update","maven","enforcer","plugin","version","to"],"filtered":["update","maven","enforcer","plugin","version"],"features":{"type":0,"size":1000,"indices":[139,343,388,562,980,995],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"consolidate  ExpressionEncoder tuple  and  Encoders tuple","document":"consolidate  ExpressionEncoder tuple  and  Encoders tuple","words":["consolidate","","expressionencoder","tuple","","and","","encoders","tuple"],"filtered":["consolidate","","expressionencoder","tuple","","","encoders","tuple"],"features":{"type":0,"size":1000,"indices":[333,367,372,462,482,903],"values":[1.0,2.0,3.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"create a script to setup application in order to create root directories for application such hbase  hcat  hive etc","document":"create a script to setup application in order to create root directories for application such hbase  hcat  hive etc","words":["create","a","script","to","setup","application","in","order","to","create","root","directories","for","application","such","hbase","","hcat","","hive","etc"],"filtered":["create","script","setup","application","order","create","root","directories","application","hbase","","hcat","","hive","etc"],"features":{"type":0,"size":1000,"indices":[29,36,101,169,170,265,272,337,346,350,372,388,445,599,669,718,820],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"make SubqueryHolder an inner class","document":"make SubqueryHolder an inner class","words":["make","subqueryholder","an","inner","class"],"filtered":["make","subqueryholder","inner","class"],"features":{"type":0,"size":1000,"indices":[101,525,534,752,800],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"move BucketSpec to catalyst module and use it in CatalogTable","document":"move BucketSpec to catalyst module and use it in CatalogTable","words":["move","bucketspec","to","catalyst","module","and","use","it","in","catalogtable"],"filtered":["move","bucketspec","catalyst","module","use","catalogtable"],"features":{"type":0,"size":1000,"indices":[82,282,299,333,375,388,445,489,495,651],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"refactor object operator framework to make it easy to eliminate serializations","document":"refactor object operator framework to make it easy to eliminate serializations","words":["refactor","object","operator","framework","to","make","it","easy","to","eliminate","serializations"],"filtered":["refactor","object","operator","framework","make","easy","eliminate","serializations"],"features":{"type":0,"size":1000,"indices":[199,245,388,486,495,525,599,623,650,760],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"remove GenericInternalRowWithSchema","document":"remove GenericInternalRowWithSchema","words":["remove","genericinternalrowwithschema"],"filtered":["remove","genericinternalrowwithschema"],"features":{"type":0,"size":1000,"indices":[288,861],"values":[1.0,1.0]},"cluster_label":13}
{"_c1":"remove OverwriteOptions","document":"remove OverwriteOptions","words":["remove","overwriteoptions"],"filtered":["remove","overwriteoptions"],"features":{"type":0,"size":1000,"indices":[210,288],"values":[1.0,1.0]},"cluster_label":13}
{"_c1":"remove catalog table type INDEX","document":"remove catalog table type INDEX","words":["remove","catalog","table","type","index"],"filtered":["remove","catalog","table","type","index"],"features":{"type":0,"size":1000,"indices":[288,307,510,526,837],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"remove the createCode and createStructCode  and replace the usage of them by createStructCode","document":"remove the createCode and createStructCode  and replace the usage of them by createStructCode","words":["remove","the","createcode","and","createstructcode","","and","replace","the","usage","of","them","by","createstructcode"],"filtered":["remove","createcode","createstructcode","","replace","usage","createstructcode"],"features":{"type":0,"size":1000,"indices":[122,154,223,288,333,343,372,710,787,889,924],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c1":"remove the supportsPartial flag in AggregateFunction","document":"remove the supportsPartial flag in AggregateFunction","words":["remove","the","supportspartial","flag","in","aggregatefunction"],"filtered":["remove","supportspartial","flag","aggregatefunction"],"features":{"type":0,"size":1000,"indices":[288,390,407,445,710,932],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"update InternalRow toSeq to make it accept data type info","document":"update InternalRow toSeq to make it accept data type info","words":["update","internalrow","toseq","to","make","it","accept","data","type","info"],"filtered":["update","internalrow","toseq","make","accept","data","type","info"],"features":{"type":0,"size":1000,"indices":[263,343,388,495,525,526,549,695,846,980],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"use StructType in CatalogTable and remove CatalogColumn","document":"use StructType in CatalogTable and remove CatalogColumn","words":["use","structtype","in","catalogtable","and","remove","catalogcolumn"],"filtered":["use","structtype","catalogtable","remove","catalogcolumn"],"features":{"type":0,"size":1000,"indices":[22,288,333,375,376,445,489],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"A majority of Spark SQL queries likely run though   HadoopFSRelation    however there are currently several complexity and performance problems with this path","_c1":"Simplify and Speedup HadoopFSRelation","document":"A majority of Spark SQL queries likely run though   HadoopFSRelation    however there are currently several complexity and performance problems with this path Simplify and Speedup HadoopFSRelation","words":["a","majority","of","spark","sql","queries","likely","run","though","","","hadoopfsrelation","","","","however","there","are","currently","several","complexity","and","performance","problems","with","this","path","simplify","and","speedup","hadoopfsrelation"],"filtered":["majority","spark","sql","queries","likely","run","though","","","hadoopfsrelation","","","","however","currently","several","complexity","performance","problems","path","simplify","speedup","hadoopfsrelation"],"features":{"type":0,"size":1000,"indices":[19,91,105,107,138,170,244,333,343,364,372,373,530,547,579,650,668,673,686,703,759,763,831,870,997],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"A small performance optimization    we don t need to generate a Tuple  and then immediately discard the key  We also don t need an extra wrapper","_c1":"Remove SqlNewHadoopRDD s generated Tuple  and InterruptibleIterator","document":"A small performance optimization    we don t need to generate a Tuple  and then immediately discard the key  We also don t need an extra wrapper Remove SqlNewHadoopRDD s generated Tuple  and InterruptibleIterator","words":["a","small","performance","optimization","","","","we","don","t","need","to","generate","a","tuple","","and","then","immediately","discard","the","key","","we","also","don","t","need","an","extra","wrapper","remove","sqlnewhadooprdd","s","generated","tuple","","and","interruptibleiterator"],"filtered":["small","performance","optimization","","","","need","generate","tuple","","immediately","discard","key","","also","need","extra","wrapper","remove","sqlnewhadooprdd","generated","tuple","","interruptibleiterator"],"features":{"type":0,"size":1000,"indices":[8,170,197,204,227,271,288,315,333,355,367,372,381,388,537,689,710,742,752,759,762,777,783,792,830,891,993],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,6.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"A specific sub case of the general priority inversion problem noted in HADOOP      is when many lower priority jobs are submitted and are waiting for mappers to free up  Even though they haven t actually done any work  they will be assigned any free reducers  If a higher priority job is submitted  priority inversion results not just due to the lower priority tasks that are in the midst of completing  but also due to the ones that haven t yet started but have claimed all the free reducers  A simple workaround is to require a job to complete some useful work before assigning it a reducer  This can be done in a tunable and backwards compatible manner by adding a  minimum map progress percentage before assigning a reducer  option to the JobConf  Setting this to   would eliminate the common case above  and setting it to     would technically eliminate the inversion of HADOOP       though likely at an unacceptably high cost","_c1":"JobConf option for minimum progress threshold before reducers are assigned","document":"A specific sub case of the general priority inversion problem noted in HADOOP      is when many lower priority jobs are submitted and are waiting for mappers to free up  Even though they haven t actually done any work  they will be assigned any free reducers  If a higher priority job is submitted  priority inversion results not just due to the lower priority tasks that are in the midst of completing  but also due to the ones that haven t yet started but have claimed all the free reducers  A simple workaround is to require a job to complete some useful work before assigning it a reducer  This can be done in a tunable and backwards compatible manner by adding a  minimum map progress percentage before assigning a reducer  option to the JobConf  Setting this to   would eliminate the common case above  and setting it to     would technically eliminate the inversion of HADOOP       though likely at an unacceptably high cost JobConf option for minimum progress threshold before reducers are assigned","words":["a","specific","sub","case","of","the","general","priority","inversion","problem","noted","in","hadoop","","","","","","is","when","many","lower","priority","jobs","are","submitted","and","are","waiting","for","mappers","to","free","up","","even","though","they","haven","t","actually","done","any","work","","they","will","be","assigned","any","free","reducers","","if","a","higher","priority","job","is","submitted","","priority","inversion","results","not","just","due","to","the","lower","priority","tasks","that","are","in","the","midst","of","completing","","but","also","due","to","the","ones","that","haven","t","yet","started","but","have","claimed","all","the","free","reducers","","a","simple","workaround","is","to","require","a","job","to","complete","some","useful","work","before","assigning","it","a","reducer","","this","can","be","done","in","a","tunable","and","backwards","compatible","manner","by","adding","a","","minimum","map","progress","percentage","before","assigning","a","reducer","","option","to","the","jobconf","","setting","this","to","","","would","eliminate","the","common","case","above","","and","setting","it","to","","","","","would","technically","eliminate","the","inversion","of","hadoop","","","","","","","though","likely","at","an","unacceptably","high","cost","jobconf","option","for","minimum","progress","threshold","before","reducers","are","assigned"],"filtered":["specific","sub","case","general","priority","inversion","problem","noted","hadoop","","","","","","many","lower","priority","jobs","submitted","waiting","mappers","free","","even","though","haven","actually","done","work","","assigned","free","reducers","","higher","priority","job","submitted","","priority","inversion","results","due","lower","priority","tasks","midst","completing","","also","due","ones","haven","yet","started","claimed","free","reducers","","simple","workaround","require","job","complete","useful","work","assigning","reducer","","done","tunable","backwards","compatible","manner","adding","","minimum","map","progress","percentage","assigning","reducer","","option","jobconf","","setting","","","eliminate","common","case","","setting","","","","","technically","eliminate","inversion","hadoop","","","","","","","though","likely","unacceptably","high","cost","jobconf","option","minimum","progress","threshold","reducers","assigned"],"features":{"type":0,"size":1000,"indices":[0,18,36,48,51,59,73,76,83,91,101,110,116,128,133,138,147,159,163,170,181,188,192,195,202,209,213,221,222,223,231,245,255,258,272,273,281,299,307,332,333,342,343,344,348,349,356,359,363,372,373,388,400,406,415,420,445,447,470,476,490,495,505,513,527,556,567,573,579,586,602,607,623,656,659,698,707,710,712,752,756,760,777,792,833,855,893,911,954,964,968,980,985,992,997],"values":[5.0,1.0,2.0,2.0,1.0,2.0,3.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,2.0,9.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,28.0,2.0,8.0,1.0,1.0,3.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,8.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":10}
{"_c0":"Add Java friendly API for StreamingListener","_c1":"Add JavaStreamingListener","document":"Add Java friendly API for StreamingListener Add JavaStreamingListener","words":["add","java","friendly","api","for","streaminglistener","add","javastreaminglistener"],"filtered":["add","java","friendly","api","streaminglistener","add","javastreaminglistener"],"features":{"type":0,"size":1000,"indices":[36,325,432,468,644,709,967],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Add Python API  user guide and example for ml feature CountVectorizerModel","_c1":"Add Python API for ml feature CountVectorizer","document":"Add Python API  user guide and example for ml feature CountVectorizerModel Add Python API for ml feature CountVectorizer","words":["add","python","api","","user","guide","and","example","for","ml","feature","countvectorizermodel","add","python","api","for","ml","feature","countvectorizer"],"filtered":["add","python","api","","user","guide","example","ml","feature","countvectorizermodel","add","python","api","ml","feature","countvectorizer"],"features":{"type":0,"size":1000,"indices":[36,241,243,318,324,333,372,432,589,644,675,736,882],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Add Python API for mllib fpm PrefixSpan","_c1":"Add Python API for PrefixSpan","document":"Add Python API for mllib fpm PrefixSpan Add Python API for PrefixSpan","words":["add","python","api","for","mllib","fpm","prefixspan","add","python","api","for","prefixspan"],"filtered":["add","python","api","mllib","fpm","prefixspan","add","python","api","prefixspan"],"features":{"type":0,"size":1000,"indices":[36,260,432,521,589,644,845],"values":[2.0,2.0,2.0,1.0,2.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Add a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise    h   Example If   df x       is TRUE  then return    otherwise return","_c1":"Add  ifelse  Column function to SparkR","document":"Add a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise    h   Example If   df x       is TRUE  then return    otherwise return Add  ifelse  Column function to SparkR","words":["add","a","column","function","on","a","dataframe","like","","ifelse","","in","r","to","sparkr","","i","guess","we","could","implement","it","with","a","combination","with","","","when","","","and","","","otherwise","","","","h","","","example","if","","","df","x","","","","","","","is","true","","then","return","","","","otherwise","return","add","","ifelse","","column","function","to","sparkr"],"filtered":["add","column","function","dataframe","like","","ifelse","","r","sparkr","","guess","implement","combination","","","","","","","otherwise","","","","h","","","example","","","df","x","","","","","","","true","","return","","","","otherwise","return","add","","ifelse","","column","function","sparkr"],"features":{"type":0,"size":1000,"indices":[14,76,82,118,145,161,170,188,213,243,281,282,286,313,329,330,333,372,381,388,432,445,472,495,570,597,601,650,767,810,875,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,28.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":3}
{"_c0":"Add hash function for SparkR","_c1":"SparkR support hash function","document":"Add hash function for SparkR SparkR support hash function","words":["add","hash","function","for","sparkr","sparkr","support","hash","function"],"filtered":["add","hash","function","sparkr","sparkr","support","hash","function"],"features":{"type":0,"size":1000,"indices":[36,313,374,432,695,767],"values":[1.0,2.0,2.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"After HADOOP       we should remove metrics v  from trunk","_c1":"Remove metrics v","document":"After HADOOP       we should remove metrics v  from trunk Remove metrics v","words":["after","hadoop","","","","","","","we","should","remove","metrics","v","","from","trunk","remove","metrics","v"],"filtered":["hadoop","","","","","","","remove","metrics","v","","trunk","remove","metrics","v"],"features":{"type":0,"size":1000,"indices":[77,83,106,181,288,372,477,665,921,993],"values":[1.0,1.0,2.0,1.0,2.0,7.0,2.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"After SPARK        we should add Python API for AFTSurvivalRegression","_c1":"Python API for AFTSurvivalRegression","document":"After SPARK        we should add Python API for AFTSurvivalRegression Python API for AFTSurvivalRegression","words":["after","spark","","","","","","","","we","should","add","python","api","for","aftsurvivalregression","python","api","for","aftsurvivalregression"],"filtered":["spark","","","","","","","","add","python","api","aftsurvivalregression","python","api","aftsurvivalregression"],"features":{"type":0,"size":1000,"indices":[36,77,105,368,372,432,589,644,665,993],"values":[2.0,1.0,1.0,2.0,7.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"After SPARK        we should add Python API for MaxAbsScaler","_c1":"Python API for MaxAbsScaler","document":"After SPARK        we should add Python API for MaxAbsScaler Python API for MaxAbsScaler","words":["after","spark","","","","","","","","we","should","add","python","api","for","maxabsscaler","python","api","for","maxabsscaler"],"filtered":["spark","","","","","","","","add","python","api","maxabsscaler","python","api","maxabsscaler"],"features":{"type":0,"size":1000,"indices":[36,77,105,372,432,589,644,663,665,993],"values":[2.0,1.0,1.0,7.0,1.0,2.0,2.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand","_c1":"AggregateFunction should not ImplicitCastInputTypes","document":"AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand AggregateFunction should not ImplicitCastInputTypes","words":["aggregatefunction","","","currently","implements","","","implicitcastinputtypes","","","","which","enables","implicit","input","type","casting","","","this","can","lead","to","unexpected","results","","and","should","only","be","enabled","when","it","is","suitable","for","the","function","at","hand","aggregatefunction","should","not","implicitcastinputtypes"],"filtered":["aggregatefunction","","","currently","implements","","","implicitcastinputtypes","","","","enables","implicit","input","type","casting","","","lead","unexpected","results","","enabled","suitable","function","hand","aggregatefunction","implicitcastinputtypes"],"features":{"type":0,"size":1000,"indices":[0,18,36,52,76,145,281,313,333,356,363,372,373,388,390,423,455,487,495,526,577,597,656,665,710,756,763,800,833,844,889,899],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,10.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Apache Parquet       is released officially last week on    Jan  This issue aims to bump Parquet version to       since it includes many fixes  https   lists apache org thread html af c   f          a   d  ec  b bbeecaea  aa ef  f   c      Cdev parquet apache org  E","_c1":"Upgrade Parquet to","document":"Apache Parquet       is released officially last week on    Jan  This issue aims to bump Parquet version to       since it includes many fixes  https   lists apache org thread html af c   f          a   d  ec  b bbeecaea  aa ef  f   c      Cdev parquet apache org  E Upgrade Parquet to","words":["apache","parquet","","","","","","","is","released","officially","last","week","on","","","","jan","","this","issue","aims","to","bump","parquet","version","to","","","","","","","since","it","includes","many","fixes","","https","","","lists","apache","org","thread","html","af","c","","","f","","","","","","","","","","a","","","d","","ec","","b","bbeecaea","","aa","ef","","f","","","c","","","","","","cdev","parquet","apache","org","","e","upgrade","parquet","to"],"filtered":["apache","parquet","","","","","","","released","officially","last","week","","","","jan","","issue","aims","bump","parquet","version","","","","","","","since","includes","many","fixes","","https","","","lists","apache","org","thread","html","af","c","","","f","","","","","","","","","","","","d","","ec","","b","bbeecaea","","aa","ef","","f","","","c","","","","","","cdev","parquet","apache","org","","e","upgrade","parquet"],"features":{"type":0,"size":1000,"indices":[82,94,122,124,166,170,172,188,189,240,242,248,281,352,361,372,373,388,399,444,495,498,535,585,634,643,652,672,722,748,805,844,852,878,895,989,995,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,44.0,1.0,3.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"As Hadoop widespreads and matures the number of tools and utilities for users keeps growing  Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie  Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other  The lack of a common foundation creates issues for developers and users","_c1":"Common foundation for Hadoop client tools","document":"As Hadoop widespreads and matures the number of tools and utilities for users keeps growing  Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie  Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other  The lack of a common foundation creates issues for developers and users Common foundation for Hadoop client tools","words":["as","hadoop","widespreads","and","matures","the","number","of","tools","and","utilities","for","users","keeps","growing","","some","of","them","are","bundled","with","hadoop","core","","some","with","hadoop","contrib","","some","on","their","own","","some","are","full","fledged","servers","on","their","own","","for","example","","just","to","name","a","few","","distcp","","streaming","","pipes","","har","","pig","","hive","","oozie","","today","there","is","no","standard","mechanism","for","making","these","tools","available","to","users","","neither","there","is","a","standard","mechanism","for","these","tools","to","integrate","and","distributed","them","with","each","other","","the","lack","of","a","common","foundation","creates","issues","for","developers","and","users","common","foundation","for","hadoop","client","tools"],"filtered":["hadoop","widespreads","matures","number","tools","utilities","users","keeps","growing","","bundled","hadoop","core","","hadoop","contrib","","","full","fledged","servers","","example","","name","","distcp","","streaming","","pipes","","har","","pig","","hive","","oozie","","today","standard","mechanism","making","tools","available","users","","neither","standard","mechanism","tools","integrate","distributed","","lack","common","foundation","creates","issues","developers","users","common","foundation","hadoop","client","tools"],"features":{"type":0,"size":1000,"indices":[15,26,36,58,82,116,118,135,138,170,181,228,235,243,263,281,307,315,333,343,346,347,352,371,372,388,400,409,427,461,462,475,487,494,513,521,572,583,599,608,650,653,661,674,706,710,755,761,769,801,831,885,897,922,924,945,954,967,970,996],"values":[1.0,1.0,6.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,4.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,16.0,3.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,4.0,1.0]},"cluster_label":17}
{"_c0":"As a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults  To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review","_c1":"Use reference counting to prevent blocks from being evicted during reads","document":"As a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults  To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review Use reference counting to prevent blocks from being evicted during reads","words":["as","a","pre","requisite","to","off","heap","caching","of","blocks","","we","need","a","mechanism","to","prevent","pages","","","blocks","from","being","evicted","while","they","are","being","read","","with","on","heap","objects","","evicting","a","block","while","it","is","being","read","merely","leads","to","memory","accounting","problems","","because","we","assume","that","an","evicted","block","is","a","candidate","for","garbage","collection","","which","will","not","be","true","during","a","read","","","but","with","off","heap","memory","this","will","lead","to","either","data","corruption","or","segmentation","faults","","to","address","this","","we","should","add","a","reference","counting","mechanism","to","track","which","blocks","pages","are","being","read","in","order","to","prevent","them","from","being","evicted","prematurely","","i","propose","to","do","this","in","two","phases","","first","","add","a","safe","","conservative","approach","in","which","all","blockmanager","get","","","","calls","implicitly","increment","the","reference","count","of","blocks","and","where","tasks","","references","are","automatically","freed","upon","task","completion","","this","will","be","correct","but","may","have","adverse","performance","impacts","because","it","will","prevent","legitimate","block","evictions","","in","phase","two","","we","should","incrementally","add","release","","","calls","in","order","to","fix","the","eviction","of","unreferenced","blocks","","the","latter","change","may","need","to","touch","many","different","components","","which","is","why","i","propose","to","do","it","separately","in","order","to","make","the","changes","easier","to","reason","about","and","review","use","reference","counting","to","prevent","blocks","from","being","evicted","during","reads"],"filtered":["pre","requisite","heap","caching","blocks","","need","mechanism","prevent","pages","","","blocks","evicted","read","","heap","objects","","evicting","block","read","merely","leads","memory","accounting","problems","","assume","evicted","block","candidate","garbage","collection","","true","read","","","heap","memory","lead","either","data","corruption","segmentation","faults","","address","","add","reference","counting","mechanism","track","blocks","pages","read","order","prevent","evicted","prematurely","","propose","two","phases","","first","","add","safe","","conservative","approach","blockmanager","get","","","","calls","implicitly","increment","reference","count","blocks","tasks","","references","automatically","freed","upon","task","completion","","correct","may","adverse","performance","impacts","prevent","legitimate","block","evictions","","phase","two","","incrementally","add","release","","","calls","order","fix","eviction","unreferenced","blocks","","latter","change","may","need","touch","many","different","components","","propose","separately","order","make","changes","easier","reason","review","use","reference","counting","prevent","blocks","evicted","reads"],"features":{"type":0,"size":1000,"indices":[8,18,20,30,36,48,78,82,83,89,92,95,116,125,138,139,158,170,183,187,188,211,230,236,251,268,272,277,281,294,299,303,329,330,333,336,343,344,356,359,363,371,372,373,374,388,393,399,403,408,420,421,432,445,451,452,474,489,493,494,495,497,505,511,520,525,528,534,537,551,572,590,594,597,604,621,622,642,650,656,664,665,666,689,693,695,707,709,710,718,735,739,752,759,760,777,788,813,866,870,908,914,921,924,936,945,959,963,968,973,993,996],"values":[4.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,7.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,26.0,6.0,6.0,14.0,1.0,1.0,4.0,3.0,4.0,2.0,3.0,7.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,6.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,3.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,4.0,1.0]},"cluster_label":14}
{"_c0":"As hadoop     will drop the support of Java    the jenkins slaves should be compiling code using Java","_c1":"Move jenkins to Java","document":"As hadoop     will drop the support of Java    the jenkins slaves should be compiling code using Java Move jenkins to Java","words":["as","hadoop","","","","","will","drop","the","support","of","java","","","","the","jenkins","slaves","should","be","compiling","code","using","java","move","jenkins","to","java"],"filtered":["hadoop","","","","","drop","support","java","","","","jenkins","slaves","compiling","code","using","java","move","jenkins","java"],"features":{"type":0,"size":1000,"indices":[181,276,282,292,343,372,383,388,420,572,577,624,656,665,695,710,967],"values":[1.0,2.0,1.0,1.0,1.0,7.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0]},"cluster_label":9}
{"_c0":"As noted in MAPREDUCE      and HADOOP       LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler  We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations","_c1":"LocalDirAllocator should avoid holding locks while accessing the filesystem","document":"As noted in MAPREDUCE      and HADOOP       LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler  We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations LocalDirAllocator should avoid holding locks while accessing the filesystem","words":["as","noted","in","mapreduce","","","","","","and","hadoop","","","","","","","localdirallocator","can","be","a","bottleneck","for","multithreaded","setups","like","the","shufflehandler","","we","should","consider","moving","to","a","lockless","design","or","minimizing","the","critical","sections","to","a","very","small","amount","of","time","that","does","not","involve","i","o","operations","localdirallocator","should","avoid","holding","locks","while","accessing","the","filesystem"],"filtered":["noted","mapreduce","","","","","","hadoop","","","","","","","localdirallocator","bottleneck","multithreaded","setups","like","shufflehandler","","consider","moving","lockless","design","minimizing","critical","sections","small","amount","time","involve","o","operations","localdirallocator","avoid","holding","locks","accessing","filesystem"],"features":{"type":0,"size":1000,"indices":[9,18,26,36,92,109,114,125,157,170,181,187,189,257,312,329,330,333,343,364,372,388,394,414,445,513,537,572,580,609,656,665,688,698,707,710,730,735,742,760,833,880,893,929,944,953,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,12.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"As of Spark      Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API","_c1":"Native database table system catalog","document":"As of Spark      Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API Native database table system catalog","words":["as","of","spark","","","","","","spark","sql","internally","has","only","a","limited","catalog","","and","does","not","support","any","of","the","ddls","","this","is","an","umbrella","ticket","to","introduce","an","internal","api","for","a","system","catalog","","and","the","associated","ddl","implementations","using","this","api","native","database","table","system","catalog"],"filtered":["spark","","","","","","spark","sql","internally","limited","catalog","","support","ddls","","umbrella","ticket","introduce","internal","api","system","catalog","","associated","ddl","implementations","using","api","native","database","table","system","catalog"],"features":{"type":0,"size":1000,"indices":[18,36,91,105,160,170,187,255,281,291,295,333,343,362,372,373,388,443,495,510,572,580,624,639,641,644,686,695,698,710,752,837,858,899,925],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,8.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"As of https   issues apache org jira browse SPARK       we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine  this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them  We should remove the legacy log syncing code  since this is a blocker to disabling Worker    Master SSH on Jenkins","_c1":"Remove legacy SCP based Jenkins log archiving code","document":"As of https   issues apache org jira browse SPARK       we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine  this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them  We should remove the legacy log syncing code  since this is a blocker to disabling Worker    Master SSH on Jenkins Remove legacy SCP based Jenkins log archiving code","words":["as","of","https","","","issues","apache","org","jira","browse","spark","","","","","","","we","no","longer","need","to","use","our","custom","scp","based","mechanism","for","archiving","jenkins","logs","on","the","master","machine","","this","has","been","superseded","by","the","use","of","a","jenkins","plugin","which","archives","the","logs","and","provides","public","viewing","of","them","","we","should","remove","the","legacy","log","syncing","code","","since","this","is","a","blocker","to","disabling","worker","","","","master","ssh","on","jenkins","remove","legacy","scp","based","jenkins","log","archiving","code"],"filtered":["https","","","issues","apache","org","jira","browse","spark","","","","","","","longer","need","use","custom","scp","based","mechanism","archiving","jenkins","logs","master","machine","","superseded","use","jenkins","plugin","archives","logs","provides","public","viewing","","remove","legacy","log","syncing","code","","since","blocker","disabling","worker","","","","master","ssh","jenkins","remove","legacy","scp","based","jenkins","log","archiving","code"],"features":{"type":0,"size":1000,"indices":[36,82,105,122,154,170,223,233,239,243,270,276,281,288,333,343,346,366,372,373,388,394,400,420,475,489,495,498,535,537,572,580,585,597,625,631,656,665,704,710,821,852,868,876,888,907,924,945,970,980,991,993,998],"values":[1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,2.0,1.0,3.0,2.0,2.0,14.0,2.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":19}
{"_c0":"As part of the work to implement SPARK        it would be nice to have the network library efficiently stream data over a connection  Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately  Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug","_c1":"Support streaming data using network library","document":"As part of the work to implement SPARK        it would be nice to have the network library efficiently stream data over a connection  Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately  Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug Support streaming data using network library","words":["as","part","of","the","work","to","implement","spark","","","","","","","","it","would","be","nice","to","have","the","network","library","efficiently","stream","data","over","a","connection","","currently","all","it","has","is","the","shuffle","data","protocol","","which","is","not","very","efficient","for","large","files","","it","requires","the","whole","file","to","be","buffered","on","the","receiver","side","before","the","receiver","can","do","anything","","for","large","files","","that","comes","at","a","huge","cost","in","memory","","you","can","chunk","large","files","but","that","requires","the","client","to","ask","for","each","chunk","separately","","instead","","a","similar","approach","but","allowing","the","data","to","be","processed","as","it","arrives","would","be","a","lot","more","efficient","","and","make","it","easier","to","implement","the","file","server","in","the","referenced","bug","support","streaming","data","using","network","library"],"filtered":["part","work","implement","spark","","","","","","","","nice","network","library","efficiently","stream","data","connection","","currently","shuffle","data","protocol","","efficient","large","files","","requires","whole","file","buffered","receiver","side","receiver","anything","","large","files","","comes","huge","cost","memory","","chunk","large","files","requires","client","ask","chunk","separately","","instead","","similar","approach","allowing","data","processed","arrives","lot","efficient","","make","easier","implement","file","server","referenced","bug","support","streaming","data","using","network","library"],"features":{"type":0,"size":1000,"indices":[18,36,51,52,74,82,83,95,105,108,114,135,154,159,163,170,188,198,218,235,249,263,281,299,333,343,352,370,372,388,411,425,445,446,460,472,474,490,494,495,525,527,534,551,567,568,572,580,597,624,629,656,662,695,710,722,735,740,756,760,763,781,782,788,833,863,885,903,910,938,944,957,968,993],"values":[1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,16.0,6.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,5.0,10.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"As per  comment https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           from   wheat   in HADOOP        Need to remove FileUtil copyMerge  CC  to   wheat","_c1":"Remove FileUtil copyMerge","document":"As per  comment https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           from   wheat   in HADOOP        Need to remove FileUtil copyMerge  CC  to   wheat Remove FileUtil copyMerge","words":["as","per","","comment","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","","","","","","","","","","","from","","","wheat","","","in","hadoop","","","","","","","","need","to","remove","fileutil","copymerge","","cc","","to","","","wheat","remove","fileutil","copymerge"],"filtered":["per","","comment","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","","","","","","","","","","","","","wheat","","","hadoop","","","","","","","","need","remove","fileutil","copymerge","","cc","","","","wheat","remove","fileutil","copymerge"],"features":{"type":0,"size":1000,"indices":[76,110,154,181,221,280,288,294,370,372,388,440,445,475,495,535,537,572,605,639,662,821,827,862,921,980,998],"values":[1.0,2.0,1.0,2.0,1.0,2.0,2.0,3.0,1.0,43.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"As work continues on HADOOP        it s become evident that we need better hooks to start daemons as specifically configured users  Via the  command   subcommand  USER environment variables in   x  we actually have a standardized way to do that  This in turn means we can make the sbin scripts super functional with a bit of updating    Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users","_c1":"Update scripts to be smarter when running with privilege","document":"As work continues on HADOOP        it s become evident that we need better hooks to start daemons as specifically configured users  Via the  command   subcommand  USER environment variables in   x  we actually have a standardized way to do that  This in turn means we can make the sbin scripts super functional with a bit of updating    Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users Update scripts to be smarter when running with privilege","words":["as","work","continues","on","hadoop","","","","","","","","it","s","become","evident","that","we","need","better","hooks","to","start","daemons","as","specifically","configured","users","","via","the","","command","","","subcommand","","user","environment","variables","in","","","x","","we","actually","have","a","standardized","way","to","do","that","","this","in","turn","means","we","can","make","the","sbin","scripts","super","functional","with","a","bit","of","updating","","","","consolidate","start","dfs","sh","and","start","secure","dns","sh","into","one","script","","","make","start","","","","sh","and","stop","","","","sh","know","how","to","switch","users","when","run","as","root","","","undeprecate","start","stop","all","sh","so","that","it","could","be","used","as","root","for","production","purposes","and","as","a","single","user","for","non","production","users","update","scripts","to","be","smarter","when","running","with","privilege"],"filtered":["work","continues","hadoop","","","","","","","","become","evident","need","better","hooks","start","daemons","specifically","configured","users","","via","","command","","","subcommand","","user","environment","variables","","","x","","actually","standardized","way","","turn","means","make","sbin","scripts","super","functional","bit","updating","","","","consolidate","start","dfs","sh","start","secure","dns","sh","one","script","","","make","start","","","","sh","stop","","","","sh","know","switch","users","run","root","","","undeprecate","start","stop","sh","used","root","production","purposes","single","user","non","production","users","update","scripts","smarter","running","privilege"],"features":{"type":0,"size":1000,"indices":[6,36,44,48,76,82,135,159,170,181,197,213,224,229,237,249,272,275,299,313,333,340,343,350,364,368,372,373,388,394,441,445,446,447,482,495,498,520,525,527,531,534,536,537,540,572,573,579,595,605,615,631,650,656,657,658,659,660,710,733,755,758,760,779,781,796,810,820,833,846,882,886,891,904,941,963,968,993,996],"values":[2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,29.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,5.0]},"cluster_label":3}
{"_c0":"Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency  This JIRA removes the SDK snapshot dependency to released SDK candidate  There is not functional change in the SDK and no impact to live contract test","_c1":"Remove snapshot version of SDK dependency from Azure Data Lake Store File System","document":"Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency  This JIRA removes the SDK snapshot dependency to released SDK candidate  There is not functional change in the SDK and no impact to live contract test Remove snapshot version of SDK dependency from Azure Data Lake Store File System","words":["azure","data","lake","store","file","system","dependent","azure","data","lake","store","sdk","is","released","and","has","not","need","for","further","snapshot","version","dependency","","this","jira","removes","the","sdk","snapshot","dependency","to","released","sdk","candidate","","there","is","not","functional","change","in","the","sdk","and","no","impact","to","live","contract","test","remove","snapshot","version","of","sdk","dependency","from","azure","data","lake","store","file","system"],"filtered":["azure","data","lake","store","file","system","dependent","azure","data","lake","store","sdk","released","need","snapshot","version","dependency","","jira","removes","sdk","snapshot","dependency","released","sdk","candidate","","functional","change","sdk","impact","live","contract","test","remove","snapshot","version","sdk","dependency","azure","data","lake","store","file","system"],"features":{"type":0,"size":1000,"indices":[18,36,108,125,158,171,246,280,281,288,333,343,346,352,372,373,388,445,491,493,520,527,537,580,586,588,615,639,695,710,736,743,810,821,831,908,921,995],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,5.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,3.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative  AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client  This JIRA is created to enable Azure Storage Client Side logging at the Job submission level  User should be able to configure Client Side logging on a Per Job bases","_c1":"Enable Azure Storage Client Side logging","document":"AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative  AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client  This JIRA is created to enable Azure Storage Client Side logging at the Job submission level  User should be able to configure Client Side logging on a Per Job bases Enable Azure Storage Client Side logging","words":["azurestorageexceptions","currently","are","logged","as","part","of","the","wab","code","which","often","is","not","too","informative","","azurestorage","sdk","supports","client","side","logging","that","can","be","enabled","that","logs","relevant","information","w","r","t","request","made","from","the","storage","client","","this","jira","is","created","to","enable","azure","storage","client","side","logging","at","the","job","submission","level","","user","should","be","able","to","configure","client","side","logging","on","a","per","job","bases","enable","azure","storage","client","side","logging"],"filtered":["azurestorageexceptions","currently","logged","part","wab","code","often","informative","","azurestorage","sdk","supports","client","side","logging","enabled","logs","relevant","information","w","r","request","made","storage","client","","jira","created","enable","azure","storage","client","side","logging","job","submission","level","","user","able","configure","client","side","logging","per","job","bases","enable","azure","storage","client","side","logging"],"features":{"type":0,"size":1000,"indices":[18,61,82,92,135,138,170,222,235,236,262,280,281,343,344,372,373,388,389,394,401,420,440,455,470,480,491,496,562,570,572,597,644,645,656,665,710,740,756,760,763,777,781,810,821,827,830,833,876,882,915,921,978,994],"values":[1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Bagel has been deprecated and we haven t done any changes to it  There is no need to run those tests","_c1":"Remove Bagel test suites","document":"Bagel has been deprecated and we haven t done any changes to it  There is no need to run those tests Remove Bagel test suites","words":["bagel","has","been","deprecated","and","we","haven","t","done","any","changes","to","it","","there","is","no","need","to","run","those","tests","remove","bagel","test","suites"],"filtered":["bagel","deprecated","haven","done","changes","","need","run","tests","remove","bagel","test","suites"],"features":{"type":0,"size":1000,"indices":[91,281,288,333,346,363,364,372,388,495,535,537,539,580,586,600,619,620,707,777,831,931,964,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Based on discussion at YETUS      this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff","_c1":"Incorporate checkcompatibility script which runs Java API Compliance Checker","document":"Based on discussion at YETUS      this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff Incorporate checkcompatibility script which runs Java API Compliance Checker","words":["based","on","discussion","at","yetus","","","","","","this","code","can","t","go","there","","but","it","s","still","very","useful","for","release","managers","","a","similar","variant","of","this","script","has","been","used","for","a","while","by","apache","hbase","and","apache","kudu","","and","imo","jacc","output","is","easier","to","understand","than","jdiff","incorporate","checkcompatibility","script","which","runs","java","api","compliance","checker"],"filtered":["based","discussion","yetus","","","","","","code","go","","still","useful","release","managers","","similar","variant","script","used","apache","hbase","apache","kudu","","imo","jacc","output","easier","understand","jdiff","incorporate","checkcompatibility","script","runs","java","api","compliance","checker"],"features":{"type":0,"size":1000,"indices":[29,36,77,82,83,96,122,164,170,197,223,224,241,261,265,272,281,333,343,350,371,372,373,388,397,420,474,495,535,580,597,605,625,644,668,682,689,695,707,719,756,777,800,831,833,910,944,967],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,8.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Based on discussion offline with   marmbrus   we should remove GenerateProjection","_c1":"Remove GenerateProjection","document":"Based on discussion offline with   marmbrus   we should remove GenerateProjection Remove GenerateProjection","words":["based","on","discussion","offline","with","","","marmbrus","","","we","should","remove","generateprojection","remove","generateprojection"],"filtered":["based","discussion","offline","","","marmbrus","","","remove","generateprojection","remove","generateprojection"],"features":{"type":0,"size":1000,"indices":[19,82,288,372,444,625,650,665,695,924,993],"values":[1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Because CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S  and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file","_c1":"Add   direct  flag option for fs copy so that user can choose not to create    COPYING   file","document":"Because CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S  and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file Add   direct  flag option for fs copy so that user can choose not to create    COPYING   file","words":["because","cli","is","using","commandwithdestination","java","which","add","","","","copying","","","to","the","tail","of","file","name","when","it","does","the","copy","","for","blobstore","like","s","","and","swift","","to","create","","","","copying","","","file","and","rename","it","is","expensive","","","","direct","","flag","can","allow","user","to","avoiding","the","","","","copying","","","file","add","","","direct","","flag","option","for","fs","copy","so","that","user","can","choose","not","to","create","","","","copying","","","file"],"filtered":["cli","using","commandwithdestination","java","add","","","","copying","","","tail","file","name","copy","","blobstore","like","","swift","","create","","","","copying","","","file","rename","expensive","","","","direct","","flag","allow","user","avoiding","","","","copying","","","file","add","","","direct","","flag","option","fs","copy","user","choose","create","","","","copying","","","file"],"features":{"type":0,"size":1000,"indices":[15,18,36,62,69,76,108,135,197,216,222,231,265,281,330,333,343,368,372,388,421,432,466,488,495,518,597,624,694,698,706,710,749,760,833,852,867,882,897,932,967],"values":[1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,30.0,4.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":3}
{"_c0":"Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads","_c1":"Automatically use Kryo serializer when shuffling RDDs with simple types","document":"Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads Automatically use Kryo serializer when shuffling RDDs with simple types","words":["because","classtags","are","available","when","constructing","shuffledrdd","we","can","use","them","to","automatically","use","kryo","for","shuffle","serialization","when","the","rdd","s","types","are","guaranteed","to","be","compatible","with","kryo","","e","g","","rdds","whose","key","","value","","and","or","combiner","types","are","primitives","","arrays","of","primitives","","or","strings","","","this","is","likely","to","result","in","a","large","performance","gain","for","many","rdd","api","workloads","automatically","use","kryo","serializer","when","shuffling","rdds","with","simple","types"],"filtered":["classtags","available","constructing","shuffledrdd","use","automatically","use","kryo","shuffle","serialization","rdd","types","guaranteed","compatible","kryo","","e","g","","rdds","whose","key","","value","","combiner","types","primitives","","arrays","primitives","","strings","","","likely","result","large","performance","gain","many","rdd","api","workloads","automatically","use","kryo","serializer","shuffling","rdds","simple","types"],"features":{"type":0,"size":1000,"indices":[5,15,30,36,76,82,84,130,138,170,187,188,197,212,217,261,281,283,316,333,343,355,371,372,373,388,417,421,445,465,476,477,481,489,505,568,644,650,656,710,759,768,782,791,833,843,865,870,878,924,980,993,997],"values":[1.0,1.0,1.0,2.0,3.0,3.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Because of reasons listed here  http   findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE comparators should be serializable  To make deserialization work  it is required that all superclasses have no arg constructor  http   findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR Simply add no arg constructor to WritableComparator","_c1":"WritableComparator must implement no arg constructor","document":"Because of reasons listed here  http   findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE comparators should be serializable  To make deserialization work  it is required that all superclasses have no arg constructor  http   findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR Simply add no arg constructor to WritableComparator WritableComparator must implement no arg constructor","words":["because","of","reasons","listed","here","","http","","","findbugs","sourceforge","net","bugdescriptions","html","se","comparator","should","be","serializable","comparators","should","be","serializable","","to","make","deserialization","work","","it","is","required","that","all","superclasses","have","no","arg","constructor","","http","","","findbugs","sourceforge","net","bugdescriptions","html","se","no","suitable","constructor","simply","add","no","arg","constructor","to","writablecomparator","writablecomparator","must","implement","no","arg","constructor"],"filtered":["reasons","listed","","http","","","findbugs","sourceforge","net","bugdescriptions","html","se","comparator","serializable","comparators","serializable","","make","deserialization","work","","required","superclasses","arg","constructor","","http","","","findbugs","sourceforge","net","bugdescriptions","html","se","suitable","constructor","simply","add","arg","constructor","writablecomparator","writablecomparator","must","implement","arg","constructor"],"features":{"type":0,"size":1000,"indices":[23,32,73,135,140,252,281,299,319,343,346,372,373,388,421,432,452,459,472,487,495,510,525,527,529,627,630,652,656,665,760,865,879,915,968,991],"values":[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,8.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,4.0,1.0,2.0,2.0,2.0,3.0,2.0]},"cluster_label":9}
{"_c0":"Before        https   github com apache spark pull        submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now        https   github com apache spark pull       has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now","_c1":"Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result","document":"Before        https   github com apache spark pull        submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now        https   github com apache spark pull       has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result","words":["before","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","submitjob","would","create","a","separate","thread","to","wait","for","the","job","result","","","submitjobthreadpool","","was","a","workaround","in","","receivertracker","","to","run","these","waiting","job","result","threads","","now","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","has","been","merged","to","master","and","resolved","this","blocking","issue","","","submitjobthreadpool","","can","be","removed","now","remove","submitjobthreadpool","since","submitjob","doesn","t","create","a","separate","thread","to","wait","for","the","job","result"],"filtered":["","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","submitjob","create","separate","thread","wait","job","result","","","submitjobthreadpool","","workaround","","receivertracker","","run","waiting","job","result","threads","","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","merged","master","resolved","blocking","issue","","","submitjobthreadpool","","removed","remove","submitjobthreadpool","since","submitjob","doesn","create","separate","thread","wait","job","result"],"features":{"type":0,"size":1000,"indices":[36,98,105,109,133,159,163,170,221,234,245,265,270,288,333,343,364,372,373,388,445,461,470,477,495,500,509,510,512,535,556,580,585,591,597,656,690,710,718,748,759,777,805,833,865,998],"values":[2.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,40.0,1.0,4.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0]},"cluster_label":16}
{"_c0":"Besides the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP       HADOOP        HADOOP      makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it","_c1":"Hadoop Core should support source filesfor multiple schedulers","document":"Besides the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP       HADOOP        HADOOP      makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it Hadoop Core should support source filesfor multiple schedulers","words":["besides","the","default","jt","scheduling","algorithm","","there","is","work","going","on","with","at","least","two","more","schedulers","","hadoop","","","","","","","hadoop","","","","","","","","hadoop","","","","","","makes","it","easier","to","plug","in","new","schedulers","into","the","jt","","where","do","we","place","the","source","files","for","various","schedulers","so","that","it","s","easy","for","users","to","choose","their","scheduler","of","choice","during","deployment","","and","easy","for","developers","to","add","in","more","schedulers","into","the","framework","","without","inundating","it","hadoop","core","should","support","source","filesfor","multiple","schedulers"],"filtered":["besides","default","jt","scheduling","algorithm","","work","going","least","two","schedulers","","hadoop","","","","","","","hadoop","","","","","","","","hadoop","","","","","","makes","easier","plug","new","schedulers","jt","","place","source","files","various","schedulers","easy","users","choose","scheduler","choice","deployment","","easy","developers","add","schedulers","framework","","without","inundating","hadoop","core","support","source","filesfor","multiple","schedulers"],"features":{"type":0,"size":1000,"indices":[25,36,40,70,82,139,181,191,197,215,228,235,268,277,281,315,333,343,368,372,381,388,408,430,432,445,474,493,495,527,534,551,592,599,629,650,665,691,695,706,710,731,748,755,756,757,760,788,818,831,846,884,891,920,978,993,999],"values":[1.0,3.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,23.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"Both VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection  Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this     following DataTypes java in SQL  so Java users doesn t need the extra          Define DataTypes in Scala","_c1":"Expose VectorUDT MatrixUDT in a public API","document":"Both VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection  Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this     following DataTypes java in SQL  so Java users doesn t need the extra          Define DataTypes in Scala Expose VectorUDT MatrixUDT in a public API","words":["both","vectorudt","and","matrixudt","are","private","apis","","because","userdefinedtype","itself","is","private","in","spark","","however","","in","order","to","let","developers","implement","their","own","transformers","and","estimators","","we","should","expose","both","types","in","a","public","api","to","simply","the","implementation","of","transformschema","","transform","","etc","","otherwise","","they","need","to","get","the","data","types","using","reflection","","note","that","this","doesn","t","mean","to","expose","vectorudt","matrixudt","classes","","we","can","just","have","a","method","or","a","static","value","that","returns","vectorudt","matrixudt","instance","with","datatype","as","the","return","type","","there","are","two","ways","to","implement","this","","","","","following","datatypes","java","in","sql","","so","java","users","doesn","t","need","the","extra","","","","","","","","","","define","datatypes","in","scala","expose","vectorudt","matrixudt","in","a","public","api"],"filtered":["vectorudt","matrixudt","private","apis","","userdefinedtype","private","spark","","however","","order","let","developers","implement","transformers","estimators","","expose","types","public","api","simply","implementation","transformschema","","transform","","etc","","otherwise","","need","get","data","types","using","reflection","","note","doesn","mean","expose","vectorudt","matrixudt","classes","","method","static","value","returns","vectorudt","matrixudt","instance","datatype","return","type","","two","ways","implement","","","","","following","datatypes","java","sql","","java","users","doesn","need","extra","","","","","","","","","","define","datatypes","scala","expose","vectorudt","matrixudt","public","api"],"features":{"type":0,"size":1000,"indices":[1,8,48,53,78,79,91,101,105,109,118,122,138,144,164,170,174,187,208,228,235,281,299,307,315,319,333,343,368,372,373,377,388,408,421,428,445,465,472,490,498,500,526,537,572,609,624,644,650,651,654,665,673,686,695,698,704,710,718,755,760,768,777,782,809,831,833,842,863,875,909,950,959,967,993,996],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,25.0,2.0,1.0,5.0,1.0,1.0,1.0,10.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,4.0,1.0,2.0,2.0,1.0]},"cluster_label":3}
{"_c0":"Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that","_c1":"Remove LeftSemiJoinBNL","document":"Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that Remove LeftSemiJoinBNL","words":["broadcast","left","semi","join","without","joining","keys","is","already","supported","in","broadcastnestedloopjoin","","it","has","the","same","implementation","as","leftsemijoinbnl","","we","should","remove","that","remove","leftsemijoinbnl"],"filtered":["broadcast","left","semi","join","without","joining","keys","already","supported","broadcastnestedloopjoin","","implementation","leftsemijoinbnl","","remove","remove","leftsemijoinbnl"],"features":{"type":0,"size":1000,"indices":[57,171,220,281,288,372,393,445,495,572,580,593,617,656,662,665,698,710,760,791,884,909,952,993],"values":[1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"CC   mgummelt    tnachen    skonto  I think this is fairly easy and would be beneficial as more work goes into Mesos  It should separate into a module like YARN does  just on principle really  but because it also means anyone that doesn t need Mesos support can build without it  I m entirely willing to take a shot at this","_c1":"Collect Mesos support code into a module profile","document":"CC   mgummelt    tnachen    skonto  I think this is fairly easy and would be beneficial as more work goes into Mesos  It should separate into a module like YARN does  just on principle really  but because it also means anyone that doesn t need Mesos support can build without it  I m entirely willing to take a shot at this Collect Mesos support code into a module profile","words":["cc","","","mgummelt","","","","tnachen","","","","skonto","","i","think","this","is","fairly","easy","and","would","be","beneficial","as","more","work","goes","into","mesos","","it","should","separate","into","a","module","like","yarn","does","","just","on","principle","really","","but","because","it","also","means","anyone","that","doesn","t","need","mesos","support","can","build","without","it","","i","m","entirely","willing","to","take","a","shot","at","this","collect","mesos","support","code","into","a","module","profile"],"filtered":["cc","","","mgummelt","","","","tnachen","","","","skonto","","think","fairly","easy","beneficial","work","goes","mesos","","separate","module","like","yarn","","principle","really","","also","means","anyone","doesn","need","mesos","support","build","without","","m","entirely","willing","take","shot","collect","mesos","support","code","module","profile"],"features":{"type":0,"size":1000,"indices":[10,19,82,83,110,161,163,170,209,281,299,307,310,329,330,333,340,372,373,388,394,413,420,421,495,500,527,536,537,564,572,594,605,629,630,638,656,665,695,698,718,756,760,777,792,833,855,884,891,897,919,965,982],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,13.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,3.0,1.0]},"cluster_label":17}
{"_c0":"CSRF prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header  The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin","_c1":"Add CSRF Filter for REST APIs to Hadoop Common","document":"CSRF prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header  The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin Add CSRF Filter for REST APIs to Hadoop Common","words":["csrf","prevention","for","rest","apis","can","be","provided","through","a","common","servlet","filter","","this","filter","would","check","for","the","existence","of","an","expected","","configurable","","http","header","","","such","as","x","xsrf","header","","the","fact","that","csrf","attacks","are","entirely","browser","based","means","that","the","above","approach","can","ensure","that","requests","are","coming","from","either","","applications","served","by","the","same","origin","as","the","rest","api","or","that","there","is","explicit","policy","configuration","that","allows","the","setting","of","a","header","on","xmlhttprequest","from","another","origin","add","csrf","filter","for","rest","apis","to","hadoop","common"],"filtered":["csrf","prevention","rest","apis","provided","common","servlet","filter","","filter","check","existence","expected","","configurable","","http","header","","","x","xsrf","header","","fact","csrf","attacks","entirely","browser","based","means","approach","ensure","requests","coming","either","","applications","served","origin","rest","api","explicit","policy","configuration","allows","setting","header","xmlhttprequest","another","origin","add","csrf","filter","rest","apis","hadoop","common"],"features":{"type":0,"size":1000,"indices":[36,45,71,82,95,138,163,170,181,187,208,215,216,223,272,281,318,343,372,373,386,388,394,413,432,480,543,572,623,625,640,644,645,656,659,665,691,702,704,708,710,731,742,752,760,766,778,779,783,792,810,823,831,833,842,866,882,921,945,954,964,998],"values":[3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0,6.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":19}
{"_c0":"CSV is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users  We should consider inlining https   github com databricks spark csv","_c1":"Have a built in CSV data source implementation","document":"CSV is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users  We should consider inlining https   github com databricks spark csv Have a built in CSV data source implementation","words":["csv","is","the","most","common","data","format","in","the","","small","data","","world","","it","is","often","the","first","format","people","want","to","try","when","they","see","spark","on","a","single","node","","making","this","built","in","for","the","most","common","source","can","provide","a","better","experience","for","first","time","users","","we","should","consider","inlining","https","","","github","com","databricks","spark","csv","have","a","built","in","csv","data","source","implementation"],"filtered":["csv","common","data","format","","small","data","","world","","often","first","format","people","want","try","see","spark","single","node","","making","built","common","source","provide","better","experience","first","time","users","","consider","inlining","https","","","github","com","databricks","spark","csv","built","csv","data","source","implementation"],"features":{"type":0,"size":1000,"indices":[36,48,70,76,82,105,150,157,159,170,183,221,222,281,288,299,312,323,362,372,373,388,401,445,485,486,495,510,515,531,665,695,698,710,712,742,745,755,770,775,833,906,941,954,993,996,998],"values":[2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,7.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Checkpoint to verify the fsimage each time it creates the new one","_c1":"Keep two generations of fsimage","document":"Checkpoint to verify the fsimage each time it creates the new one Keep two generations of fsimage","words":["checkpoint","to","verify","the","fsimage","each","time","it","creates","the","new","one","keep","two","generations","of","fsimage"],"filtered":["checkpoint","verify","fsimage","time","creates","new","one","keep","two","generations","fsimage"],"features":{"type":0,"size":1000,"indices":[25,44,157,343,388,408,443,487,495,580,594,710,771,885,907],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail","_c1":"Improve the type check of CollectSet in CheckAnalysis","document":"CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail Improve the type check of CollectSet in CheckAnalysis","words":["collectset","","cannot","have","map","typed","data","because","maptypedata","does","not","implement","","equals","","","so","","if","we","find","map","type","in","","collectset","","","queries","fail","improve","the","type","check","of","collectset","in","checkanalysis"],"filtered":["collectset","","map","typed","data","maptypedata","implement","","equals","","","","find","map","type","","collectset","","","queries","fail","improve","type","check","collectset","checkanalysis"],"features":{"type":0,"size":1000,"indices":[18,73,130,170,172,202,254,299,343,368,372,421,445,472,510,522,526,532,695,698,703,710,882,931,993],"values":[1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,8.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Collection functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented","_c1":"Implement collection functions in SparkR","document":"Collection functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented Implement collection functions in SparkR","words":["collection","functions","documented","at","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","","are","size","","","","explode","","","","array","contains","","","and","sort","array","","","","size","","","","explode","","","are","already","implemented","","array","contains","","","and","sort","array","","","are","to","be","implemented","implement","collection","functions","in","sparkr"],"filtered":["collection","functions","documented","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","","size","","","","explode","","","","array","contains","","","sort","array","","","","size","","","","explode","","","already","implemented","","array","contains","","","sort","array","","","implemented","implement","collection","functions","sparkr"],"features":{"type":0,"size":1000,"indices":[48,57,105,138,177,192,230,307,333,372,376,388,445,472,490,495,498,535,545,587,644,652,656,665,686,706,720,756,767,899],"values":[2.0,1.0,2.0,3.0,2.0,2.0,2.0,1.0,2.0,24.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"Common tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development","_c1":"Add unit tests framework  Mockito","document":"Common tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development Add unit tests framework  Mockito","words":["common","tests","are","functional","tests","or","end","to","end","","it","makes","sense","to","have","mockito","framework","for","the","convenience","of","true","unit","tests","development","add","unit","tests","framework","","mockito"],"filtered":["common","tests","functional","tests","end","end","","makes","sense","mockito","framework","convenience","true","unit","tests","development","add","unit","tests","framework","","mockito"],"features":{"type":0,"size":1000,"indices":[5,36,138,187,188,207,284,299,305,335,343,356,372,388,432,495,599,615,619,691,710,954],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read","_c1":"Configuration sends too much data to log j","document":"Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read Configuration sends too much data to log j","words":["configuration","objects","send","a","debug","level","log","message","every","time","they","re","instantiated","","which","include","a","full","stack","trace","","this","is","more","appropriate","for","trace","level","logging","","as","it","renders","other","debug","logs","very","hard","to","read","configuration","sends","too","much","data","to","log","j"],"filtered":["configuration","objects","send","debug","level","log","message","every","time","re","instantiated","","include","full","stack","trace","","appropriate","trace","level","logging","","renders","debug","logs","hard","read","configuration","sends","much","data","log","j"],"features":{"type":0,"size":1000,"indices":[36,48,101,125,149,157,170,236,281,336,372,373,388,395,401,425,495,523,524,572,597,601,629,631,644,645,646,650,674,691,695,722,778,811,876,897,936,944,981],"values":[1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Continue the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive","_c1":"Remove setCheckpointDir from LDA and tree Strategy","document":"Continue the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive Remove setCheckpointDir from LDA and tree Strategy","words":["continue","the","discussion","from","the","lda","pr","","checkpoingdir","is","a","global","spark","configuration","","which","should","not","be","altered","by","an","ml","algorithm","","we","could","check","whether","checkpointdir","is","set","if","checkpointinterval","is","positive","remove","setcheckpointdir","from","lda","and","tree","strategy"],"filtered":["continue","discussion","lda","pr","","checkpoingdir","global","spark","configuration","","altered","ml","algorithm","","check","whether","checkpointdir","set","checkpointinterval","positive","remove","setcheckpointdir","lda","tree","strategy"],"features":{"type":0,"size":1000,"indices":[1,18,25,82,105,170,213,215,223,281,288,291,324,333,356,372,494,597,601,607,624,656,665,691,695,710,732,743,752,807,813,871,882,921,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Created a new private variable  boundTEncoder  that can be shared by multiple functions   RDD    select  and  collect     Replaced all the  queryExecution analyzed  by the function call  logicalPlan    A few API comments are using wrong class names  e g    DataFrame   or parameter names  e g    n     A few API descriptions are wrong   e g    mapPartitions","_c1":"SQL  Code refactoring and comment correction in Dataset APIs","document":"Created a new private variable  boundTEncoder  that can be shared by multiple functions   RDD    select  and  collect     Replaced all the  queryExecution analyzed  by the function call  logicalPlan    A few API comments are using wrong class names  e g    DataFrame   or parameter names  e g    n     A few API descriptions are wrong   e g    mapPartitions SQL  Code refactoring and comment correction in Dataset APIs","words":["created","a","new","private","variable","","boundtencoder","","that","can","be","shared","by","multiple","functions","","","rdd","","","","select","","and","","collect","","","","","replaced","all","the","","queryexecution","analyzed","","by","the","function","call","","logicalplan","","","","a","few","api","comments","are","using","wrong","class","names","","e","g","","","","dataframe","","","or","parameter","names","","e","g","","","","n","","","","","a","few","api","descriptions","are","wrong","","","e","g","","","","mappartitions","sql","","code","refactoring","and","comment","correction","in","dataset","apis"],"filtered":["created","new","private","variable","","boundtencoder","","shared","multiple","functions","","","rdd","","","","select","","","collect","","","","","replaced","","queryexecution","analyzed","","function","call","","logicalplan","","","","api","comments","using","wrong","class","names","","e","g","","","","dataframe","","","parameter","names","","e","g","","","","n","","","","","api","descriptions","wrong","","","e","g","","","","mappartitions","sql","","code","refactoring","comment","correction","dataset","apis"],"features":{"type":0,"size":1000,"indices":[20,24,25,34,51,138,146,161,170,187,223,227,262,276,293,294,313,333,372,400,417,420,445,478,493,515,534,549,577,586,587,592,594,624,644,655,656,686,704,710,721,760,822,833,842,870,878,939,968,997],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,39.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"Cross Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting  There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate","_c1":"Add XFS Filter for UIs to Hadoop Common","document":"Cross Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting  There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate Add XFS Filter for UIs to Hadoop Common","words":["cross","frame","scripting","","xfs","","prevention","for","uis","can","be","provided","through","a","common","servlet","filter","","this","filter","will","set","the","x","frame","options","http","header","to","deny","unless","configured","to","another","valid","setting","","there","are","a","number","of","uis","that","could","just","add","this","to","their","filters","as","well","as","the","yarn","webapp","proxy","which","could","add","it","for","all","it","s","proxied","uis","","","if","appropriate","add","xfs","filter","for","uis","to","hadoop","common"],"filtered":["cross","frame","scripting","","xfs","","prevention","uis","provided","common","servlet","filter","","filter","set","x","frame","options","http","header","deny","unless","configured","another","valid","setting","","number","uis","add","filters","well","yarn","webapp","proxy","add","proxied","uis","","","appropriate","add","xfs","filter","uis","hadoop","common"],"features":{"type":0,"size":1000,"indices":[36,39,100,138,157,170,181,197,213,229,234,235,307,343,372,373,388,420,432,495,543,564,570,572,583,597,640,646,651,656,659,665,677,704,710,731,733,740,760,775,778,779,810,813,831,833,852,877,945,953,954,963,968],"values":[3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,6.0,2.0,4.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Current   ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException","_c1":"ViewFileSystem should support storage policy related API","document":"Current   ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException ViewFileSystem should support storage policy related API","words":["current","","","viewfilesystem","","","does","not","support","storage","policy","related","api","","it","will","throw","","","unsupportedoperationexception","viewfilesystem","should","support","storage","policy","related","api"],"filtered":["current","","","viewfilesystem","","","support","storage","policy","related","api","","throw","","","unsupportedoperationexception","viewfilesystem","support","storage","policy","related","api"],"features":{"type":0,"size":1000,"indices":[18,139,199,267,372,394,420,495,612,644,665,695,698,710,998],"values":[1.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"Current implementation of WASB  only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration  which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers  Added to the fact that WASB does not inherently support ACL s  WASB is its current implementation cannot be securely used for environments like secure hadoop cluster  This JIRA is created to add a new mode in WASB  which operates on Azure Storage SAS keys  which can provide fine grained timed access to containers and blobs  providing a segway into supporting WASB for secure hadoop cluster  More details about the issue and the proposal are provided in the design proposal document","_c1":"Azure  Add a new SAS key mode for WASB","document":"Current implementation of WASB  only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration  which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers  Added to the fact that WASB does not inherently support ACL s  WASB is its current implementation cannot be securely used for environments like secure hadoop cluster  This JIRA is created to add a new mode in WASB  which operates on Azure Storage SAS keys  which can provide fine grained timed access to containers and blobs  providing a segway into supporting WASB for secure hadoop cluster  More details about the issue and the proposal are provided in the design proposal document Azure  Add a new SAS key mode for WASB","words":["current","implementation","of","wasb","","only","supports","azure","storage","keys","and","sas","key","being","provided","via","org","apache","hadoop","conf","configuration","","which","results","in","these","secrets","residing","in","the","same","address","space","as","the","wasb","process","and","providing","complete","access","to","the","azure","storage","account","and","its","containers","","added","to","the","fact","that","wasb","does","not","inherently","support","acl","s","","wasb","is","its","current","implementation","cannot","be","securely","used","for","environments","like","secure","hadoop","cluster","","this","jira","is","created","to","add","a","new","mode","in","wasb","","which","operates","on","azure","storage","sas","keys","","which","can","provide","fine","grained","timed","access","to","containers","and","blobs","","providing","a","segway","into","supporting","wasb","for","secure","hadoop","cluster","","more","details","about","the","issue","and","the","proposal","are","provided","in","the","design","proposal","document","azure","","add","a","new","sas","key","mode","for","wasb"],"filtered":["current","implementation","wasb","","supports","azure","storage","keys","sas","key","provided","via","org","apache","hadoop","conf","configuration","","results","secrets","residing","address","space","wasb","process","providing","complete","access","azure","storage","account","containers","","added","fact","wasb","inherently","support","acl","","wasb","current","implementation","securely","used","environments","like","secure","hadoop","cluster","","jira","created","add","new","mode","wasb","","operates","azure","storage","sas","keys","","provide","fine","grained","timed","access","containers","blobs","","providing","segway","supporting","wasb","secure","hadoop","cluster","","details","issue","proposal","provided","design","proposal","document","azure","","add","new","sas","key","mode","wasb"],"features":{"type":0,"size":1000,"indices":[18,22,25,30,36,74,82,91,128,138,170,181,189,197,198,207,215,224,262,281,288,296,330,333,343,355,363,372,373,374,384,388,394,431,432,445,461,464,479,495,496,519,522,535,547,572,595,597,605,629,656,691,695,698,709,710,724,731,734,736,748,760,775,777,799,810,813,820,821,833,873,891,897,899,904,909,931,960,992,994,996],"values":[1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,2.0,1.0,10.0,1.0,1.0,1.0,4.0,3.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,9.0,2.0,2.0,7.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":19}
{"_c0":"Currently    ViewFileSystem   does not dispatch snapshot methods through the mount table  All snapshot methods throw   UnsupportedOperationException    even though the underlying mount points could be HDFS instances that support snapshots  We need to update   ViewFileSystem   to implement the snapshot methods","_c1":"ViewFileSystem should support snapshot methods","document":"Currently    ViewFileSystem   does not dispatch snapshot methods through the mount table  All snapshot methods throw   UnsupportedOperationException    even though the underlying mount points could be HDFS instances that support snapshots  We need to update   ViewFileSystem   to implement the snapshot methods ViewFileSystem should support snapshot methods","words":["currently","","","","viewfilesystem","","","does","not","dispatch","snapshot","methods","through","the","mount","table","","all","snapshot","methods","throw","","","unsupportedoperationexception","","","","even","though","the","underlying","mount","points","could","be","hdfs","instances","that","support","snapshots","","we","need","to","update","","","viewfilesystem","","","to","implement","the","snapshot","methods","viewfilesystem","should","support","snapshot","methods"],"filtered":["currently","","","","viewfilesystem","","","dispatch","snapshot","methods","mount","table","","snapshot","methods","throw","","","unsupportedoperationexception","","","","even","though","underlying","mount","points","hdfs","instances","support","snapshots","","need","update","","","viewfilesystem","","","implement","snapshot","methods","viewfilesystem","support","snapshot","methods"],"features":{"type":0,"size":1000,"indices":[18,129,139,213,263,267,317,326,343,372,388,406,472,520,537,543,579,612,656,665,695,698,710,760,763,787,796,837,945,967,968,993],"values":[1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,16.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Currently  KMS audit log is using log j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible","_c1":"Allow pluggable audit loggers in KMS","document":"Currently  KMS audit log is using log j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible Allow pluggable audit loggers in KMS","words":["currently","","kms","audit","log","is","using","log","j","","to","write","a","text","format","log","","we","should","refactor","this","","so","that","people","can","easily","add","new","format","audit","logs","","the","current","text","format","log","should","be","the","default","","and","all","of","its","behavior","should","remain","compatible","allow","pluggable","audit","loggers","in","kms"],"filtered":["currently","","kms","audit","log","using","log","j","","write","text","format","log","","refactor","","people","easily","add","new","format","audit","logs","","current","text","format","log","default","","behavior","remain","compatible","allow","pluggable","audit","loggers","kms"],"features":{"type":0,"size":1000,"indices":[25,113,149,169,170,222,231,281,296,333,343,368,372,373,381,388,424,432,445,476,486,533,615,623,624,631,646,656,665,710,735,760,763,766,832,833,876,968,993],"values":[1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently  There will be ConvertToSafe for PythonUDF  that s not needed actually","_c1":"PythonUDF could process UnsafeRow","document":"Currently  There will be ConvertToSafe for PythonUDF  that s not needed actually PythonUDF could process UnsafeRow","words":["currently","","there","will","be","converttosafe","for","pythonudf","","that","s","not","needed","actually","pythonudf","could","process","unsaferow"],"filtered":["currently","","converttosafe","pythonudf","","needed","actually","pythonudf","process","unsaferow"],"features":{"type":0,"size":1000,"indices":[18,22,36,175,197,213,244,372,420,447,656,760,763,822,831,973],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Currently  downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S A need to list dependencies on each one  This creates an ongoing maintenance burden for those projects  because they need to update their build whenever a new Hadoop compatible file system is introduced  This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems  Similar to hadoop client  this new artifact will consist of just a pom xml listing the individual dependencies  Downstream users can depend on this artifact to sweep in everything  and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version","_c1":"Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop","document":"Currently  downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S A need to list dependencies on each one  This creates an ongoing maintenance burden for those projects  because they need to update their build whenever a new Hadoop compatible file system is introduced  This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems  Similar to hadoop client  this new artifact will consist of just a pom xml listing the individual dependencies  Downstream users can depend on this artifact to sweep in everything  and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop","words":["currently","","downstream","projects","that","want","to","integrate","with","different","hadoop","compatible","file","systems","like","wasb","and","s","a","need","to","list","dependencies","on","each","one","","this","creates","an","ongoing","maintenance","burden","for","those","projects","","because","they","need","to","update","their","build","whenever","a","new","hadoop","compatible","file","system","is","introduced","","this","issue","proposes","adding","a","new","artifact","that","transitively","includes","all","hadoop","compatible","file","systems","","similar","to","hadoop","client","","this","new","artifact","will","consist","of","just","a","pom","xml","listing","the","individual","dependencies","","downstream","users","can","depend","on","this","artifact","to","sweep","in","everything","","and","picking","up","a","new","file","system","in","a","future","version","will","be","just","a","matter","of","updating","the","hadoop","dependency","version","provide","a","unified","dependency","artifact","that","transitively","includes","the","cloud","storage","modules","shipped","with","hadoop"],"filtered":["currently","","downstream","projects","want","integrate","different","hadoop","compatible","file","systems","like","wasb","need","list","dependencies","one","","creates","ongoing","maintenance","burden","projects","","need","update","build","whenever","new","hadoop","compatible","file","system","introduced","","issue","proposes","adding","new","artifact","transitively","includes","hadoop","compatible","file","systems","","similar","hadoop","client","","new","artifact","consist","pom","xml","listing","individual","dependencies","","downstream","users","depend","artifact","sweep","everything","","picking","new","file","system","future","version","matter","updating","hadoop","dependency","version","provide","unified","dependency","artifact","transitively","includes","cloud","storage","modules","shipped","hadoop"],"features":{"type":0,"size":1000,"indices":[18,25,36,44,48,55,82,89,91,92,108,128,135,157,170,181,196,197,225,231,235,266,281,288,307,330,333,337,343,346,360,372,373,388,394,399,420,421,436,437,445,465,476,482,487,536,537,552,558,588,591,600,639,650,656,710,712,713,728,734,748,752,755,760,763,787,792,833,852,885,910,922,924,968,981,995],"values":[1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,8.0,6.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,3.0,2.0,1.0,8.0,4.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":15}
{"_c0":"Currently  many functions do now show usages like the followings     The only exceptions are  cube    grouping    grouping id    rollup    window","_c1":"All functions should show usages by command  DESC FUNCTION","document":"Currently  many functions do now show usages like the followings     The only exceptions are  cube    grouping    grouping id    rollup    window All functions should show usages by command  DESC FUNCTION","words":["currently","","many","functions","do","now","show","usages","like","the","followings","","","","","the","only","exceptions","are","","cube","","","","grouping","","","","grouping","id","","","","rollup","","","","window","all","functions","should","show","usages","by","command","","desc","function"],"filtered":["currently","","many","functions","show","usages","like","followings","","","","","exceptions","","cube","","","","grouping","","","","grouping","id","","","","rollup","","","","window","functions","show","usages","command","","desc","function"],"features":{"type":0,"size":1000,"indices":[2,17,19,98,127,135,138,188,223,313,330,372,396,428,492,511,534,587,665,710,763,781,825,899,968],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,19.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Currently  the Future returned by ipc async call only support Future get   but not Future get timeout  unit   We should support the latter as well","_c1":"Support Future get with timeout in ipc async calls","document":"Currently  the Future returned by ipc async call only support Future get   but not Future get timeout  unit   We should support the latter as well Support Future get with timeout in ipc async calls","words":["currently","","the","future","returned","by","ipc","async","call","only","support","future","get","","","but","not","future","get","timeout","","unit","","","we","should","support","the","latter","as","well","support","future","get","with","timeout","in","ipc","async","calls"],"filtered":["currently","","future","returned","ipc","async","call","support","future","get","","","future","get","timeout","","unit","","","support","latter","well","support","future","get","timeout","ipc","async","calls"],"features":{"type":0,"size":1000,"indices":[18,54,55,83,146,157,166,223,335,372,445,456,474,483,572,650,665,695,710,763,866,899,959,993],"values":[1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0]},"cluster_label":2}
{"_c0":"Currently  there are a few reports about Spark     query performance regression for large queries  This issue speeds up SQL query processing performance by removing redundant consecutive  executePlan  call in  Dataset ofRows  function and  Dataset  instantiation  Specifically  this issue aims to reduce the overhead of SQL query execution plan generation  not real query execution  So  we can not see the result in the Spark Web UI  Please use the following query script    Before","_c1":"Speed up SQL query performance by removing redundant  executePlan  call in  Dataset","document":"Currently  there are a few reports about Spark     query performance regression for large queries  This issue speeds up SQL query processing performance by removing redundant consecutive  executePlan  call in  Dataset ofRows  function and  Dataset  instantiation  Specifically  this issue aims to reduce the overhead of SQL query execution plan generation  not real query execution  So  we can not see the result in the Spark Web UI  Please use the following query script    Before Speed up SQL query performance by removing redundant  executePlan  call in  Dataset","words":["currently","","there","are","a","few","reports","about","spark","","","","","query","performance","regression","for","large","queries","","this","issue","speeds","up","sql","query","processing","performance","by","removing","redundant","consecutive","","executeplan","","call","in","","dataset","ofrows","","function","and","","dataset","","instantiation","","specifically","","this","issue","aims","to","reduce","the","overhead","of","sql","query","execution","plan","generation","","not","real","query","execution","","so","","we","can","not","see","the","result","in","the","spark","web","ui","","please","use","the","following","query","script","","","","before","speed","up","sql","query","performance","by","removing","redundant","","executeplan","","call","in","","dataset"],"filtered":["currently","","reports","spark","","","","","query","performance","regression","large","queries","","issue","speeds","sql","query","processing","performance","removing","redundant","consecutive","","executeplan","","call","","dataset","ofrows","","function","","dataset","","instantiation","","specifically","","issue","aims","reduce","overhead","sql","query","execution","plan","generation","","real","query","execution","","","see","result","spark","web","ui","","please","use","following","query","script","","","","speed","sql","query","performance","removing","redundant","","executeplan","","call","","dataset"],"features":{"type":0,"size":1000,"indices":[18,36,91,105,111,114,117,123,128,138,146,159,170,198,223,242,284,296,313,333,343,350,366,368,372,373,388,400,415,445,458,489,493,502,515,585,626,686,691,695,703,710,748,758,759,763,764,777,782,831,833,841,844,865,968,993],"values":[2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,6.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,24.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,4.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":3}
{"_c0":"Currently  we don t support using DecimalType with precision      in new unsafe aggregation  it s good to support it","_c1":"Support update DecimalType with precision      in UnsafeRow","document":"Currently  we don t support using DecimalType with precision      in new unsafe aggregation  it s good to support it Support update DecimalType with precision      in UnsafeRow","words":["currently","","we","don","t","support","using","decimaltype","with","precision","","","","","","in","new","unsafe","aggregation","","it","s","good","to","support","it","support","update","decimaltype","with","precision","","","","","","in","unsaferow"],"filtered":["currently","","support","using","decimaltype","precision","","","","","","new","unsafe","aggregation","","good","support","support","update","decimaltype","precision","","","","","","unsaferow"],"features":{"type":0,"size":1000,"indices":[25,168,175,197,204,242,288,343,372,388,445,495,624,650,695,763,777,811,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,12.0,1.0,2.0,2.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Currently  we try to support multiple sessions in SQL within a Spark Context  but it s broken and not complete  We should isolate these for each session      current database of Hive    SQLConf    UDF UDAF UDTF    temporary table For added jar and cached tables  they should be accessible for all sessions","_c1":"Improve session management for SQL","document":"Currently  we try to support multiple sessions in SQL within a Spark Context  but it s broken and not complete  We should isolate these for each session      current database of Hive    SQLConf    UDF UDAF UDTF    temporary table For added jar and cached tables  they should be accessible for all sessions Improve session management for SQL","words":["currently","","we","try","to","support","multiple","sessions","in","sql","within","a","spark","context","","but","it","s","broken","and","not","complete","","we","should","isolate","these","for","each","session","","","","","","current","database","of","hive","","","","sqlconf","","","","udf","udaf","udtf","","","","temporary","table","for","added","jar","and","cached","tables","","they","should","be","accessible","for","all","sessions","improve","session","management","for","sql"],"filtered":["currently","","try","support","multiple","sessions","sql","within","spark","context","","broken","complete","","isolate","session","","","","","","current","database","hive","","","","sqlconf","","","","udf","udaf","udtf","","","","temporary","table","added","jar","cached","tables","","accessible","sessions","improve","session","management","sql"],"features":{"type":0,"size":1000,"indices":[18,36,48,83,105,159,170,197,263,327,333,343,372,384,388,405,412,445,450,461,482,495,502,522,523,592,593,599,603,656,665,686,695,710,763,799,809,826,837,858,885,915,942,968,992,993],"values":[1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,18.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"Currently FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp  Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS  For example  we can use them to identify jobs that end up creating a large number of files  Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary","_c1":"Add a new interface for retrieving FS and FC Statistics","document":"Currently FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp  Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS  For example  we can use them to identify jobs that end up creating a large number of files  Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary Add a new interface for retrieving FS and FC Statistics","words":["currently","filesystem","statistics","exposes","the","following","statistics","","bytesread","byteswritten","readops","largereadops","writeops","these","are","in","turn","exposed","as","job","counters","by","mapreduce","and","other","frameworks","","there","is","logic","within","dfsclient","to","map","operations","to","these","counters","that","can","be","confusing","","for","instance","","mkdirs","counts","as","a","writeop","","proposed","enhancement","","add","a","statistic","for","each","dfsclient","operation","including","create","","append","","createsymlink","","delete","","exists","","mkdirs","","rename","and","expose","them","as","new","properties","on","the","statistics","object","","the","operation","specific","counters","can","be","used","for","analyzing","the","load","imposed","by","a","particular","job","on","hdfs","","for","example","","we","can","use","them","to","identify","jobs","that","end","up","creating","a","large","number","of","files","","once","this","information","is","available","in","the","statistics","object","","the","app","frameworks","like","mapreduce","can","expose","them","as","additional","counters","to","be","aggregated","and","recorded","as","part","of","job","summary","add","a","new","interface","for","retrieving","fs","and","fc","statistics"],"filtered":["currently","filesystem","statistics","exposes","following","statistics","","bytesread","byteswritten","readops","largereadops","writeops","turn","exposed","job","counters","mapreduce","frameworks","","logic","within","dfsclient","map","operations","counters","confusing","","instance","","mkdirs","counts","writeop","","proposed","enhancement","","add","statistic","dfsclient","operation","including","create","","append","","createsymlink","","delete","","exists","","mkdirs","","rename","expose","new","properties","statistics","object","","operation","specific","counters","used","analyzing","load","imposed","particular","job","hdfs","","example","","use","identify","jobs","end","creating","large","number","files","","information","available","statistics","object","","app","frameworks","like","mapreduce","expose","additional","counters","aggregated","recorded","part","job","summary","add","new","interface","retrieving","fs","fc","statistics"],"features":{"type":0,"size":1000,"indices":[25,28,36,53,58,59,82,91,94,109,128,135,138,142,143,147,170,202,223,243,250,258,265,281,284,304,321,330,333,341,343,344,349,351,357,364,371,372,373,385,388,401,412,425,432,445,461,470,489,522,551,556,572,573,583,586,605,609,612,621,650,656,664,674,687,690,697,710,735,737,740,754,755,760,763,767,782,801,831,833,867,885,886,909,916,924,953,954,967,978,993],"values":[2.0,1.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,17.0,2.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,3.0,3.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Currently FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach","_c1":"Change untar to use Java API on Windows instead of spawning tar process","document":"Currently FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach Change untar to use Java API on Windows instead of spawning tar process","words":["currently","fileutil","untar","","","spawns","tar","utility","to","do","the","work","","tar","may","not","be","present","on","all","platforms","by","default","eg","","windows","","so","changing","this","to","use","java","api","s","would","help","make","it","more","cross","platform","","fileutil","unzip","","","uses","the","same","approach","change","untar","to","use","java","api","on","windows","instead","of","spawning","tar","process"],"filtered":["currently","fileutil","untar","","","spawns","tar","utility","work","","tar","may","present","platforms","default","eg","","windows","","changing","use","java","api","help","make","cross","platform","","fileutil","unzip","","","uses","approach","change","untar","use","java","api","windows","instead","spawning","tar","process"],"features":{"type":0,"size":1000,"indices":[9,18,22,82,95,109,111,158,163,167,197,223,280,285,335,343,347,368,370,372,373,381,388,433,489,495,525,527,534,570,629,644,649,656,666,710,731,763,766,798,809,863,967,968],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Currently Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again  So  this JIRA requests two functionalities     Support for external cluster managers    Allow a cluster manager to clean up the tasks without taking the parent process down","_c1":"Add support for pluggable cluster manager","document":"Currently Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again  So  this JIRA requests two functionalities     Support for external cluster managers    Allow a cluster manager to clean up the tasks without taking the parent process down Add support for pluggable cluster manager","words":["currently","spark","allows","only","a","few","cluster","managers","viz","yarn","","mesos","and","standalone","","but","","as","spark","is","now","being","used","in","newer","and","different","use","cases","","there","is","a","need","for","allowing","other","cluster","managers","to","manage","spark","components","","one","such","use","case","is","","","embedding","spark","components","like","executor","and","driver","inside","another","process","which","may","be","a","datastore","","this","allows","colocation","of","data","and","processing","","another","requirement","that","stems","from","such","a","use","case","is","that","the","executors","driver","should","not","take","the","parent","process","down","when","they","go","down","and","the","components","can","be","relaunched","inside","the","same","process","again","","so","","this","jira","requests","two","functionalities","","","","","support","for","external","cluster","managers","","","","allow","a","cluster","manager","to","clean","up","the","tasks","without","taking","the","parent","process","down","add","support","for","pluggable","cluster","manager"],"filtered":["currently","spark","allows","cluster","managers","viz","yarn","","mesos","standalone","","","spark","used","newer","different","use","cases","","need","allowing","cluster","managers","manage","spark","components","","one","use","case","","","embedding","spark","components","like","executor","driver","inside","another","process","may","datastore","","allows","colocation","data","processing","","another","requirement","stems","use","case","executors","driver","take","parent","process","go","components","relaunched","inside","process","","","jira","requests","two","functionalities","","","","","support","external","cluster","managers","","","","allow","cluster","manager","clean","tasks","without","taking","parent","process","add","support","pluggable","cluster","manager"],"features":{"type":0,"size":1000,"indices":[11,18,22,36,44,48,66,71,76,77,83,89,98,105,128,135,139,141,170,185,217,231,235,272,281,330,333,340,342,343,359,368,372,373,374,388,393,395,400,408,424,432,439,441,445,452,460,463,480,487,489,522,537,547,564,572,576,585,586,597,605,656,665,666,668,669,674,695,710,760,763,779,800,801,821,831,833,855,866,884,899,921,965],"values":[1.0,1.0,4.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,1.0,5.0,1.0,3.0,1.0,1.0,2.0,4.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,18.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,3.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,3.0,6.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule  This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem  This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods","_c1":"Cleanup TestFilterFileSystem","document":"Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule  This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem  This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods Cleanup TestFilterFileSystem","words":["currently","testfilterfilesystem","only","checks","for","filesystem","methods","that","must","be","implemented","in","filterfilesystem","with","a","list","of","methods","that","are","exception","to","this","rule","","this","jira","wants","to","make","this","check","stricter","by","adding","a","test","for","ensuring","the","methods","in","exception","rule","list","must","not","be","implemented","by","the","filterfilesystem","","this","also","cleans","up","the","current","class","that","has","methods","from","exception","rule","list","to","interface","to","avoid","having","to","provide","dummy","implementation","of","the","methods","cleanup","testfilterfilesystem"],"filtered":["currently","testfilterfilesystem","checks","filesystem","methods","must","implemented","filterfilesystem","list","methods","exception","rule","","jira","wants","make","check","stricter","adding","test","ensuring","methods","exception","rule","list","must","implemented","filterfilesystem","","also","cleans","current","class","methods","exception","rule","list","interface","avoid","provide","dummy","implementation","methods","cleanup","testfilterfilesystem"],"features":{"type":0,"size":1000,"indices":[18,23,36,62,109,123,128,129,138,170,177,223,231,288,343,364,372,373,388,445,525,534,536,556,580,586,591,593,626,650,656,675,698,710,728,745,760,763,792,821,837,872,878,882,899,921],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,2.0,2.0,1.0,1.0,5.0,1.0,2.0,4.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,5.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently back off policy from HADOOP       is hard coded to base on whether call queue is full  This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities","_c1":"Allow RPC scheduler callqueue backoff using response times","document":"Currently back off policy from HADOOP       is hard coded to base on whether call queue is full  This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities Allow RPC scheduler callqueue backoff using response times","words":["currently","back","off","policy","from","hadoop","","","","","","","is","hard","coded","to","base","on","whether","call","queue","is","full","","this","ticket","is","open","to","allow","flexible","back","off","policies","such","as","moving","average","of","response","time","in","rpc","calls","of","different","priorities","allow","rpc","scheduler","callqueue","backoff","using","response","times"],"filtered":["currently","back","policy","hadoop","","","","","","","hard","coded","base","whether","call","queue","full","","ticket","open","allow","flexible","back","policies","moving","average","response","time","rpc","calls","different","priorities","allow","rpc","scheduler","callqueue","backoff","using","response","times"],"features":{"type":0,"size":1000,"indices":[82,89,125,146,157,173,181,231,263,272,281,343,372,373,388,424,430,443,445,497,513,572,591,607,624,674,738,763,783,807,810,846,849,866,897,921,978,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,2.0,7.0,1.0,2.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Currently filters for   TimestampType   and   DecimalType   are not being pushed down in ORC data source although ORC filters support both","_c1":"Support for pushing down filters for decimal and timestamp types in ORC","document":"Currently filters for   TimestampType   and   DecimalType   are not being pushed down in ORC data source although ORC filters support both Support for pushing down filters for decimal and timestamp types in ORC","words":["currently","filters","for","","","timestamptype","","","and","","","decimaltype","","","are","not","being","pushed","down","in","orc","data","source","although","orc","filters","support","both","support","for","pushing","down","filters","for","decimal","and","timestamp","types","in","orc"],"filtered":["currently","filters","","","timestamptype","","","","","decimaltype","","","pushed","orc","data","source","although","orc","filters","support","support","pushing","filters","decimal","timestamp","types","orc"],"features":{"type":0,"size":1000,"indices":[18,36,70,138,217,333,343,372,374,381,382,419,436,445,465,677,695,741,749,763,779,863],"values":[1.0,3.0,1.0,1.0,2.0,2.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Currently if the value of ipc client rpc timeout ms is greater than    the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed  RPC timeout should work without effectively disabling IPC ping","_c1":"RPC timeout should not override IPC ping interval","document":"Currently if the value of ipc client rpc timeout ms is greater than    the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed  RPC timeout should work without effectively disabling IPC ping RPC timeout should not override IPC ping interval","words":["currently","if","the","value","of","ipc","client","rpc","timeout","ms","is","greater","than","","","","the","timeout","overrides","the","ipc","ping","interval","and","client","will","throw","exception","instead","of","sending","ping","when","the","interval","is","passed","","rpc","timeout","should","work","without","effectively","disabling","ipc","ping","rpc","timeout","should","not","override","ipc","ping","interval"],"filtered":["currently","value","ipc","client","rpc","timeout","ms","greater","","","","timeout","overrides","ipc","ping","interval","client","throw","exception","instead","sending","ping","interval","passed","","rpc","timeout","work","without","effectively","disabling","ipc","ping","rpc","timeout","override","ipc","ping","interval"],"features":{"type":0,"size":1000,"indices":[18,76,120,122,135,170,181,235,261,267,281,315,333,343,372,420,447,456,462,483,497,527,593,665,710,763,768,848,863,884,888],"values":[1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,4.0,4.0,1.0,4.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently max queue size for IPC server is set to        handlers   Usually when RPC failures are observed  e g  HADOOP        we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than       There are other improvements also  HADOOP        Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic  Say handler count is     default  and Server can handle      RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for   sec before it is dropped  If there      clients and all of them send RPCs around the same time  not very rare  with heartbeats etc        will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management  For this jira I propose to make queue size per handler configurable  with a larger default  may be","_c1":"IPC server max queue size should be configurable","document":"Currently max queue size for IPC server is set to        handlers   Usually when RPC failures are observed  e g  HADOOP        we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than       There are other improvements also  HADOOP        Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic  Say handler count is     default  and Server can handle      RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for   sec before it is dropped  If there      clients and all of them send RPCs around the same time  not very rare  with heartbeats etc        will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management  For this jira I propose to make queue size per handler configurable  with a larger default  may be IPC server max queue size should be configurable","words":["currently","max","queue","size","for","ipc","server","is","set","to","","","","","","","","handlers","","","usually","when","rpc","failures","are","observed","","e","g","","hadoop","","","","","","","","we","increase","number","of","handlers","and","the","problem","goes","away","","i","think","a","big","part","of","such","a","fix","is","increase","in","max","queue","size","","i","think","we","should","make","maxqsize","per","handler","configurable","","with","a","bigger","default","than","","","","","","","there","are","other","improvements","also","","hadoop","","","","","","","","server","keeps","reading","rpc","requests","from","clients","","when","the","number","in","flight","rpcs","is","larger","than","maxqsize","","the","earliest","rpcs","are","deleted","","this","is","the","main","feedback","server","has","for","the","client","","i","have","often","heard","from","users","that","hadoop","doesn","t","handle","bursty","traffic","","say","handler","count","is","","","","","default","","and","server","can","handle","","","","","","rpcs","a","sec","","quite","conservative","low","for","a","typical","server","","","it","implies","that","an","rpc","can","wait","for","only","for","","","sec","before","it","is","dropped","","if","there","","","","","","clients","and","all","of","them","send","rpcs","around","the","same","time","","not","very","rare","","with","heartbeats","etc","","","","","","","","will","be","dropped","","in","stead","of","dropping","the","earliest","rpcs","","if","the","server","delays","reading","new","rpcs","","the","feedback","to","clients","would","be","much","smoother","","i","will","file","another","jira","regd","queue","management","","for","this","jira","i","propose","to","make","queue","size","per","handler","configurable","","with","a","larger","default","","may","be","ipc","server","max","queue","size","should","be","configurable"],"filtered":["currently","max","queue","size","ipc","server","set","","","","","","","","handlers","","","usually","rpc","failures","observed","","e","g","","hadoop","","","","","","","","increase","number","handlers","problem","goes","away","","think","big","part","fix","increase","max","queue","size","","think","make","maxqsize","per","handler","configurable","","bigger","default","","","","","","","improvements","also","","hadoop","","","","","","","","server","keeps","reading","rpc","requests","clients","","number","flight","rpcs","larger","maxqsize","","earliest","rpcs","deleted","","main","feedback","server","client","","often","heard","users","hadoop","doesn","handle","bursty","traffic","","say","handler","count","","","","","default","","server","handle","","","","","","rpcs","sec","","quite","conservative","low","typical","server","","","implies","rpc","wait","","","sec","dropped","","","","","","","clients","send","rpcs","around","time","","rare","","heartbeats","etc","","","","","","","","dropped","","stead","dropping","earliest","rpcs","","server","delays","reading","new","rpcs","","feedback","clients","much","smoother","","file","another","jira","regd","queue","management","","jira","propose","make","queue","size","per","handler","configurable","","larger","default","","may","ipc","server","max","queue","size","configurable"],"features":{"type":0,"size":1000,"indices":[18,25,28,36,56,58,71,76,88,92,94,101,108,110,135,138,157,159,163,170,181,192,220,229,255,261,272,280,281,283,299,313,329,330,333,340,343,357,372,373,381,383,388,393,401,405,411,415,417,420,424,440,445,452,460,477,483,495,500,510,521,523,524,525,552,564,580,583,598,604,618,622,625,637,650,655,656,665,666,674,693,710,724,737,740,752,755,760,763,771,777,779,780,792,806,809,813,821,831,833,852,871,878,899,921,924,934,944,963,964,968,993],"values":[1.0,1.0,2.0,6.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,8.0,6.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,6.0,3.0,1.0,3.0,5.0,1.0,3.0,1.0,4.0,1.0,77.0,2.0,3.0,2.0,3.0,2.0,1.0,1.0,7.0,2.0,1.0,2.0,5.0,2.0,4.0,1.0,2.0,1.0,2.0,2.0,1.0,7.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,1.0,9.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,2.0]},"cluster_label":11}
{"_c0":"Currently the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types","_c1":"Support to specify join type when calling join with usingColumns","document":"Currently the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types Support to specify join type when calling join with usingColumns","words":["currently","the","method","join","right","","dataframe","","usingcolumns","","seq","string","","","only","supports","inner","join","","it","is","more","convenient","to","have","it","support","other","join","types","support","to","specify","join","type","when","calling","join","with","usingcolumns"],"filtered":["currently","method","join","right","","dataframe","","usingcolumns","","seq","string","","","supports","inner","join","","convenient","support","join","types","support","specify","join","type","calling","join","usingcolumns"],"features":{"type":0,"size":1000,"indices":[76,161,193,281,299,372,388,410,465,495,526,574,583,629,650,654,674,695,710,730,763,800,888,899,952,991,994],"values":[1.0,1.0,1.0,1.0,1.0,6.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones","_c1":"Add support for launching multiple Mesos dispatchers","document":"Currently the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones Add support for launching multiple Mesos dispatchers","words":["currently","the","sbin","","start","stop","","mesos","dispatcher","scripts","only","assume","there","is","one","mesos","dispatcher","launched","","but","potentially","users","that","like","to","run","multi","tenant","dispatcher","might","want","to","launch","multiples","","it","also","helps","local","development","to","have","the","ability","to","launch","multiple","ones","add","support","for","launching","multiple","mesos","dispatchers"],"filtered":["currently","sbin","","start","stop","","mesos","dispatcher","scripts","assume","one","mesos","dispatcher","launched","","potentially","users","like","run","multi","tenant","dispatcher","might","want","launch","multiples","","also","helps","local","development","ability","launch","multiple","ones","add","support","launching","multiple","mesos","dispatchers"],"features":{"type":0,"size":1000,"indices":[1,36,44,83,281,299,311,312,327,330,331,356,364,365,372,388,432,433,495,498,514,567,573,592,594,608,631,695,710,712,753,755,760,763,792,796,831,899,965,985,996],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently we support read df  write df  jsonFile  parquetFile in SQLContext  we should support more external data source API such as read json  read parquet  read orc  read jdbc  read csv and so on  Some of the exist API is deprecated and will remove at Spark      we should also deprecate them at SparkR  Note  we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style  DataFrameReader API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameWriter","_c1":"Support more external data source API in SparkR","document":"Currently we support read df  write df  jsonFile  parquetFile in SQLContext  we should support more external data source API such as read json  read parquet  read orc  read jdbc  read csv and so on  Some of the exist API is deprecated and will remove at Spark      we should also deprecate them at SparkR  Note  we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style  DataFrameReader API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameWriter Support more external data source API in SparkR","words":["currently","we","support","read","df","","write","df","","jsonfile","","parquetfile","in","sqlcontext","","we","should","support","more","external","data","source","api","such","as","read","json","","read","parquet","","read","orc","","read","jdbc","","read","csv","and","so","on","","some","of","the","exist","api","is","deprecated","and","will","remove","at","spark","","","","","","we","should","also","deprecate","them","at","sparkr","","note","","we","should","refer","the","dataframereader","and","dataframewriter","api","at","spark","sql","but","defined","with","r","like","style","","dataframereader","api","","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframereader","dataframewriter","api","","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframewriter","support","more","external","data","source","api","in","sparkr"],"filtered":["currently","support","read","df","","write","df","","jsonfile","","parquetfile","sqlcontext","","support","external","data","source","api","read","json","","read","parquet","","read","orc","","read","jdbc","","read","csv","","exist","api","deprecated","remove","spark","","","","","","also","deprecate","sparkr","","note","","refer","dataframereader","dataframewriter","api","spark","sql","defined","r","like","style","","dataframereader","api","","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframereader","dataframewriter","api","","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframewriter","support","external","data","source","api","sparkr"],"features":{"type":0,"size":1000,"indices":[40,70,82,83,105,113,119,126,144,145,172,210,256,272,275,281,288,307,323,330,333,343,368,372,400,420,445,451,490,492,495,498,535,545,570,572,586,620,629,644,650,652,662,665,686,695,710,749,756,763,767,790,792,909,924,957,993],"values":[1.0,2.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,23.0,1.0,1.0,2.0,1.0,2.0,1.0,4.0,2.0,4.0,2.0,1.0,1.0,2.0,1.0,2.0,8.0,7.0,2.0,1.0,5.0,3.0,5.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0]},"cluster_label":3}
{"_c0":"Currently when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes","_c1":"Improve ActiveStandbyElector s behavior when session expires","document":"Currently when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes Improve ActiveStandbyElector s behavior when session expires","words":["currently","when","the","zk","session","expires","","it","results","in","a","fatal","error","being","sent","to","the","application","callback","","this","is","not","the","best","behavior","","","","for","example","","in","the","case","of","ha","","if","zk","goes","down","","we","would","like","the","current","state","to","be","maintained","","rather","than","causing","either","nn","to","abort","","when","the","zk","clients","are","able","to","reconnect","","they","should","sort","out","the","correct","leader","based","on","the","normal","locking","schemes","improve","activestandbyelector","s","behavior","when","session","expires"],"filtered":["currently","zk","session","expires","","results","fatal","error","sent","application","callback","","best","behavior","","","","example","","case","ha","","zk","goes","","like","current","state","maintained","","rather","causing","either","nn","abort","","zk","clients","able","reconnect","","sort","correct","leader","based","normal","locking","schemes","improve","activestandbyelector","behavior","session","expires"],"features":{"type":0,"size":1000,"indices":[18,20,36,48,76,82,86,89,116,117,138,163,168,169,170,197,217,243,261,281,286,330,333,340,342,343,349,363,372,373,374,388,389,405,412,437,445,495,496,507,522,572,625,649,654,656,658,665,710,714,720,724,735,750,763,883,993,999],"values":[1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":19}
{"_c0":"Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination  We should support it","_c1":"Subexpression elimination in wholestage codegen version of TungstenAggregate","document":"Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination  We should support it Subexpression elimination in wholestage codegen version of TungstenAggregate","words":["currently","wholestage","codegen","version","of","tungstenaggregate","does","not","support","subexpression","elimination","","we","should","support","it","subexpression","elimination","in","wholestage","codegen","version","of","tungstenaggregate"],"filtered":["currently","wholestage","codegen","version","tungstenaggregate","support","subexpression","elimination","","support","subexpression","elimination","wholestage","codegen","version","tungstenaggregate"],"features":{"type":0,"size":1000,"indices":[18,263,297,343,372,445,495,621,665,695,698,727,763,794,993,995],"values":[1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Currently with quota turned on  user cannot call   rmr  on large directory that causes over quota     Besides from error message being unfriendly  how should this be handled","_c1":"Handling of Trash with quota","document":"Currently with quota turned on  user cannot call   rmr  on large directory that causes over quota     Besides from error message being unfriendly  how should this be handled Handling of Trash with quota","words":["currently","with","quota","turned","on","","user","cannot","call","","","rmr","","on","large","directory","that","causes","over","quota","","","","","besides","from","error","message","being","unfriendly","","how","should","this","be","handled","handling","of","trash","with","quota"],"filtered":["currently","quota","turned","","user","call","","","rmr","","large","directory","causes","quota","","","","","besides","error","message","unfriendly","","handled","handling","trash","quota"],"features":{"type":0,"size":1000,"indices":[18,82,146,237,286,287,333,343,352,372,373,374,453,463,634,650,656,665,731,760,763,782,825,838,882,921,931,981,992],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"DaemonFactory class is defined in hdfs util  common would be a better place for this class","_c1":"DaemonFactory should be moved from HDFS to common","document":"DaemonFactory class is defined in hdfs util  common would be a better place for this class DaemonFactory should be moved from HDFS to common","words":["daemonfactory","class","is","defined","in","hdfs","util","","common","would","be","a","better","place","for","this","class","daemonfactory","should","be","moved","from","hdfs","to","common"],"filtered":["daemonfactory","class","defined","hdfs","util","","common","better","place","class","daemonfactory","moved","hdfs","common"],"features":{"type":0,"size":1000,"indices":[36,126,138,163,170,191,203,281,372,373,388,445,534,656,665,699,921,941,954,967],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion","_c1":"Remove the internal implicit conversion from LogicalPlan to DataFrame","document":"DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion Remove the internal implicit conversion from LogicalPlan to DataFrame","words":["dataframe","has","an","internal","implicit","conversion","that","turns","a","logicalplan","into","a","dataframe","","this","has","been","fairly","confusing","to","a","few","new","contributors","","since","it","doesn","t","buy","us","much","","we","should","just","remove","that","implicit","conversion","remove","the","internal","implicit","conversion","from","logicalplan","to","dataframe"],"filtered":["dataframe","internal","implicit","conversion","turns","logicalplan","dataframe","","fairly","confusing","new","contributors","","since","doesn","buy","us","much","","remove","implicit","conversion","remove","internal","implicit","conversion","logicalplan","dataframe"],"features":{"type":0,"size":1000,"indices":[25,110,161,170,208,213,250,288,295,307,372,373,388,400,420,471,495,500,515,524,535,577,580,585,665,710,752,760,777,795,891,921,993],"values":[1.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Dataset always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it","_c1":"Remove spark sql eagerAnalysis","document":"Dataset always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it Remove spark sql eagerAnalysis","words":["dataset","always","does","eager","analysis","now","","thus","","spark","sql","eageranalysis","is","not","used","any","more","","thus","","we","need","to","remove","it","remove","spark","sql","eageranalysis"],"filtered":["dataset","always","eager","analysis","","thus","","spark","sql","eageranalysis","used","","thus","","need","remove","remove","spark","sql","eageranalysis"],"features":{"type":0,"size":1000,"indices":[13,18,91,98,105,182,281,288,296,372,388,450,493,495,537,605,629,684,686,698,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Different from the other leaf nodes   MetastoreRelation  and  SimpleCatalogRelation  have a pre defined  alias   which is used to change the qualifier of the node  However  based on the existing alias handling  alias should be put in  SubqueryAlias   This PR is to separate alias handling from  MetastoreRelation  and  SimpleCatalogRelation  to make it consistent with the other nodes  For example  below is an example query for  MetastoreRelation   which is converted to  LogicalRelation        Note  the optimized plans are the same    For  SimpleCatalogRelation   the existing code always generates two Subqueries  Thus  no change is needed","_c1":"Remove Alias from MetastoreRelation and SimpleCatalogRelation","document":"Different from the other leaf nodes   MetastoreRelation  and  SimpleCatalogRelation  have a pre defined  alias   which is used to change the qualifier of the node  However  based on the existing alias handling  alias should be put in  SubqueryAlias   This PR is to separate alias handling from  MetastoreRelation  and  SimpleCatalogRelation  to make it consistent with the other nodes  For example  below is an example query for  MetastoreRelation   which is converted to  LogicalRelation        Note  the optimized plans are the same    For  SimpleCatalogRelation   the existing code always generates two Subqueries  Thus  no change is needed Remove Alias from MetastoreRelation and SimpleCatalogRelation","words":["different","from","the","other","leaf","nodes","","","metastorerelation","","and","","simplecatalogrelation","","have","a","pre","defined","","alias","","","which","is","used","to","change","the","qualifier","of","the","node","","however","","based","on","the","existing","alias","handling","","alias","should","be","put","in","","subqueryalias","","","this","pr","is","to","separate","alias","handling","from","","metastorerelation","","and","","simplecatalogrelation","","to","make","it","consistent","with","the","other","nodes","","for","example","","below","is","an","example","query","for","","metastorerelation","","","which","is","converted","to","","logicalrelation","","","","","","","","note","","the","optimized","plans","are","the","same","","","","for","","simplecatalogrelation","","","the","existing","code","always","generates","two","subqueries","","thus","","no","change","is","needed","remove","alias","from","metastorerelation","and","simplecatalogrelation"],"filtered":["different","leaf","nodes","","","metastorerelation","","","simplecatalogrelation","","pre","defined","","alias","","","used","change","qualifier","node","","however","","based","existing","alias","handling","","alias","put","","subqueryalias","","","pr","separate","alias","handling","","metastorerelation","","","simplecatalogrelation","","make","consistent","nodes","","example","","example","query","","metastorerelation","","","converted","","logicalrelation","","","","","","","","note","","optimized","plans","","","","","simplecatalogrelation","","","existing","code","always","generates","two","subqueries","","thus","","change","needed","remove","alias","metastorerelation","simplecatalogrelation"],"features":{"type":0,"size":1000,"indices":[13,36,54,57,64,82,89,91,126,138,158,170,240,242,243,244,281,287,288,299,333,343,346,362,371,372,373,388,399,408,418,420,445,482,495,525,597,605,607,617,625,634,650,656,665,669,673,674,684,710,718,752,825,865,897,909,921,954,973],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,5.0,5.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,40.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,8.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":16}
{"_c0":"DiskChecker can fail to detect total disk controller failures indefinitely  We have seen this in real clusters  DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted  A simple improvement is to write some data and flush it to the disk","_c1":"DiskChecker should perform some disk IO","document":"DiskChecker can fail to detect total disk controller failures indefinitely  We have seen this in real clusters  DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted  A simple improvement is to write some data and flush it to the disk DiskChecker should perform some disk IO","words":["diskchecker","can","fail","to","detect","total","disk","controller","failures","indefinitely","","we","have","seen","this","in","real","clusters","","diskchecker","performs","simple","permissions","based","checks","on","directories","which","do","not","guarantee","that","any","disk","io","will","be","attempted","","a","simple","improvement","is","to","write","some","data","and","flush","it","to","the","disk","diskchecker","should","perform","some","disk","io"],"filtered":["diskchecker","fail","detect","total","disk","controller","failures","indefinitely","","seen","real","clusters","","diskchecker","performs","simple","permissions","based","checks","directories","guarantee","disk","io","attempted","","simple","improvement","write","data","flush","disk","diskchecker","perform","disk","io"],"features":{"type":0,"size":1000,"indices":[6,18,82,91,113,170,229,281,299,315,329,330,332,333,346,372,373,388,400,420,424,445,495,532,534,568,571,597,625,640,656,665,680,691,695,710,745,760,765,807,817,833,838,850,980,993,994],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,4.0]},"cluster_label":2}
{"_c0":"During http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is    hours  For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated  The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be    minutes","_c1":"Enable MaxInactiveInterval for hadoop http auth token","document":"During http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is    hours  For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated  The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be    minutes Enable MaxInactiveInterval for hadoop http auth token","words":["during","http","authentication","","a","cookie","which","contains","the","authentication","token","is","dropped","","the","expiry","time","of","the","authentication","token","can","be","configured","via","hadoop","http","authentication","token","validity","","the","default","value","is","","","","hours","","for","clusters","which","require","enhanced","security","","it","is","desirable","to","have","a","configurable","maxinactiveinterval","for","the","authentication","token","","if","there","is","no","activity","during","maxinactiveinterval","","the","authentication","token","will","be","invalidated","","the","maxinactiveinterval","will","be","less","than","hadoop","http","authentication","token","validity","","the","default","value","will","be","","","","minutes","enable","maxinactiveinterval","for","hadoop","http","auth","token"],"filtered":["http","authentication","","cookie","contains","authentication","token","dropped","","expiry","time","authentication","token","configured","via","hadoop","http","authentication","token","validity","","default","value","","","","hours","","clusters","require","enhanced","security","","desirable","configurable","maxinactiveinterval","authentication","token","","activity","maxinactiveinterval","","authentication","token","invalidated","","maxinactiveinterval","less","hadoop","http","authentication","token","validity","","default","value","","","","minutes","enable","maxinactiveinterval","hadoop","http","auth","token"],"features":{"type":0,"size":1000,"indices":[36,48,110,132,146,157,170,181,195,229,261,266,277,280,281,299,332,343,346,372,381,388,420,495,528,580,586,595,597,610,634,656,664,665,688,696,710,721,768,806,831,833,964,992],"values":[3.0,1.0,1.0,1.0,7.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,15.0,2.0,1.0,3.0,1.0,7.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,4.0,4.0,1.0,8.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":12}
{"_c0":"Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs  The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request  Using JWT provides a number of benefits    It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens","_c1":"Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth","document":"Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs  The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request  Using JWT provides a number of benefits    It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth","words":["extend","altkerberosauthenticationhandler","to","provide","websso","flow","for","uis","","the","actual","authentication","is","done","by","some","external","service","that","the","handler","will","redirect","to","when","there","is","no","hadoop","auth","cookie","and","no","jwt","token","found","in","the","incoming","request","","using","jwt","provides","a","number","of","benefits","","","","it","is","not","tied","to","any","specific","authentication","mechanism","","","so","buys","us","many","sso","integrations","","","it","is","cryptographically","verifiable","for","determining","whether","it","can","be","trusted","","","checking","for","expiration","allows","for","a","limited","lifetime","and","window","for","compromised","use","this","will","introduce","the","use","of","nimbus","jose","jwt","library","for","processing","","validating","and","parsing","jwt","tokens","add","redirecting","websso","behavior","with","jwt","token","in","hadoop","auth"],"filtered":["extend","altkerberosauthenticationhandler","provide","websso","flow","uis","","actual","authentication","done","external","service","handler","redirect","hadoop","auth","cookie","jwt","token","found","incoming","request","","using","jwt","provides","number","benefits","","","","tied","specific","authentication","mechanism","","","buys","us","many","sso","integrations","","","cryptographically","verifiable","determining","whether","trusted","","","checking","expiration","allows","limited","lifetime","window","compromised","use","introduce","use","nimbus","jose","jwt","library","processing","","validating","parsing","jwt","tokens","add","redirecting","websso","behavior","jwt","token","hadoop","auth"],"features":{"type":0,"size":1000,"indices":[18,33,36,59,76,91,92,99,146,160,170,181,188,208,223,239,255,269,281,288,313,333,339,343,346,362,368,372,373,388,400,420,426,432,445,446,480,489,493,495,511,524,528,533,577,581,583,585,586,605,610,624,628,644,645,650,656,664,677,707,710,735,753,760,771,775,807,830,831,833,834,837,909,945,955],"values":[1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,2.0,13.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":19}
{"_c0":"FileSystem createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method  For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist","_c1":"Undeprecate createNonRecursive","document":"FileSystem createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method  For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist Undeprecate createNonRecursive","words":["filesystem","createnonrecursive","","","is","deprecated","","however","","there","is","no","distributedfilesystem","create","","","implementation","which","throws","exception","if","parent","directory","doesn","t","exist","","this","limits","clients","","migration","away","from","the","deprecated","method","","for","hbase","","io","fencing","relies","on","the","behavior","of","filesystem","createnonrecursive","","","","variant","of","create","","","method","should","be","added","which","throws","exception","if","parent","directory","doesn","t","exist","undeprecate","createnonrecursive"],"filtered":["filesystem","createnonrecursive","","","deprecated","","however","","distributedfilesystem","create","","","implementation","throws","exception","parent","directory","doesn","exist","","limits","clients","","migration","away","deprecated","method","","hbase","","io","fencing","relies","behavior","filesystem","createnonrecursive","","","","variant","create","","","method","added","throws","exception","parent","directory","doesn","exist","undeprecate","createnonrecursive"],"features":{"type":0,"size":1000,"indices":[29,36,66,82,144,156,170,209,265,281,340,343,346,364,372,373,384,500,512,593,597,599,620,654,656,660,665,673,689,698,710,724,735,737,764,777,831,838,921,962,992],"values":[1.0,1.0,2.0,1.0,2.0,3.0,2.0,2.0,2.0,2.0,1.0,2.0,1.0,2.0,15.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"Fix a todo in spark sql  remove    Command    and use    RunnableCommand    instead","_c1":"Refactory command in spark sql","document":"Fix a todo in spark sql  remove    Command    and use    RunnableCommand    instead Refactory command in spark sql","words":["fix","a","todo","in","spark","sql","","remove","","","","command","","","","and","use","","","","runnablecommand","","","","instead","refactory","command","in","spark","sql"],"filtered":["fix","todo","spark","sql","","remove","","","","command","","","","use","","","","runnablecommand","","","","instead","refactory","command","spark","sql"],"features":{"type":0,"size":1000,"indices":[105,135,170,218,288,333,372,445,489,686,863,915,939],"values":[2.0,2.0,1.0,1.0,1.0,1.0,13.0,3.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Following SPARK        we can add a wrapper for naive Bayes in SparkR  R s naive Bayes implementation is from package e     with signature     It should be easy for us to match the parameters","_c1":"Naive Bayes wrapper in SparkR","document":"Following SPARK        we can add a wrapper for naive Bayes in SparkR  R s naive Bayes implementation is from package e     with signature     It should be easy for us to match the parameters Naive Bayes wrapper in SparkR","words":["following","spark","","","","","","","","we","can","add","a","wrapper","for","naive","bayes","in","sparkr","","r","s","naive","bayes","implementation","is","from","package","e","","","","","with","signature","","","","","it","should","be","easy","for","us","to","match","the","parameters","naive","bayes","wrapper","in","sparkr"],"filtered":["following","spark","","","","","","","","add","wrapper","naive","bayes","sparkr","","r","naive","bayes","implementation","package","e","","","","","signature","","","","","easy","us","match","parameters","naive","bayes","wrapper","sparkr"],"features":{"type":0,"size":1000,"indices":[36,64,91,105,170,189,197,208,266,281,372,388,432,445,495,500,570,650,656,665,698,710,729,760,762,767,833,878,921,977,993],"values":[2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,16.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0]},"cluster_label":17}
{"_c0":"For HDFS     we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache   license","_c1":"Add support for unix domain sockets to JNI libs","document":"For HDFS     we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache   license Add support for unix domain sockets to JNI libs","words":["for","hdfs","","","","","we","need","to","use","unix","domain","sockets","","this","jira","is","to","include","a","library","in","common","which","adds","a","o","a","h","net","unix","package","based","on","the","code","from","android","","apache","","","license","add","support","for","unix","domain","sockets","to","jni","libs"],"filtered":["hdfs","","","","","need","use","unix","domain","sockets","","jira","include","library","common","adds","o","h","net","unix","package","based","code","android","","apache","","","license","add","support","unix","domain","sockets","jni","libs"],"features":{"type":0,"size":1000,"indices":[7,36,82,170,182,186,260,266,281,282,372,373,388,401,411,420,428,432,445,446,489,490,495,537,597,625,695,710,821,880,915,921,954,967,985,993],"values":[3.0,2.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,8.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"For some use cases  it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts  At here root SQLContext means the first SQLContext that gets created","_c1":"Introduce a mechanism to ban creating new root SQLContexts in a JVM","document":"For some use cases  it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts  At here root SQLContext means the first SQLContext that gets created Introduce a mechanism to ban creating new root SQLContexts in a JVM","words":["for","some","use","cases","","it","will","be","useful","to","explicitly","ban","creating","multiple","root","sqlcontexts","hivecontexts","","at","here","root","sqlcontext","means","the","first","sqlcontext","that","gets","created","introduce","a","mechanism","to","ban","creating","new","root","sqlcontexts","in","a","jvm"],"filtered":["use","cases","","useful","explicitly","ban","creating","multiple","root","sqlcontexts","hivecontexts","","root","sqlcontext","means","first","sqlcontext","gets","created","introduce","mechanism","ban","creating","new","root","sqlcontexts","jvm"],"features":{"type":0,"size":1000,"indices":[25,36,135,160,170,183,262,272,275,300,372,388,394,400,420,445,451,489,495,507,576,592,656,688,703,710,756,760,767,820,945,980],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0]},"cluster_label":2}
{"_c0":"For very large source trees on s  distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used     M files    K dirs  it was taking    minutes before my fix in HADOOP       and    minutes after the fix","_c1":"Speed up distcp buildListing   using threadpool","document":"For very large source trees on s  distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used     M files    K dirs  it was taking    minutes before my fix in HADOOP       and    minutes after the fix Speed up distcp buildListing   using threadpool","words":["for","very","large","source","trees","on","s","","distcp","is","taking","long","time","to","build","file","listing","","client","code","","before","starting","mappers","","","for","a","dataset","i","used","","","","","m","files","","","","k","dirs","","it","was","taking","","","","minutes","before","my","fix","in","hadoop","","","","","","","and","","","","minutes","after","the","fix","speed","up","distcp","buildlisting","","","using","threadpool"],"filtered":["large","source","trees","","distcp","taking","long","time","build","file","listing","","client","code","","starting","mappers","","","dataset","used","","","","","m","files","","","","k","dirs","","taking","","","","minutes","fix","hadoop","","","","","","","","","","minutes","fix","speed","distcp","buildlisting","","","using","threadpool"],"features":{"type":0,"size":1000,"indices":[36,70,77,82,108,128,135,157,159,170,181,197,198,234,281,317,329,333,372,388,420,437,445,449,456,493,494,495,536,551,605,616,624,638,696,710,782,801,855,881,904,916,944],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,27.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"From my perspective as a code reviewer  I find them more confusing than using String directly","_c1":"Remove Term Code type aliases in code generation","document":"From my perspective as a code reviewer  I find them more confusing than using String directly Remove Term Code type aliases in code generation","words":["from","my","perspective","as","a","code","reviewer","","i","find","them","more","confusing","than","using","string","directly","remove","term","code","type","aliases","in","code","generation"],"filtered":["perspective","code","reviewer","","find","confusing","using","string","directly","remove","term","code","type","aliases","code","generation"],"features":{"type":0,"size":1000,"indices":[117,170,188,203,250,261,288,329,372,420,445,510,526,572,624,629,831,851,888,916,921,924],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Function related  HiveExternalCatalog  APIs do not have enough verification logics  After the PR   HiveExternalCatalog  and  InMemoryCatalog  become consistent in the error handling  For example  below is the exception we got when calling  renameFunction","_c1":"Verification of Function related ExternalCatalog APIs","document":"Function related  HiveExternalCatalog  APIs do not have enough verification logics  After the PR   HiveExternalCatalog  and  InMemoryCatalog  become consistent in the error handling  For example  below is the exception we got when calling  renameFunction Verification of Function related ExternalCatalog APIs","words":["function","related","","hiveexternalcatalog","","apis","do","not","have","enough","verification","logics","","after","the","pr","","","hiveexternalcatalog","","and","","inmemorycatalog","","become","consistent","in","the","error","handling","","for","example","","below","is","the","exception","we","got","when","calling","","renamefunction","verification","of","function","related","externalcatalog","apis"],"filtered":["function","related","","hiveexternalcatalog","","apis","enough","verification","logics","","pr","","","hiveexternalcatalog","","","inmemorycatalog","","become","consistent","error","handling","","example","","exception","got","calling","","renamefunction","verification","function","related","externalcatalog","apis"],"features":{"type":0,"size":1000,"indices":[7,18,23,36,61,76,77,130,199,240,243,249,281,299,313,333,343,372,445,534,546,583,593,607,710,825,842,860,880,954,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,11.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Generate   currently does not support code generation  Lets add support for CG and for it and its most important generators    explode   and   json tuple","_c1":"Implement code generation for Generate","document":"Generate   currently does not support code generation  Lets add support for CG and for it and its most important generators    explode   and   json tuple Implement code generation for Generate","words":["generate","","","currently","does","not","support","code","generation","","lets","add","support","for","cg","and","for","it","and","its","most","important","generators","","","","explode","","","and","","","json","tuple","implement","code","generation","for","generate"],"filtered":["generate","","","currently","support","code","generation","","lets","add","support","cg","important","generators","","","","explode","","","","","json","tuple","implement","code","generation","generate"],"features":{"type":0,"size":1000,"indices":[18,36,97,117,190,296,333,367,372,376,420,432,472,495,537,662,695,698,763,770,782,830],"values":[1.0,3.0,1.0,2.0,1.0,1.0,3.0,1.0,10.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"HADOOP       added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment  start build env sh   This needs to be fixed","_c1":"Intel ISA L libraries should be added to the Dockerfile","document":"HADOOP       added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment  start build env sh   This needs to be fixed Intel ISA L libraries should be added to the Dockerfile","words":["hadoop","","","","","","","added","a","compile","and","runtime","dependence","on","the","intel","isa","l","library","but","didn","t","add","it","to","the","dockerfile","so","that","it","could","be","part","of","the","docker","based","build","environment","","start","build","env","sh","","","this","needs","to","be","fixed","intel","isa","l","libraries","should","be","added","to","the","dockerfile"],"filtered":["hadoop","","","","","","","added","compile","runtime","dependence","intel","isa","l","library","didn","add","dockerfile","part","docker","based","build","environment","","start","build","env","sh","","","needs","fixed","intel","isa","l","libraries","added","dockerfile"],"features":{"type":0,"size":1000,"indices":[6,55,82,83,136,141,150,170,181,213,333,343,368,372,373,374,384,388,394,432,446,463,466,495,536,566,625,656,665,666,710,740,760,777,781,846,862,871,996],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":9}
{"_c0":"HADOOP      added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook  For each of the shutdown hook registered  we currently don t have an upper bound for its execution time  We have seen namenode failed to shutdown completely  waiting for shutdown hook to finish after failover  for a long period of time  which breaks the namenode high availability scenarios  This ticket is opened to allow specifying a timeout value for the registered shutdown hook","_c1":"ShutdownHookManager should have a timeout for each of the Registered shutdown hook","document":"HADOOP      added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook  For each of the shutdown hook registered  we currently don t have an upper bound for its execution time  We have seen namenode failed to shutdown completely  waiting for shutdown hook to finish after failover  for a long period of time  which breaks the namenode high availability scenarios  This ticket is opened to allow specifying a timeout value for the registered shutdown hook ShutdownHookManager should have a timeout for each of the Registered shutdown hook","words":["hadoop","","","","","","added","a","shutdownhookmanager","to","be","used","by","different","components","instead","of","the","jvm","shutdownhook","","for","each","of","the","shutdown","hook","registered","","we","currently","don","t","have","an","upper","bound","for","its","execution","time","","we","have","seen","namenode","failed","to","shutdown","completely","","waiting","for","shutdown","hook","to","finish","after","failover","","for","a","long","period","of","time","","which","breaks","the","namenode","high","availability","scenarios","","this","ticket","is","opened","to","allow","specifying","a","timeout","value","for","the","registered","shutdown","hook","shutdownhookmanager","should","have","a","timeout","for","each","of","the","registered","shutdown","hook"],"filtered":["hadoop","","","","","","added","shutdownhookmanager","used","different","components","instead","jvm","shutdownhook","","shutdown","hook","registered","","currently","upper","bound","execution","time","","seen","namenode","failed","shutdown","completely","","waiting","shutdown","hook","finish","failover","","long","period","time","","breaks","namenode","high","availability","scenarios","","ticket","opened","allow","specifying","timeout","value","registered","shutdown","hook","shutdownhookmanager","timeout","registered","shutdown","hook"],"features":{"type":0,"size":1000,"indices":[36,77,86,89,116,128,139,157,170,181,204,223,231,238,281,284,296,297,299,300,321,343,366,372,373,384,388,436,443,456,532,556,568,577,597,605,642,656,665,667,710,752,763,768,773,777,792,817,863,885,897,904,941,971,993],"values":[6.0,1.0,4.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,12.0,1.0,1.0,4.0,3.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":19}
{"_c0":"HDFS     added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo","_c1":"Forward port SequenceFile syncFs and friends from Hadoop   x","document":"HDFS     added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo Forward port SequenceFile syncFs and friends from Hadoop   x","words":["hdfs","","","","","added","a","new","public","api","sequencefile","syncfs","","we","need","to","forward","port","this","for","compatibility","","looks","like","it","might","have","introduced","other","apis","that","need","forward","porting","as","well","","eg","localtedblocks","setfilelength","","and","datanode","getblockinfo","forward","port","sequencefile","syncfs","and","friends","from","hadoop","","","x"],"filtered":["hdfs","","","","","added","new","public","api","sequencefile","syncfs","","need","forward","port","compatibility","","looks","like","might","introduced","apis","need","forward","porting","well","","eg","localtedblocks","setfilelength","","datanode","getblockinfo","forward","port","sequencefile","syncfs","friends","hadoop","","","x"],"features":{"type":0,"size":1000,"indices":[9,25,36,157,170,173,179,181,240,299,330,331,333,372,373,384,388,404,442,495,498,504,537,572,583,644,667,674,713,760,772,810,826,834,842,921,967,985,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Hadoop currently supports one JVM defined through JAVA HOME  Since multiple JVMs  Java          are active  it will be helpful if there is an user configuration to choose the custom but supported JVM for her job  In other words  user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM","_c1":"Allow user to choose JVM for container execution","document":"Hadoop currently supports one JVM defined through JAVA HOME  Since multiple JVMs  Java          are active  it will be helpful if there is an user configuration to choose the custom but supported JVM for her job  In other words  user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM Allow user to choose JVM for container execution","words":["hadoop","currently","supports","one","jvm","defined","through","java","home","","since","multiple","jvms","","java","","","","","","","","","","are","active","","it","will","be","helpful","if","there","is","an","user","configuration","to","choose","the","custom","but","supported","jvm","for","her","job","","in","other","words","","user","will","be","able","to","choose","her","expected","jvm","only","for","her","container","execution","while","hadoop","services","may","be","running","on","different","jvm","allow","user","to","choose","jvm","for","container","execution"],"filtered":["hadoop","currently","supports","one","jvm","defined","java","home","","since","multiple","jvms","","java","","","","","","","","","","active","","helpful","user","configuration","choose","custom","supported","jvm","job","","words","","user","able","choose","expected","jvm","container","execution","hadoop","services","may","running","different","jvm","allow","user","choose","jvm","container","execution"],"features":{"type":0,"size":1000,"indices":[36,44,82,83,89,126,138,170,181,231,281,284,300,346,356,365,372,388,420,445,470,495,496,543,585,592,593,641,656,666,674,676,691,706,707,710,716,752,763,792,822,831,882,899,963,967,978,990,994],"values":[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,14.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,3.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Hive already supports this according to https   issues apache org jira browse HIVE        Currently Spark sql still supports only primitive types","_c1":"collect list   and collect set   should accept struct types as argument","document":"Hive already supports this according to https   issues apache org jira browse HIVE        Currently Spark sql still supports only primitive types collect list   and collect set   should accept struct types as argument","words":["hive","already","supports","this","according","to","https","","","issues","apache","org","jira","browse","hive","","","","","","","","currently","spark","sql","still","supports","only","primitive","types","collect","list","","","and","collect","set","","","should","accept","struct","types","as","argument"],"filtered":["hive","already","supports","according","https","","","issues","apache","org","jira","browse","hive","","","","","","","","currently","spark","sql","still","supports","primitive","types","collect","list","","","collect","set","","","accept","struct","types","argument"],"features":{"type":0,"size":1000,"indices":[57,105,116,154,333,372,373,388,465,475,495,500,535,572,594,599,613,665,686,728,763,800,813,821,846,849,899,994,998],"values":[1.0,1.0,1.0,1.0,1.0,13.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Hudson should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up   It would be nice if  when the timer goes off  Hudson did a     See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer","_c1":"Hudson should kill long running tests","document":"Hudson should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up   It would be nice if  when the timer goes off  Hudson did a     See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer Hudson should kill long running tests","words":["hudson","should","kill","long","running","tests","","","i","believe","it","is","supposed","to","but","doesn","t","quite","seem","to","do","the","job","if","the","test","is","really","hung","up","","","it","would","be","nice","if","","when","the","timer","goes","off","","hudson","did","a","","","","","see","the","section","","killing","a","hung","test","","at","http","","","wiki","apache","org","lucene","hadoop","hudsonbuildserver","hudson","should","kill","long","running","tests"],"filtered":["hudson","kill","long","running","tests","","","believe","supposed","doesn","quite","seem","job","test","really","hung","","","nice","","timer","goes","","hudson","","","","","see","section","","killing","hung","test","","http","","","wiki","apache","org","lucene","hadoop","hudsonbuildserver","hudson","kill","long","running","tests"],"features":{"type":0,"size":1000,"indices":[23,42,76,83,84,128,163,170,181,255,281,310,329,340,361,370,371,372,388,392,470,481,495,497,500,515,534,535,586,588,607,619,656,665,696,710,756,777,873,904,963,986],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,14.0,2.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":17}
{"_c0":"I recently hit a bug of com thoughtworks paranamer paranamer  which causes jackson fail to handle byte array defined in a case class  Then I find https   github com FasterXML jackson module scala issues     which suggests that it is caused by a bug in paranamer  Let s upgrade paranamer  Since we are using jackson       and jackson module paranamer       use com thoughtworks paranamer paranamer      I suggests that we upgrade paranamer to","_c1":"Upgrade com thoughtworks paranamer paranamer to","document":"I recently hit a bug of com thoughtworks paranamer paranamer  which causes jackson fail to handle byte array defined in a case class  Then I find https   github com FasterXML jackson module scala issues     which suggests that it is caused by a bug in paranamer  Let s upgrade paranamer  Since we are using jackson       and jackson module paranamer       use com thoughtworks paranamer paranamer      I suggests that we upgrade paranamer to Upgrade com thoughtworks paranamer paranamer to","words":["i","recently","hit","a","bug","of","com","thoughtworks","paranamer","paranamer","","which","causes","jackson","fail","to","handle","byte","array","defined","in","a","case","class","","then","i","find","https","","","github","com","fasterxml","jackson","module","scala","issues","","","","","which","suggests","that","it","is","caused","by","a","bug","in","paranamer","","let","s","upgrade","paranamer","","since","we","are","using","jackson","","","","","","","and","jackson","module","paranamer","","","","","","","use","com","thoughtworks","paranamer","paranamer","","","","","","i","suggests","that","we","upgrade","paranamer","to","upgrade","com","thoughtworks","paranamer","paranamer","to"],"filtered":["recently","hit","bug","com","thoughtworks","paranamer","paranamer","","causes","jackson","fail","handle","byte","array","defined","case","class","","find","https","","","github","com","fasterxml","jackson","module","scala","issues","","","","","suggests","caused","bug","paranamer","","let","upgrade","paranamer","","since","using","jackson","","","","","","","jackson","module","paranamer","","","","","","","use","com","thoughtworks","paranamer","paranamer","","","","","","suggests","upgrade","paranamer","upgrade","com","thoughtworks","paranamer","paranamer"],"features":{"type":0,"size":1000,"indices":[18,34,56,126,138,164,168,170,186,197,221,223,242,249,265,281,299,314,329,333,342,343,372,381,388,445,475,489,490,495,510,532,534,585,597,609,624,706,760,808,824,836,993,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,3.0,2.0,1.0,1.0,2.0,10.0,3.0,1.0,1.0,1.0,27.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,2.0,3.0,2.0,1.0]},"cluster_label":3}
{"_c0":"I think that we should upgrade from Tachyon       to       in order to get the fix for https   tachyon atlassian net browse TACHYON","_c1":"Upgrade Tachyon dependency to","document":"I think that we should upgrade from Tachyon       to       in order to get the fix for https   tachyon atlassian net browse TACHYON Upgrade Tachyon dependency to","words":["i","think","that","we","should","upgrade","from","tachyon","","","","","","","to","","","","","","","in","order","to","get","the","fix","for","https","","","tachyon","atlassian","net","browse","tachyon","upgrade","tachyon","dependency","to"],"filtered":["think","upgrade","tachyon","","","","","","","","","","","","","order","get","fix","https","","","tachyon","atlassian","net","browse","tachyon","upgrade","tachyon","dependency"],"features":{"type":0,"size":1000,"indices":[36,154,242,329,372,388,445,564,586,588,662,665,710,718,760,915,921,959,993,998],"values":[1.0,1.0,2.0,1.0,14.0,3.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite  and the end to end tests FunctionsSuite  It d be great to make all test suites consistent with this naming convention","_c1":"Use consistent naming for expression test suites","document":"I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite  and the end to end tests FunctionsSuite  It d be great to make all test suites consistent with this naming convention Use consistent naming for expression test suites","words":["i","think","we","have","an","undocumented","naming","convention","to","call","expression","unit","tests","expressionssuite","","and","the","end","to","end","tests","functionssuite","","it","d","be","great","to","make","all","test","suites","consistent","with","this","naming","convention","use","consistent","naming","for","expression","test","suites"],"filtered":["think","undocumented","naming","convention","call","expression","unit","tests","expressionssuite","","end","end","tests","functionssuite","","d","great","make","test","suites","consistent","naming","convention","use","consistent","naming","expression","test","suites"],"features":{"type":0,"size":1000,"indices":[36,94,146,160,260,284,287,299,329,333,335,372,373,388,489,495,525,557,564,583,586,619,650,656,710,748,752,804,931,954,968,993],"values":[1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"I took a look at the commit messages in git log    it looks like the individual commit messages are not that useful to include  but do make the commit messages more verbose  They are usually just a bunch of extremely concise descriptions of  bug fixes    merges   etc     See mailing list discussions  http   apache spark developers list         n  nabble com discuss Removing individual commit messages from the squash commit message td      html","_c1":"Remove individual commit messages from the squash commit message","document":"I took a look at the commit messages in git log    it looks like the individual commit messages are not that useful to include  but do make the commit messages more verbose  They are usually just a bunch of extremely concise descriptions of  bug fixes    merges   etc     See mailing list discussions  http   apache spark developers list         n  nabble com discuss Removing individual commit messages from the squash commit message td      html Remove individual commit messages from the squash commit message","words":["i","took","a","look","at","the","commit","messages","in","git","log","","","","it","looks","like","the","individual","commit","messages","are","not","that","useful","to","include","","but","do","make","the","commit","messages","more","verbose","","they","are","usually","just","a","bunch","of","extremely","concise","descriptions","of","","bug","fixes","","","","merges","","","etc","","","","","see","mailing","list","discussions","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","discuss","removing","individual","commit","messages","from","the","squash","commit","message","td","","","","","","html","remove","individual","commit","messages","from","the","squash","commit","message"],"filtered":["took","look","commit","messages","git","log","","","","looks","like","individual","commit","messages","useful","include","","make","commit","messages","verbose","","usually","bunch","extremely","concise","descriptions","","bug","fixes","","","","merges","","","etc","","","","","see","mailing","list","discussions","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","discuss","removing","individual","commit","messages","squash","commit","message","td","","","","","","html","remove","individual","commit","messages","squash","commit","message"],"features":{"type":0,"size":1000,"indices":[18,48,50,83,94,101,105,120,138,170,173,221,240,249,257,272,283,288,307,315,329,330,343,372,388,399,401,445,495,515,525,534,586,591,620,629,631,652,653,655,665,703,710,728,751,756,759,760,830,861,901,921,939,968,981,983],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,32.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,7.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0]},"cluster_label":3}
{"_c0":"I was investingating progress in SPARK       and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK       use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK        Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute","_c1":"TrainValidationSplit is missing in pyspark ml tuning","document":"I was investingating progress in SPARK       and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK       use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK        Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute TrainValidationSplit is missing in pyspark ml tuning","words":["i","was","investingating","progress","in","spark","","","","","","","and","i","noticed","that","there","is","no","trainvalidationsplit","class","in","pyspark","ml","tuning","module","","java","scala","s","examples","spark","","","","","","","use","org","apache","spark","ml","tuning","trainvalidationsplit","that","is","not","available","from","python","and","this","blocks","spark","","","","","","","","does","the","class","have","different","name","in","pyspark","","maybe","","also","","i","couldn","t","find","any","jira","task","to","saying","it","need","to","be","implemented","","is","it","by","design","that","the","trainvalidationsplit","estimator","is","not","ported","to","pyspark","","if","not","","that","is","if","the","estimator","needs","porting","then","i","would","like","to","contribute","trainvalidationsplit","is","missing","in","pyspark","ml","tuning"],"filtered":["investingating","progress","spark","","","","","","","noticed","trainvalidationsplit","class","pyspark","ml","tuning","module","","java","scala","examples","spark","","","","","","","use","org","apache","spark","ml","tuning","trainvalidationsplit","available","python","blocks","spark","","","","","","","","class","different","name","pyspark","","maybe","","also","","couldn","find","jira","task","saying","need","implemented","","design","trainvalidationsplit","estimator","ported","pyspark","","","estimator","needs","porting","like","contribute","trainvalidationsplit","missing","pyspark","ml","tuning"],"features":{"type":0,"size":1000,"indices":[15,18,20,89,91,105,163,170,177,189,197,213,223,234,281,299,324,329,330,333,346,350,371,372,373,381,388,399,445,451,454,489,490,495,509,510,525,534,535,537,556,589,596,626,649,656,666,667,698,710,756,760,777,792,821,831,868,921,932,942,967],"values":[1.0,3.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,3.0,4.0,1.0,2.0,1.0,1.0,1.0,26.0,1.0,1.0,4.0,1.0,4.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"If it returns Text  we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF String without extra string decoding and encoding","_c1":"WholeTextFileRDD should return Text rather than String","document":"If it returns Text  we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF String without extra string decoding and encoding WholeTextFileRDD should return Text rather than String","words":["if","it","returns","text","","we","can","reuse","this","in","spark","sql","to","provide","a","wholetextfile","data","source","and","directly","convert","the","text","into","utf","string","without","extra","string","decoding","and","encoding","wholetextfilerdd","should","return","text","rather","than","string"],"filtered":["returns","text","","reuse","spark","sql","provide","wholetextfile","data","source","directly","convert","text","utf","string","without","extra","string","decoding","encoding","wholetextfilerdd","return","text","rather","string"],"features":{"type":0,"size":1000,"indices":[8,70,105,118,169,170,188,190,208,261,288,333,356,372,373,388,407,437,445,475,495,665,686,695,710,811,833,884,888,891,906,961,993],"values":[1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Implement Python API for bisecting k means","_c1":"Python API for bisecting k means","document":"Implement Python API for bisecting k means Python API for bisecting k means","words":["implement","python","api","for","bisecting","k","means","python","api","for","bisecting","k","means"],"filtered":["implement","python","api","bisecting","k","means","python","api","bisecting","k","means"],"features":{"type":0,"size":1000,"indices":[36,394,401,456,472,589,644],"values":[2.0,2.0,2.0,2.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Implement a simple wrapper in SparkR to support k means","_c1":"K means wrapper in SparkR","document":"Implement a simple wrapper in SparkR to support k means K means wrapper in SparkR","words":["implement","a","simple","wrapper","in","sparkr","to","support","k","means","k","means","wrapper","in","sparkr"],"filtered":["implement","simple","wrapper","sparkr","support","k","means","k","means","wrapper","sparkr"],"features":{"type":0,"size":1000,"indices":[170,388,394,445,456,472,695,762,767,980],"values":[1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis","_c1":"Survival analysis in SparkR","document":"Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis Survival analysis in SparkR","words":["implement","a","simple","wrapper","of","aftsurvivalregression","in","sparkr","to","support","survival","analysis","survival","analysis","in","sparkr"],"filtered":["implement","simple","wrapper","aftsurvivalregression","sparkr","support","survival","analysis","survival","analysis","sparkr"],"features":{"type":0,"size":1000,"indices":[170,296,343,368,388,445,472,695,762,767,813,980],"values":[1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Implement a wrapper in SparkR to support bisecting k means","_c1":"Bisecting k means wrapper in SparkR","document":"Implement a wrapper in SparkR to support bisecting k means Bisecting k means wrapper in SparkR","words":["implement","a","wrapper","in","sparkr","to","support","bisecting","k","means","bisecting","k","means","wrapper","in","sparkr"],"filtered":["implement","wrapper","sparkr","support","bisecting","k","means","bisecting","k","means","wrapper","sparkr"],"features":{"type":0,"size":1000,"indices":[170,388,394,401,445,456,472,695,762,767],"values":[1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"In     and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs       api java index html  However  this disappeared in        http   spark apache org docs       api java index html  Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since","_c1":"Remove package grouping in genjavadoc","document":"In     and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs       api java index html  However  this disappeared in        http   spark apache org docs       api java index html  Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since Remove package grouping in genjavadoc","words":["in","","","","","and","earlier","releases","","we","have","package","grouping","in","the","generated","java","api","docs","","see","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","however","","this","disappeared","in","","","","","","","","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","rather","than","fixing","it","","i","d","suggest","removing","grouping","","because","it","might","take","some","time","to","fix","and","it","is","a","manual","process","to","update","the","grouping","in","sparkbuild","scala","","no","one","complained","about","missing","groups","since","remove","package","grouping","in","genjavadoc"],"filtered":["","","","","earlier","releases","","package","grouping","generated","java","api","docs","","see","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","however","","disappeared","","","","","","","","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","rather","fixing","","d","suggest","removing","grouping","","might","take","time","fix","manual","process","update","grouping","sparkbuild","scala","","one","complained","missing","groups","since","remove","package","grouping","genjavadoc"],"features":{"type":0,"size":1000,"indices":[22,44,94,105,157,170,194,207,261,266,281,288,299,307,315,329,332,333,343,346,372,373,388,400,421,437,445,453,490,495,515,525,535,545,585,605,632,644,652,656,665,673,710,775,777,825,855,858,911,967,968,985,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,35.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"In HADOOP       the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over    seconds causing the Failover Controller to kill it  The way the current Guava cache implementation works is approximately     On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache     When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value  I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed  Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters     hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys    hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is    which is likely to be enough","_c1":"Reload cached groups in background after expiry","document":"In HADOOP       the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over    seconds causing the Failover Controller to kill it  The way the current Guava cache implementation works is approximately     On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache     When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value  I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed  Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters     hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys    hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is    which is likely to be enough Reload cached groups in background after expiry","words":["in","hadoop","","","","","","","the","guava","cache","was","introduced","to","allow","refreshes","on","the","namenode","group","cache","to","run","in","the","background","","avoiding","many","slow","group","lookups","","even","with","this","change","","i","have","seen","quite","a","few","clusters","with","issues","due","to","slow","group","lookups","","the","problem","is","most","prevalent","in","ha","clusters","","where","a","slow","group","lookup","on","the","hdfs","user","can","fail","to","return","for","over","","","","seconds","causing","the","failover","controller","to","kill","it","","the","way","the","current","guava","cache","implementation","works","is","approximately","","","","","on","initial","load","","the","first","thread","to","request","groups","for","a","given","user","blocks","until","it","returns","","any","subsequent","threads","requesting","that","user","block","until","that","first","thread","populates","the","cache","","","","","when","the","key","expires","","the","first","thread","to","hit","the","cache","after","expiry","blocks","","while","it","is","blocked","","other","threads","will","return","the","old","value","","i","feel","it","is","this","blocking","thread","that","still","gives","the","namenode","issues","on","slow","group","lookups","","if","the","call","from","the","fc","is","the","one","that","blocks","and","lookups","are","slow","","if","can","cause","the","nn","to","be","killed","","guava","has","the","ability","to","refresh","expired","keys","completely","in","the","background","","where","the","first","thread","that","hits","an","expired","key","schedules","a","background","cache","reload","","but","still","returns","the","old","value","","then","the","cache","is","eventually","updated","","this","patch","introduces","this","background","reload","feature","","there","are","two","new","parameters","","","","","hadoop","security","groups","cache","background","reload","","","default","false","to","keep","the","current","behaviour","","set","to","true","to","enable","a","small","thread","pool","and","background","refresh","for","expired","keys","","","","hadoop","security","groups","cache","background","reload","threads","","","only","relevant","if","the","above","is","set","to","true","","controls","how","many","threads","are","in","the","background","refresh","pool","","default","is","","","","which","is","likely","to","be","enough","reload","cached","groups","in","background","after","expiry"],"filtered":["hadoop","","","","","","","guava","cache","introduced","allow","refreshes","namenode","group","cache","run","background","","avoiding","many","slow","group","lookups","","even","change","","seen","quite","clusters","issues","due","slow","group","lookups","","problem","prevalent","ha","clusters","","slow","group","lookup","hdfs","user","fail","return","","","","seconds","causing","failover","controller","kill","","way","current","guava","cache","implementation","works","approximately","","","","","initial","load","","first","thread","request","groups","given","user","blocks","returns","","subsequent","threads","requesting","user","block","first","thread","populates","cache","","","","","key","expires","","first","thread","hit","cache","expiry","blocks","","blocked","","threads","return","old","value","","feel","blocking","thread","still","gives","namenode","issues","slow","group","lookups","","call","fc","one","blocks","lookups","slow","","cause","nn","killed","","guava","ability","refresh","expired","keys","completely","background","","first","thread","hits","expired","key","schedules","background","cache","reload","","still","returns","old","value","","cache","eventually","updated","","patch","introduces","background","reload","feature","","two","new","parameters","","","","","hadoop","security","groups","cache","background","reload","","","default","false","keep","current","behaviour","","set","true","enable","small","thread","pool","background","refresh","expired","keys","","","","hadoop","security","groups","cache","background","reload","threads","","","relevant","set","true","","controls","many","threads","background","refresh","pool","","default","","","","likely","enough","reload","cached","groups","background","expiry"],"features":{"type":0,"size":1000,"indices":[20,25,36,44,52,76,77,82,83,89,91,92,100,101,109,113,115,118,132,137,138,139,140,146,158,159,163,170,181,183,188,203,208,213,222,229,231,234,237,252,255,258,280,281,299,321,329,332,333,352,355,364,372,373,381,388,389,396,400,406,408,410,420,445,450,475,490,495,511,518,532,539,541,553,569,580,586,591,592,594,597,598,605,609,623,634,650,656,658,667,672,674,683,694,698,707,710,713,722,729,736,737,742,745,747,750,752,753,756,759,760,768,770,773,800,805,813,817,831,833,856,860,867,880,882,883,899,909,911,920,921,967,971,997],"values":[3.0,1.0,3.0,1.0,5.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,1.0,3.0,2.0,3.0,4.0,2.0,1.0,9.0,8.0,3.0,4.0,4.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0,54.0,4.0,3.0,14.0,1.0,9.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,4.0,1.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,29.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,2.0,6.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":18}
{"_c0":"In ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used  It would be nice to detect this pattern and re use intermediate values","_c1":"Optimize sequential projections","document":"In ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used  It would be nice to detect this pattern and re use intermediate values Optimize sequential projections","words":["in","ml","pipelines","","each","transformer","estimator","appends","new","columns","to","the","input","dataframe","","for","example","","it","might","produce","dataframes","like","the","following","columns","","a","","b","","c","","d","","where","a","is","from","raw","input","","b","","","udf","b","a","","","c","","","udf","c","b","","","and","d","","","udf","d","c","","","some","udfs","could","be","expensive","","however","","if","we","materialize","c","and","d","","udf","b","","and","udf","c","are","triggered","twice","","i","e","","","value","c","is","not","re","used","","it","would","be","nice","to","detect","this","pattern","and","re","use","intermediate","values","optimize","sequential","projections"],"filtered":["ml","pipelines","","transformer","estimator","appends","new","columns","input","dataframe","","example","","might","produce","dataframes","like","following","columns","","","b","","c","","d","","raw","input","","b","","","udf","b","","","c","","","udf","c","b","","","d","","","udf","d","c","","","udfs","expensive","","however","","materialize","c","d","","udf","b","","udf","c","triggered","twice","","e","","","value","c","re","used","","nice","detect","pattern","re","use","intermediate","values","optimize","sequential","projections"],"features":{"type":0,"size":1000,"indices":[0,18,25,36,62,86,91,94,138,139,161,162,163,170,189,213,230,243,273,281,315,324,327,329,330,333,361,366,370,372,373,388,400,422,425,445,483,489,495,506,508,551,605,617,626,656,660,673,693,710,722,768,843,878,885,921,922,969,985,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,4.0,5.0,1.0,1.0,29.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":3}
{"_c0":"In RDDSampler  it try use numpy to gain better performance for possion    but the number of call of random   is only    faction    N in the pure python implementation of possion    so there is no much performance gain from numpy  numpy is not a dependent of pyspark  so it maybe introduce some problem  such as there is no numpy installed in slaves  but only installed master  as reported in xxxx  It also complicate the code a lot  so we may should remove numpy from RDDSampler","_c1":"remove numpy from RDDSampler of PySpark","document":"In RDDSampler  it try use numpy to gain better performance for possion    but the number of call of random   is only    faction    N in the pure python implementation of possion    so there is no much performance gain from numpy  numpy is not a dependent of pyspark  so it maybe introduce some problem  such as there is no numpy installed in slaves  but only installed master  as reported in xxxx  It also complicate the code a lot  so we may should remove numpy from RDDSampler remove numpy from RDDSampler of PySpark","words":["in","rddsampler","","it","try","use","numpy","to","gain","better","performance","for","possion","","","","but","the","number","of","call","of","random","","","is","only","","","","faction","","","","n","in","the","pure","python","implementation","of","possion","","","","so","there","is","no","much","performance","gain","from","numpy","","numpy","is","not","a","dependent","of","pyspark","","so","it","maybe","introduce","some","problem","","such","as","there","is","no","numpy","installed","in","slaves","","but","only","installed","master","","as","reported","in","xxxx","","it","also","complicate","the","code","a","lot","","so","we","may","should","remove","numpy","from","rddsampler","remove","numpy","from","rddsampler","of","pyspark"],"filtered":["rddsampler","","try","use","numpy","gain","better","performance","possion","","","","number","call","random","","","","","","faction","","","","n","pure","python","implementation","possion","","","","much","performance","gain","numpy","","numpy","dependent","pyspark","","maybe","introduce","problem","","numpy","installed","slaves","","installed","master","","reported","xxxx","","also","complicate","code","lot","","may","remove","numpy","rddsampler","remove","numpy","rddsampler","pyspark"],"features":{"type":0,"size":1000,"indices":[18,30,36,83,101,146,159,160,170,270,272,281,288,304,332,343,346,368,372,374,383,384,388,399,400,420,445,489,492,495,509,524,572,583,589,655,661,665,666,698,710,735,736,738,741,759,792,831,858,899,921,941,993,995],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,2.0,1.0,1.0,5.0,2.0,3.0,22.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,6.0,2.0,3.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"In SPARK        support for AES encryption was added to the Spark network library  But the authentication of different Spark processes is still performed using SASL s DIGEST MD  mechanism  That means the authentication part is the weakest link  since the AES keys are currently encrypted using  des  strongest cipher supported by SASL   Spark can t really claim to provide the full benefits of using AES for encryption  We should add a new auth protocol that doesn t need these disclaimers","_c1":"AES based authentication mechanism for Spark","document":"In SPARK        support for AES encryption was added to the Spark network library  But the authentication of different Spark processes is still performed using SASL s DIGEST MD  mechanism  That means the authentication part is the weakest link  since the AES keys are currently encrypted using  des  strongest cipher supported by SASL   Spark can t really claim to provide the full benefits of using AES for encryption  We should add a new auth protocol that doesn t need these disclaimers AES based authentication mechanism for Spark","words":["in","spark","","","","","","","","support","for","aes","encryption","was","added","to","the","spark","network","library","","but","the","authentication","of","different","spark","processes","is","still","performed","using","sasl","s","digest","md","","mechanism","","that","means","the","authentication","part","is","the","weakest","link","","since","the","aes","keys","are","currently","encrypted","using","","des","","strongest","cipher","supported","by","sasl","","","spark","can","t","really","claim","to","provide","the","full","benefits","of","using","aes","for","encryption","","we","should","add","a","new","auth","protocol","that","doesn","t","need","these","disclaimers","aes","based","authentication","mechanism","for","spark"],"filtered":["spark","","","","","","","","support","aes","encryption","added","spark","network","library","","authentication","different","spark","processes","still","performed","using","sasl","digest","md","","mechanism","","means","authentication","part","weakest","link","","since","aes","keys","currently","encrypted","using","","des","","strongest","cipher","supported","sasl","","","spark","really","claim","provide","full","benefits","using","aes","encryption","","add","new","auth","protocol","doesn","need","disclaimers","aes","based","authentication","mechanism","spark"],"features":{"type":0,"size":1000,"indices":[25,36,40,66,74,83,89,105,118,128,138,146,170,177,182,197,205,223,234,249,281,288,310,343,372,384,388,394,432,445,446,461,470,500,533,534,537,585,593,610,624,625,645,665,695,710,722,740,760,763,777,790,800,833,897,909,911,928,945,993],"values":[1.0,3.0,4.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,16.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"In SPARK       we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container  cc    cloud fan    yhuai","_c1":"VectorUDT MatrixUDT should take primitive arrays without boxing","document":"In SPARK       we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container  cc    cloud fan    yhuai VectorUDT MatrixUDT should take primitive arrays without boxing","words":["in","spark","","","","","","","we","switched","to","use","genericarraydata","to","store","indices","and","values","in","vector","matrix","udts","","however","","genericarraydata","is","not","specialized","for","primitive","types","","this","might","hurt","mllib","performance","badly","","we","should","consider","either","specialize","genericarraydata","or","use","a","different","container","","cc","","","","cloud","fan","","","","yhuai","vectorudt","matrixudt","should","take","primitive","arrays","without","boxing"],"filtered":["spark","","","","","","","switched","use","genericarraydata","store","indices","values","vector","matrix","udts","","however","","genericarraydata","specialized","primitive","types","","might","hurt","mllib","performance","badly","","consider","either","specialize","genericarraydata","use","different","container","","cc","","","","cloud","fan","","","","yhuai","vectorudt","matrixudt","take","primitive","arrays","without","boxing"],"features":{"type":0,"size":1000,"indices":[18,36,58,86,89,91,105,109,165,170,187,227,230,281,283,312,333,352,372,373,388,445,465,472,489,500,514,521,563,572,605,665,673,734,742,743,759,822,855,884,903,950,982,985,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,1.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0]},"cluster_label":17}
{"_c0":"In Spark       DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset  in Java  This is a source compatible change","_c1":"Accept Dataset    instead of DataFrame in MLlib APIs","document":"In Spark       DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset  in Java  This is a source compatible change Accept Dataset    instead of DataFrame in MLlib APIs","words":["in","spark","","","","","","","dataframe","","is","an","alias","of","","dataset","row","","","","mllib","api","actually","works","for","other","types","of","","dataset","","","so","we","should","accept","","dataset","","","","","instead","","it","maps","to","","dataset","","in","java","","this","is","a","source","compatible","change","accept","dataset","","","","instead","of","dataframe","in","mllib","apis"],"filtered":["spark","","","","","","","dataframe","","alias","","dataset","row","","","","mllib","api","actually","works","types","","dataset","","","accept","","dataset","","","","","instead","","maps","","dataset","","java","","source","compatible","change","accept","dataset","","","","instead","dataframe","mllib","apis"],"features":{"type":0,"size":1000,"indices":[36,70,105,158,161,170,220,281,287,343,368,372,373,388,445,447,465,476,493,495,521,644,665,674,752,788,842,846,863,920,967,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,26.0,1.0,1.0,3.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"In SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well","_c1":"Support formula in spark kmeans in SparkR","document":"In SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well Support formula in spark kmeans in SparkR","words":["in","sparkr","","spark","kmeans","take","a","dataframe","with","double","columns","","this","is","different","from","other","ml","methods","we","implemented","","which","support","r","model","formula","","we","should","add","support","for","that","as","well","support","formula","in","spark","kmeans","in","sparkr"],"filtered":["sparkr","","spark","kmeans","take","dataframe","double","columns","","different","ml","methods","implemented","","support","r","model","formula","","add","support","well","support","formula","spark","kmeans","sparkr"],"features":{"type":0,"size":1000,"indices":[36,89,100,105,129,157,161,170,177,281,305,324,372,373,432,445,570,572,573,597,650,665,674,695,760,767,855,857,921,969,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"In branch     and later  the patches for various child and related bugs listed in HADOOP        most recently including HADOOP        HADOOP        HADOOP        HADOOP        and HDFS        eliminate all use of  commons httpclient  from Hadoop and its sub projects  except for hadoop tools hadoop openstack  see HADOOP         However  after incorporating these patches   commons httpclient  is still listed as a dependency in these POM files    hadoop project pom xml   hadoop yarn project hadoop yarn hadoop yarn registry pom xml We wish to remove these  but since commons httpclient is still used in many files in hadoop tools hadoop openstack  we ll need to  add  the dependency to   hadoop tools hadoop openstack pom xml  We ll add a note to HADOOP       to undo this when commons httpclient is removed from hadoop openstack   In      this was mostly done by HADOOP        but the version info formerly inherited from hadoop project pom xml also needs to be added  so that is in the branch     version of the patch  Other projects with undeclared transitive dependencies on commons httpclient  previously provided via hadoop common or hadoop client  may find this to be an incompatible change  Of course that also means such project is exposed to the commons httpclient CVE  and needs to be fixed for that reason as well","_c1":"remove unneeded commons httpclient dependencies from POM files in Hadoop and sub projects","document":"In branch     and later  the patches for various child and related bugs listed in HADOOP        most recently including HADOOP        HADOOP        HADOOP        HADOOP        and HDFS        eliminate all use of  commons httpclient  from Hadoop and its sub projects  except for hadoop tools hadoop openstack  see HADOOP         However  after incorporating these patches   commons httpclient  is still listed as a dependency in these POM files    hadoop project pom xml   hadoop yarn project hadoop yarn hadoop yarn registry pom xml We wish to remove these  but since commons httpclient is still used in many files in hadoop tools hadoop openstack  we ll need to  add  the dependency to   hadoop tools hadoop openstack pom xml  We ll add a note to HADOOP       to undo this when commons httpclient is removed from hadoop openstack   In      this was mostly done by HADOOP        but the version info formerly inherited from hadoop project pom xml also needs to be added  so that is in the branch     version of the patch  Other projects with undeclared transitive dependencies on commons httpclient  previously provided via hadoop common or hadoop client  may find this to be an incompatible change  Of course that also means such project is exposed to the commons httpclient CVE  and needs to be fixed for that reason as well remove unneeded commons httpclient dependencies from POM files in Hadoop and sub projects","words":["in","branch","","","","","and","later","","the","patches","for","various","child","and","related","bugs","listed","in","hadoop","","","","","","","","most","recently","including","hadoop","","","","","","","","hadoop","","","","","","","","hadoop","","","","","","","","hadoop","","","","","","","","and","hdfs","","","","","","","","eliminate","all","use","of","","commons","httpclient","","from","hadoop","and","its","sub","projects","","except","for","hadoop","tools","hadoop","openstack","","see","hadoop","","","","","","","","","however","","after","incorporating","these","patches","","","commons","httpclient","","is","still","listed","as","a","dependency","in","these","pom","files","","","","hadoop","project","pom","xml","","","hadoop","yarn","project","hadoop","yarn","hadoop","yarn","registry","pom","xml","we","wish","to","remove","these","","but","since","commons","httpclient","is","still","used","in","many","files","in","hadoop","tools","hadoop","openstack","","we","ll","need","to","","add","","the","dependency","to","","","hadoop","tools","hadoop","openstack","pom","xml","","we","ll","add","a","note","to","hadoop","","","","","","","to","undo","this","when","commons","httpclient","is","removed","from","hadoop","openstack","","","in","","","","","","this","was","mostly","done","by","hadoop","","","","","","","","but","the","version","info","formerly","inherited","from","hadoop","project","pom","xml","also","needs","to","be","added","","so","that","is","in","the","branch","","","","","version","of","the","patch","","other","projects","with","undeclared","transitive","dependencies","on","commons","httpclient","","previously","provided","via","hadoop","common","or","hadoop","client","","may","find","this","to","be","an","incompatible","change","","of","course","that","also","means","such","project","is","exposed","to","the","commons","httpclient","cve","","and","needs","to","be","fixed","for","that","reason","as","well","remove","unneeded","commons","httpclient","dependencies","from","pom","files","in","hadoop","and","sub","projects"],"filtered":["branch","","","","","later","","patches","various","child","related","bugs","listed","hadoop","","","","","","","","recently","including","hadoop","","","","","","","","hadoop","","","","","","","","hadoop","","","","","","","","hadoop","","","","","","","","hdfs","","","","","","","","eliminate","use","","commons","httpclient","","hadoop","sub","projects","","except","hadoop","tools","hadoop","openstack","","see","hadoop","","","","","","","","","however","","incorporating","patches","","","commons","httpclient","","still","listed","dependency","pom","files","","","","hadoop","project","pom","xml","","","hadoop","yarn","project","hadoop","yarn","hadoop","yarn","registry","pom","xml","wish","remove","","since","commons","httpclient","still","used","many","files","hadoop","tools","hadoop","openstack","","ll","need","","add","","dependency","","","hadoop","tools","hadoop","openstack","pom","xml","","ll","add","note","hadoop","","","","","","","undo","commons","httpclient","removed","hadoop","openstack","","","","","","","","mostly","done","hadoop","","","","","","","","version","info","formerly","inherited","hadoop","project","pom","xml","also","needs","added","","branch","","","","","version","patch","","projects","undeclared","transitive","dependencies","commons","httpclient","","previously","provided","via","hadoop","common","hadoop","client","","may","find","incompatible","change","","course","also","means","project","exposed","commons","httpclient","cve","","needs","fixed","reason","well","remove","unneeded","commons","httpclient","dependencies","pom","files","hadoop","sub","projects"],"features":{"type":0,"size":1000,"indices":[34,36,37,76,77,82,83,92,135,137,150,157,158,170,181,187,188,196,199,223,232,234,245,258,268,272,281,288,296,333,343,360,368,372,373,384,388,394,432,435,445,460,461,464,466,489,495,505,510,511,512,513,515,520,529,537,540,551,564,572,575,585,588,595,605,612,650,656,666,671,673,674,687,705,707,710,721,731,735,752,760,764,770,792,800,803,846,875,909,921,954,956,967,968,970,978,980,993,995,999],"values":[1.0,3.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,4.0,8.0,2.0,24.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,6.0,3.0,6.0,1.0,105.0,3.0,1.0,9.0,1.0,2.0,1.0,8.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,7.0,3.0,3.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,4.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,2.0,1.0]},"cluster_label":1}
{"_c0":"In current Async DFS implementation  file system calls are invoked and returns Future immediately to clients  Clients call Future get to retrieve final results  Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB  ProtobufRpcEngine and ipc Client  The callback path bypasses the original retry layer logic designed for synchronous DFS  This proposes refactoring to make retry also works for Async DFS","_c1":"Support async call retry and failover","document":"In current Async DFS implementation  file system calls are invoked and returns Future immediately to clients  Clients call Future get to retrieve final results  Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB  ProtobufRpcEngine and ipc Client  The callback path bypasses the original retry layer logic designed for synchronous DFS  This proposes refactoring to make retry also works for Async DFS Support async call retry and failover","words":["in","current","async","dfs","implementation","","file","system","calls","are","invoked","and","returns","future","immediately","to","clients","","clients","call","future","get","to","retrieve","final","results","","future","get","internally","invokes","a","chain","of","callbacks","residing","in","clientnamenodeprotocoltranslatorpb","","protobufrpcengine","and","ipc","client","","the","callback","path","bypasses","the","original","retry","layer","logic","designed","for","synchronous","dfs","","this","proposes","refactoring","to","make","retry","also","works","for","async","dfs","support","async","call","retry","and","failover"],"filtered":["current","async","dfs","implementation","","file","system","calls","invoked","returns","future","immediately","clients","","clients","call","future","get","retrieve","final","results","","future","get","internally","invokes","chain","callbacks","residing","clientnamenodeprotocoltranslatorpb","","protobufrpcengine","ipc","client","","callback","path","bypasses","original","retry","layer","logic","designed","synchronous","dfs","","proposes","refactoring","make","retry","also","works","async","dfs","support","async","call","retry","failover"],"features":{"type":0,"size":1000,"indices":[21,36,54,55,78,91,108,135,138,140,146,170,208,261,271,272,275,304,309,321,332,333,343,363,372,373,388,445,464,483,500,525,558,639,641,668,693,695,698,710,717,724,735,744,792,866,920,939,959,996,999],"values":[1.0,2.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,6.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations","_c1":"Remove some internal classes  dependency on SQLContext","document":"In general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations Remove some internal classes  dependency on SQLContext","words":["in","general","it","is","better","for","internal","classes","to","not","depend","on","the","external","class","","in","this","case","sqlcontext","","to","reduce","coupling","between","user","facing","apis","and","the","internal","implementations","remove","some","internal","classes","","dependency","on","sqlcontext"],"filtered":["general","better","internal","classes","depend","external","class","","case","sqlcontext","","reduce","coupling","user","facing","apis","internal","implementations","remove","internal","classes","","dependency","sqlcontext"],"features":{"type":0,"size":1000,"indices":[18,36,82,130,273,281,288,295,333,342,372,373,388,400,416,445,451,481,482,495,502,534,586,588,710,809,842,882,925,941],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In https   github com apache spark pull      we added  FromUnsafe  to convert nexted unsafe data like array map struct to safe versions  It s a quick solution and we already have  GenerateSafe  to do the conversion which is codegened  So we should remove  FromUnsafe  and implement its codegen version in  GenerateSafe","_c1":"remove FromUnsafe and add its codegen version to GenerateSafe","document":"In https   github com apache spark pull      we added  FromUnsafe  to convert nexted unsafe data like array map struct to safe versions  It s a quick solution and we already have  GenerateSafe  to do the conversion which is codegened  So we should remove  FromUnsafe  and implement its codegen version in  GenerateSafe remove FromUnsafe and add its codegen version to GenerateSafe","words":["in","https","","","github","com","apache","spark","pull","","","","","","we","added","","fromunsafe","","to","convert","nexted","unsafe","data","like","array","map","struct","to","safe","versions","","it","s","a","quick","solution","and","we","already","have","","generatesafe","","to","do","the","conversion","which","is","codegened","","so","we","should","remove","","fromunsafe","","and","implement","its","codegen","version","in","","generatesafe","remove","fromunsafe","and","add","its","codegen","version","to","generatesafe"],"filtered":["https","","","github","com","apache","spark","pull","","","","","","added","","fromunsafe","","convert","nexted","unsafe","data","like","array","map","struct","safe","versions","","quick","solution","already","","generatesafe","","conversion","codegened","","remove","","fromunsafe","","implement","codegen","version","","generatesafe","remove","fromunsafe","add","codegen","version","generatesafe"],"features":{"type":0,"size":1000,"indices":[57,105,135,155,170,197,202,221,242,263,281,288,296,299,330,333,356,368,372,384,388,420,432,445,455,472,495,510,534,597,613,637,642,655,665,671,695,706,710,748,993,995,998],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,16.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0]},"cluster_label":17}
{"_c0":"In logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK        Here is a motivating example","_c1":"Optimize SerializeFromObject for primitive array","document":"In logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK        Here is a motivating example Optimize SerializeFromObject for primitive array","words":["in","logical","plan","","","","serializefromobject","","","for","an","array","always","use","","","genericarraydata","","","as","a","destination","","","","unsafearraydata","","","could","be","used","for","an","primitive","array","","this","is","a","simple","approach","to","solve","issues","that","are","addressed","by","spark","","","","","","","","here","is","a","motivating","example","optimize","serializefromobject","for","primitive","array"],"filtered":["logical","plan","","","","serializefromobject","","","array","always","use","","","genericarraydata","","","destination","","","","unsafearraydata","","","used","primitive","array","","simple","approach","solve","issues","addressed","spark","","","","","","","","motivating","example","optimize","serializefromobject","primitive","array"],"features":{"type":0,"size":1000,"indices":[13,25,36,95,105,114,123,135,138,150,170,213,223,243,247,281,372,373,388,445,475,489,499,500,531,551,572,605,656,706,752,760,824,980,982],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,22.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"In order to enable significantly better unit testing as well as enhanced functionality  large portions of   config sh should be pulled into functions  See first comment for more","_c1":"pull argument parsing into a function","document":"In order to enable significantly better unit testing as well as enhanced functionality  large portions of   config sh should be pulled into functions  See first comment for more pull argument parsing into a function","words":["in","order","to","enable","significantly","better","unit","testing","as","well","as","enhanced","functionality","","large","portions","of","","","config","sh","should","be","pulled","into","functions","","see","first","comment","for","more","pull","argument","parsing","into","a","function"],"filtered":["order","enable","significantly","better","unit","testing","well","enhanced","functionality","","large","portions","","","config","sh","pulled","functions","","see","first","comment","pull","argument","parsing","function"],"features":{"type":0,"size":1000,"indices":[36,116,148,157,170,183,280,294,313,335,343,352,365,371,372,388,445,489,515,524,572,587,597,629,656,665,718,782,846,891,941,964,997],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days","_c1":"Rolling mechanism for demux output","document":"In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days Rolling mechanism for demux output","words":["in","order","to","reduce","the","number","of","file","on","hdfs","we","need","to","have","a","rolling","mechanism","for","the","demux","output","","","avoid","immediate","merging","if","there","s","already","file","for","the","same","time","range","","create","a","spill","file","instead","","","merge","all","raw","files","every","hours","","","merge","all","hourly","files","every","days","rolling","mechanism","for","demux","output"],"filtered":["order","reduce","number","file","hdfs","need","rolling","mechanism","demux","output","","","avoid","immediate","merging","already","file","time","range","","create","spill","file","instead","","","merge","raw","files","every","hours","","","merge","hourly","files","every","days","rolling","mechanism","demux","output"],"features":{"type":0,"size":1000,"indices":[36,57,82,87,108,109,122,144,157,170,188,197,265,299,312,343,372,388,445,502,537,551,583,590,617,656,710,718,721,773,831,845,858,863,936,945,967,968,993],"values":[3.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"In order to upgrade to Kryo    we need to shade Kryo in our custom Hive       fork","_c1":"Shade Kryo in our custom Hive       fork","document":"In order to upgrade to Kryo    we need to shade Kryo in our custom Hive       fork Shade Kryo in our custom Hive       fork","words":["in","order","to","upgrade","to","kryo","","","","we","need","to","shade","kryo","in","our","custom","hive","","","","","","","fork","shade","kryo","in","our","custom","hive","","","","","","","fork"],"filtered":["order","upgrade","kryo","","","","need","shade","kryo","custom","hive","","","","","","","fork","shade","kryo","custom","hive","","","","","","","fork"],"features":{"type":0,"size":1000,"indices":[82,206,242,346,372,388,445,537,599,704,718,763,993],"values":[3.0,2.0,1.0,2.0,15.0,3.0,3.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"In preparation for larger refactorings  I think that we should remove the confusing returnValues   option from the BlockStore put   APIs  returning the value is only useful in one place  caching  and in other situations  such as block replication  it s simpler to put   and then get","_c1":"Remove returnValues from BlockStore APIs","document":"In preparation for larger refactorings  I think that we should remove the confusing returnValues   option from the BlockStore put   APIs  returning the value is only useful in one place  caching  and in other situations  such as block replication  it s simpler to put   and then get Remove returnValues from BlockStore APIs","words":["in","preparation","for","larger","refactorings","","i","think","that","we","should","remove","the","confusing","returnvalues","","","option","from","the","blockstore","put","","","apis","","returning","the","value","is","only","useful","in","one","place","","caching","","and","in","other","situations","","such","as","block","replication","","it","s","simpler","to","put","","","and","then","get","remove","returnvalues","from","blockstore","apis"],"filtered":["preparation","larger","refactorings","","think","remove","confusing","returnvalues","","","option","blockstore","put","","","apis","","returning","value","useful","one","place","","caching","","situations","","block","replication","","simpler","put","","","get","remove","returnvalues","blockstore","apis"],"features":{"type":0,"size":1000,"indices":[36,40,44,102,191,197,222,250,272,281,288,329,333,372,381,388,418,445,495,511,526,564,572,665,674,686,689,710,716,760,768,785,794,842,899,921,959,978,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,12.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"In publishing SparkR to CRAN  it would be nice to have a vignette as a user guide that   describes the big picture   introduces the use of various methods This is important for new users because they may not even know which method to look up","_c1":"Add package vignette to SparkR","document":"In publishing SparkR to CRAN  it would be nice to have a vignette as a user guide that   describes the big picture   introduces the use of various methods This is important for new users because they may not even know which method to look up Add package vignette to SparkR","words":["in","publishing","sparkr","to","cran","","it","would","be","nice","to","have","a","vignette","as","a","user","guide","that","","","describes","the","big","picture","","","introduces","the","use","of","various","methods","this","is","important","for","new","users","because","they","may","not","even","know","which","method","to","look","up","add","package","vignette","to","sparkr"],"filtered":["publishing","sparkr","cran","","nice","vignette","user","guide","","","describes","big","picture","","","introduces","use","various","methods","important","new","users","may","even","know","method","look","add","package","vignette","sparkr"],"features":{"type":0,"size":1000,"indices":[18,25,36,48,128,129,158,163,170,208,241,266,281,299,333,343,370,372,373,388,406,421,432,445,489,495,537,572,597,598,631,654,656,666,710,755,760,767,779,882,921,939,952,999],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and removed","_c1":"When the FileTailingAdaptor is unable to read a file it should take action instead of trying     times","document":"In the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and removed When the FileTailingAdaptor is unable to read a file it should take action instead of trying     times","words":["in","the","filetailingadaptor","","if","when","trying","to","read","a","file","","a","","file","does","not","exist","","","","","permission","denied","","exception","is","throws","then","that","file","should","be","log","and","removed","when","the","filetailingadaptor","is","unable","to","read","a","file","it","should","take","action","instead","of","trying","","","","","times"],"filtered":["filetailingadaptor","","trying","read","file","","","file","exist","","","","","permission","denied","","exception","throws","file","log","removed","filetailingadaptor","unable","read","file","take","action","instead","trying","","","","","times"],"features":{"type":0,"size":1000,"indices":[18,58,76,108,144,170,209,213,265,269,281,333,343,372,381,388,430,445,495,512,593,631,650,656,665,674,698,710,760,840,855,863],"values":[1.0,2.0,2.0,4.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,12.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Input  SELECT   FROM jdbcTable WHERE col     xxx  Current plan","_c1":"Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter","document":"Input  SELECT   FROM jdbcTable WHERE col     xxx  Current plan Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter","words":["input","","select","","","from","jdbctable","where","col","","","","","xxx","","current","plan","implement","jdbcrelation","unhandledfilters","for","removing","unnecessary","spark","filter"],"filtered":["input","","select","","","jdbctable","col","","","","","xxx","","current","plan","implement","jdbcrelation","unhandledfilters","removing","unnecessary","spark","filter"],"features":{"type":0,"size":1000,"indices":[0,36,105,123,139,242,299,372,446,472,547,640,680,710,721,801,921,968],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Instead of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer  This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a     megabyte serialized block may actually consume     megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted  This change is also a prerequisite to being able to cache blocks which are larger than  GB  although full support for that depends on several other changes which have not bee implemented yet","_c1":"Store serialized blocks as multiple chunks in MemoryStore","document":"Instead of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer  This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a     megabyte serialized block may actually consume     megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted  This change is also a prerequisite to being able to cache blocks which are larger than  GB  although full support for that depends on several other changes which have not bee implemented yet Store serialized blocks as multiple chunks in MemoryStore","words":["instead","of","storing","serialized","blocks","in","individual","bytebuffers","","the","blockmanager","should","be","capable","of","storing","a","serialized","block","in","multiple","chunks","","each","occupying","a","separate","bytebuffer","","this","change","will","help","to","improve","the","efficiency","of","memory","allocation","and","the","accuracy","of","memory","accounting","when","serializing","blocks","","our","current","serialization","code","uses","a","","","bytebufferoutputstream","","","","which","doubles","and","re","allocates","its","backing","byte","array","","this","increases","the","peak","memory","requirements","during","serialization","","since","we","need","to","hold","extra","memory","while","expanding","the","array","","","in","addition","","we","currently","don","t","account","for","the","extra","wasted","space","at","the","end","of","the","bytebuffer","s","backing","array","","so","a","","","","","megabyte","serialized","block","may","actually","consume","","","","","megabytes","of","memory","","after","switching","to","storing","blocks","in","multiple","chunks","","we","ll","be","able","to","efficiently","trim","the","backing","buffers","so","that","no","space","is","wasted","","this","change","is","also","a","prerequisite","to","being","able","to","cache","blocks","which","are","larger","than","","gb","","although","full","support","for","that","depends","on","several","other","changes","which","have","not","bee","implemented","yet","store","serialized","blocks","as","multiple","chunks","in","memorystore"],"filtered":["instead","storing","serialized","blocks","individual","bytebuffers","","blockmanager","capable","storing","serialized","block","multiple","chunks","","occupying","separate","bytebuffer","","change","help","improve","efficiency","memory","allocation","accuracy","memory","accounting","serializing","blocks","","current","serialization","code","uses","","","bytebufferoutputstream","","","","doubles","re","allocates","backing","byte","array","","increases","peak","memory","requirements","serialization","","since","need","hold","extra","memory","expanding","array","","","addition","","currently","account","extra","wasted","space","end","bytebuffer","backing","array","","","","","","megabyte","serialized","block","may","actually","consume","","","","","megabytes","memory","","switching","storing","blocks","multiple","chunks","","ll","able","efficiently","trim","backing","buffers","space","wasted","","change","also","prerequisite","able","cache","blocks","larger","","gb","","although","full","support","depends","several","changes","bee","implemented","yet","store","serialized","blocks","multiple","chunks","memorystore"],"features":{"type":0,"size":1000,"indices":[3,8,18,20,36,47,76,77,80,82,92,111,138,147,148,158,163,165,168,170,177,197,204,207,261,266,277,281,284,296,298,299,304,333,343,346,347,348,363,368,372,373,374,379,385,388,389,404,420,425,445,447,455,481,496,498,511,522,537,547,564,567,572,584,585,591,592,597,612,653,656,660,665,666,669,674,695,704,706,707,710,718,743,756,760,763,777,779,787,788,792,813,822,843,860,863,885,897,919,961,963,976,989,993],"values":[1.0,2.0,1.0,5.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,1.0,1.0,1.0,2.0,28.0,3.0,1.0,1.0,1.0,6.0,1.0,3.0,2.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,3.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,4.0]},"cluster_label":10}
{"_c0":"Interface method   FileFormat prepareRead     was added in  PR        https   github com apache spark pull        to handle a special case in the LibSVM data source  However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean","_c1":"Remove FileFormat prepareRead","document":"Interface method   FileFormat prepareRead     was added in  PR        https   github com apache spark pull        to handle a special case in the LibSVM data source  However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean Remove FileFormat prepareRead","words":["interface","method","","","fileformat","prepareread","","","","","was","added","in","","pr","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","to","handle","a","special","case","in","the","libsvm","data","source","","however","","the","semantics","of","this","interface","method","isn","t","intuitive","","it","returns","a","modified","version","of","the","data","source","options","map","","considering","that","the","libsvm","case","can","be","easily","handled","using","schema","metadata","inside","","","inferschema","","","","we","can","remove","this","interface","method","to","keep","the","","","fileformat","","","interface","clean","remove","fileformat","prepareread"],"filtered":["interface","method","","","fileformat","prepareread","","","","","added","","pr","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","handle","special","case","libsvm","data","source","","however","","semantics","interface","method","isn","intuitive","","returns","modified","version","data","source","options","map","","considering","libsvm","case","easily","handled","using","schema","metadata","inside","","","inferschema","","","","remove","interface","method","keep","","","fileformat","","","interface","clean","remove","fileformat","prepareread"],"features":{"type":0,"size":1000,"indices":[50,56,70,105,170,202,208,221,234,237,288,340,342,343,362,372,373,384,388,441,445,453,463,495,510,553,556,594,597,607,609,616,624,651,654,656,673,695,710,712,760,766,777,817,826,833,993,995,998],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,3.0,3.0,36.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"IsNotNull   filter is not being pushed down for JDBC datasource  It looks it is SQL standard according to SQL     SQL       SQL      and SQL    x and I believe most databases support this","_c1":"isnotnull operator not pushed down for JDBC datasource","document":"IsNotNull   filter is not being pushed down for JDBC datasource  It looks it is SQL standard according to SQL     SQL       SQL      and SQL    x and I believe most databases support this isnotnull operator not pushed down for JDBC datasource","words":["isnotnull","","","filter","is","not","being","pushed","down","for","jdbc","datasource","","it","looks","it","is","sql","standard","according","to","sql","","","","","sql","","","","","","","sql","","","","","","and","sql","","","","x","and","i","believe","most","databases","support","this","isnotnull","operator","not","pushed","down","for","jdbc","datasource"],"filtered":["isnotnull","","","filter","pushed","jdbc","datasource","","looks","sql","standard","according","sql","","","","","sql","","","","","","","sql","","","","","","sql","","","","x","believe","databases","support","isnotnull","operator","pushed","jdbc","datasource"],"features":{"type":0,"size":1000,"indices":[18,36,141,173,199,217,281,326,329,333,372,373,374,388,436,495,512,640,686,695,770,790,810,849,967,986],"values":[2.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,21.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"It is unnecessary and makes the type hierarchy slightly more complicated than needed","_c1":"Remove ExtractValueWithOrdinal abstract class","document":"It is unnecessary and makes the type hierarchy slightly more complicated than needed Remove ExtractValueWithOrdinal abstract class","words":["it","is","unnecessary","and","makes","the","type","hierarchy","slightly","more","complicated","than","needed","remove","extractvaluewithordinal","abstract","class"],"filtered":["unnecessary","makes","type","hierarchy","slightly","complicated","needed","remove","extractvaluewithordinal","abstract","class"],"features":{"type":0,"size":1000,"indices":[1,242,244,261,281,288,333,495,526,534,629,676,691,710,774,865,983],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"It would be easier to fix bugs and maintain the ec  script separately from Spark releases  For more information  see https   issues apache org jira browse SPARK","_c1":"Move spark ec  scripts to AMPLab","document":"It would be easier to fix bugs and maintain the ec  script separately from Spark releases  For more information  see https   issues apache org jira browse SPARK Move spark ec  scripts to AMPLab","words":["it","would","be","easier","to","fix","bugs","and","maintain","the","ec","","script","separately","from","spark","releases","","for","more","information","","see","https","","","issues","apache","org","jira","browse","spark","move","spark","ec","","scripts","to","amplab"],"filtered":["easier","fix","bugs","maintain","ec","","script","separately","spark","releases","","information","","see","https","","","issues","apache","org","jira","browse","spark","move","spark","ec","","scripts","amplab"],"features":{"type":0,"size":1000,"indices":[36,105,154,163,247,282,333,350,372,388,433,445,453,474,475,494,495,515,535,573,629,656,672,705,710,821,921,978,998],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"It would be good if we could read s  creds from a source other than via a java property Hadoop configuration option","_c1":"Read s a creds from a Credential Provider","document":"It would be good if we could read s  creds from a source other than via a java property Hadoop configuration option Read s a creds from a Credential Provider","words":["it","would","be","good","if","we","could","read","s","","creds","from","a","source","other","than","via","a","java","property","hadoop","configuration","option","read","s","a","creds","from","a","credential","provider"],"filtered":["good","read","","creds","source","via","java","property","hadoop","configuration","option","read","creds","credential","provider"],"features":{"type":0,"size":1000,"indices":[70,163,168,170,181,197,213,222,261,372,412,440,495,595,650,656,674,691,733,921,963,967,993],"values":[1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"It would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value","_c1":"Introduce additonal implementation with a dense format for UnsafeArrayData","document":"It would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value Introduce additonal implementation with a dense format for UnsafeArrayData","words":["it","would","be","good","to","have","an","additional","implementation","","which","uses","dense","format","","for","","","unsafearraydata","","","to","reduce","memory","footprint","","current","","","unsafearraydata","","","implementation","uses","only","a","sparse","format","","it","is","useful","for","an","","","unsafearraydata","","","that","is","created","by","a","method","","","fromprimitivearray","","","","which","have","no","","","null","","","value","introduce","additonal","implementation","with","a","dense","format","for","unsafearraydata"],"filtered":["good","additional","implementation","","uses","dense","format","","","","unsafearraydata","","","reduce","memory","footprint","","current","","","unsafearraydata","","","implementation","uses","sparse","format","","useful","","","unsafearraydata","","","created","method","","","fromprimitivearray","","","","","","null","","","value","introduce","additonal","implementation","dense","format","unsafearraydata"],"features":{"type":0,"size":1000,"indices":[36,109,111,114,160,163,167,168,170,193,222,223,262,272,281,299,346,372,388,396,495,502,597,650,654,656,698,710,752,760,768,788,899,909],"values":[3.0,1.0,2.0,5.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,3.0,1.0,25.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell","_c1":"Implement a native OS runtime for Hadoop","document":"It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell Implement a native OS runtime for Hadoop","words":["it","would","be","useful","to","implement","a","jni","based","runtime","for","hadoop","to","get","access","to","the","native","os","runtime","","this","would","allow","us","to","stop","relying","on","exec","ing","bash","to","get","access","to","information","such","as","user","groups","","process","limits","etc","","and","for","features","such","as","chown","chgrp","","org","apache","hadoop","util","shell","implement","a","native","os","runtime","for","hadoop"],"filtered":["useful","implement","jni","based","runtime","hadoop","get","access","native","os","runtime","","allow","us","stop","relying","exec","ing","bash","get","access","information","user","groups","","process","limits","etc","","features","chown","chgrp","","org","apache","hadoop","util","shell","implement","native","os","runtime","hadoop"],"features":{"type":0,"size":1000,"indices":[22,36,82,101,123,163,170,181,187,208,224,231,272,305,328,333,372,373,374,388,404,472,476,490,495,512,535,572,605,625,655,656,699,710,755,796,856,882,883,959,978],"values":[1.0,3.0,1.0,1.0,1.0,2.0,2.0,3.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,4.0,1.0,3.0,6.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up  including eliminating unnecessary sorting and the use of inefficient collections","_c1":"Improve performance of JSON schema inference s inferField step","document":"JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up  including eliminating unnecessary sorting and the use of inefficient collections Improve performance of JSON schema inference s inferField step","words":["json","schema","inference","spends","a","lot","of","time","in","inferfield","and","there","are","a","number","of","techniques","to","speed","it","up","","including","eliminating","unnecessary","sorting","and","the","use","of","inefficient","collections","improve","performance","of","json","schema","inference","s","inferfield","step"],"filtered":["json","schema","inference","spends","lot","time","inferfield","number","techniques","speed","","including","eliminating","unnecessary","sorting","use","inefficient","collections","improve","performance","json","schema","inference","inferfield","step"],"features":{"type":0,"size":1000,"indices":[55,128,138,157,170,189,197,198,242,333,343,372,388,441,445,489,491,495,522,558,583,662,695,706,710,732,735,759,831,854,954],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Java   is coming quickly to various clusters  Making sure Hadoop seamlessly works with Java   is important for the Apache community  This JIRA is to track the issues experiences encountered during Java   migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add a comment here","_c1":"Umbrella  Support Java   in Hadoop","document":"Java   is coming quickly to various clusters  Making sure Hadoop seamlessly works with Java   is important for the Apache community  This JIRA is to track the issues experiences encountered during Java   migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add a comment here Umbrella  Support Java   in Hadoop","words":["java","","","is","coming","quickly","to","various","clusters","","making","sure","hadoop","seamlessly","works","with","java","","","is","important","for","the","apache","community","","this","jira","is","to","track","the","issues","experiences","encountered","during","java","","","migration","","if","you","find","a","potential","bug","","","please","create","a","separate","jira","either","as","a","sub","task","or","linked","into","this","jira","","if","you","find","a","hadoop","or","jvm","configuration","tuning","","you","can","create","a","jira","as","well","","or","you","can","add","a","comment","here","umbrella","","support","java","","","in","hadoop"],"filtered":["java","","","coming","quickly","various","clusters","","making","sure","hadoop","seamlessly","works","java","","","important","apache","community","","jira","track","issues","experiences","encountered","java","","","migration","","find","potential","bug","","","please","create","separate","jira","either","sub","task","linked","jira","","find","hadoop","jvm","configuration","tuning","","create","jira","well","","add","comment","umbrella","","support","java","","","hadoop"],"features":{"type":0,"size":1000,"indices":[36,68,135,157,170,181,187,249,255,265,277,281,294,300,318,332,339,372,373,388,425,432,445,451,475,493,495,505,510,537,572,596,599,650,691,695,710,718,720,781,821,830,833,841,891,920,967,996,999],"values":[1.0,1.0,1.0,1.0,8.0,3.0,3.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,17.0,2.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,1.0]},"cluster_label":17}
{"_c0":"KMS and HttpFS are using Tomcat         we should move it to        to get bug fixes and security fixes  We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS","_c1":"Update Tomcat version used by HttpFS and KMS to latest   x version","document":"KMS and HttpFS are using Tomcat         we should move it to        to get bug fixes and security fixes  We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS Update Tomcat version used by HttpFS and KMS to latest   x version","words":["kms","and","httpfs","are","using","tomcat","","","","","","","","","we","should","move","it","to","","","","","","","","to","get","bug","fixes","and","security","fixes","","we","should","add","a","property","with","the","tomcat","version","in","the","hadoop","project","pom","and","use","that","property","from","kms","and","httpfs","update","tomcat","version","used","by","httpfs","and","kms","to","latest","","","x","version"],"filtered":["kms","httpfs","using","tomcat","","","","","","","","","move","","","","","","","","get","bug","fixes","security","fixes","","add","property","tomcat","version","hadoop","project","pom","use","property","kms","httpfs","update","tomcat","version","used","httpfs","kms","latest","","","x","version"],"features":{"type":0,"size":1000,"indices":[138,170,181,223,240,249,282,333,343,360,372,388,432,445,472,489,495,498,605,615,624,634,650,665,671,710,733,760,810,921,959,993,995],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,18.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0]},"cluster_label":17}
{"_c0":"Kafka     already released and it introduce new consumer API that not compatible with old one  So  I added new consumer api  I made separate classes in package org apache spark streaming kafka v   with changed API  I didn t remove old classes for more backward compatibility  User will not need to change his old spark applications when he uprgade to new Spark version  Please rewiew my changes","_c1":"Update KafkaDStreams to new Kafka      Consumer API","document":"Kafka     already released and it introduce new consumer API that not compatible with old one  So  I added new consumer api  I made separate classes in package org apache spark streaming kafka v   with changed API  I didn t remove old classes for more backward compatibility  User will not need to change his old spark applications when he uprgade to new Spark version  Please rewiew my changes Update KafkaDStreams to new Kafka      Consumer API","words":["kafka","","","","","already","released","and","it","introduce","new","consumer","api","that","not","compatible","with","old","one","","so","","i","added","new","consumer","api","","i","made","separate","classes","in","package","org","apache","spark","streaming","kafka","v","","","with","changed","api","","i","didn","t","remove","old","classes","for","more","backward","compatibility","","user","will","not","need","to","change","his","old","spark","applications","when","he","uprgade","to","new","spark","version","","please","rewiew","my","changes","update","kafkadstreams","to","new","kafka","","","","","","consumer","api"],"filtered":["kafka","","","","","already","released","introduce","new","consumer","api","compatible","old","one","","","added","new","consumer","api","","made","separate","classes","package","org","apache","spark","streaming","kafka","v","","","changed","api","","didn","remove","old","classes","backward","compatibility","","user","need","change","old","spark","applications","uprgade","new","spark","version","","please","rewiew","changes","update","kafkadstreams","new","kafka","","","","","","consumer","api"],"features":{"type":0,"size":1000,"indices":[18,25,36,44,57,76,105,113,141,158,160,181,230,263,266,288,329,333,335,343,344,352,363,368,372,380,384,388,392,420,445,475,476,477,495,535,537,629,644,650,672,718,742,760,777,809,834,841,847,882,916,995,997],"values":[2.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK       The change has been pushed back into Netty to support the same things under the DefaultFileRegion  https   github com netty netty issues      https   github com netty netty commit a    b  d c   f b f  e      b    a    be It looks like that went into        Final  I believe at the time we created LazyFileRegion we were on        Final and we are now using        Final so we should be able to use the netty class directly","_c1":"Remove LazyFileRegion","document":"LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK       The change has been pushed back into Netty to support the same things under the DefaultFileRegion  https   github com netty netty issues      https   github com netty netty commit a    b  d c   f b f  e      b    a    be It looks like that went into        Final  I believe at the time we created LazyFileRegion we were on        Final and we are now using        Final so we should be able to use the netty class directly Remove LazyFileRegion","words":["lazyfileregion","was","created","so","we","didn","t","create","a","file","descriptor","before","having","to","send","the","file","","see","https","","","issues","apache","org","jira","browse","spark","","","","","","","the","change","has","been","pushed","back","into","netty","to","support","the","same","things","under","the","defaultfileregion","","https","","","github","com","netty","netty","issues","","","","","","https","","","github","com","netty","netty","commit","a","","","","b","","d","c","","","f","b","f","","e","","","","","","b","","","","a","","","","be","it","looks","like","that","went","into","","","","","","","","final","","i","believe","at","the","time","we","created","lazyfileregion","we","were","on","","","","","","","","final","and","we","are","now","using","","","","","","","","final","so","we","should","be","able","to","use","the","netty","class","directly","remove","lazyfileregion"],"filtered":["lazyfileregion","created","didn","create","file","descriptor","send","file","","see","https","","","issues","apache","org","jira","browse","spark","","","","","","","change","pushed","back","netty","support","things","defaultfileregion","","https","","","github","com","netty","netty","issues","","","","","","https","","","github","com","netty","netty","commit","","","","b","","d","c","","","f","b","f","","e","","","","","","b","","","","","","","looks","like","went","","","","","","","","final","","believe","time","created","lazyfileregion","","","","","","","","final","using","","","","","","","","final","able","use","netty","class","directly","remove","lazyfileregion"],"features":{"type":0,"size":1000,"indices":[39,78,82,94,98,105,108,138,140,141,154,157,158,159,170,173,188,221,234,248,262,265,288,329,330,333,361,368,371,372,388,422,430,436,475,489,495,496,510,515,523,534,535,580,587,624,656,665,675,695,710,722,756,759,760,777,821,878,891,904,915,962,986,993,998],"values":[1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,59.0,3.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,3.0]},"cluster_label":7}
{"_c0":"LdapGroupsMapping   currently does not set timeouts on the LDAP queries  This can create a risk of a very long infinite wait on a connection","_c1":"Support timeouts in LDAP queries in LdapGroupsMapping","document":"LdapGroupsMapping   currently does not set timeouts on the LDAP queries  This can create a risk of a very long infinite wait on a connection Support timeouts in LDAP queries in LdapGroupsMapping","words":["ldapgroupsmapping","","","currently","does","not","set","timeouts","on","the","ldap","queries","","this","can","create","a","risk","of","a","very","long","infinite","wait","on","a","connection","support","timeouts","in","ldap","queries","in","ldapgroupsmapping"],"filtered":["ldapgroupsmapping","","","currently","set","timeouts","ldap","queries","","create","risk","long","infinite","wait","connection","support","timeouts","ldap","queries","ldapgroupsmapping"],"features":{"type":0,"size":1000,"indices":[18,82,170,226,265,343,347,372,373,445,477,613,695,698,703,710,763,794,813,833,904,944,957,965],"values":[1.0,2.0,3.0,2.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Leveraging HADOOP       will allow non rpc calls to be added to the call queue  This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS","_c1":"Support external calls in the RPC call queue","document":"Leveraging HADOOP       will allow non rpc calls to be added to the call queue  This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS Support external calls in the RPC call queue","words":["leveraging","hadoop","","","","","","","will","allow","non","rpc","calls","to","be","added","to","the","call","queue","","this","is","intended","to","support","routing","webhdfs","calls","through","the","call","queue","to","provide","a","unified","and","protocol","independent","qos","support","external","calls","in","the","rpc","call","queue"],"filtered":["leveraging","hadoop","","","","","","","allow","non","rpc","calls","added","call","queue","","intended","support","routing","webhdfs","calls","call","queue","provide","unified","protocol","independent","qos","support","external","calls","rpc","call","queue"],"features":{"type":0,"size":1000,"indices":[121,146,170,181,201,224,231,281,288,333,372,373,384,388,420,424,445,451,543,586,622,656,695,710,722,770,857,866,981],"values":[1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,3.0,1.0]},"cluster_label":9}
{"_c0":"LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base","_c1":"Remove InsertIntoHiveTable From Logical Plan","document":"LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base Remove InsertIntoHiveTable From Logical Plan","words":["logicalplan","","insertintohivetable","","is","useless","","thus","","we","can","remove","it","from","the","code","base","remove","insertintohivetable","from","logical","plan"],"filtered":["logicalplan","","insertintohivetable","","useless","","thus","","remove","code","base","remove","insertintohivetable","logical","plan"],"features":{"type":0,"size":1000,"indices":[123,173,247,281,288,372,420,495,515,684,710,766,833,906,921,993],"values":[1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"MLlib s Transformer uses the deprecated callUDF API","_c1":"Remove the use of the deprecated callUDF in MLlib","document":"MLlib s Transformer uses the deprecated callUDF API Remove the use of the deprecated callUDF in MLlib","words":["mllib","s","transformer","uses","the","deprecated","calludf","api","remove","the","use","of","the","deprecated","calludf","in","mllib"],"filtered":["mllib","transformer","uses","deprecated","calludf","api","remove","use","deprecated","calludf","mllib"],"features":{"type":0,"size":1000,"indices":[111,197,288,335,343,445,489,521,620,644,710,922],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0]},"cluster_label":13}
{"_c0":"Make the re j dependency consistent with other parts of Hadoop  Seeing some weird rare failures with older versions of maven that appear to be related to this","_c1":"make re j dependency consistent","document":"Make the re j dependency consistent with other parts of Hadoop  Seeing some weird rare failures with older versions of maven that appear to be related to this make re j dependency consistent","words":["make","the","re","j","dependency","consistent","with","other","parts","of","hadoop","","seeing","some","weird","rare","failures","with","older","versions","of","maven","that","appear","to","be","related","to","this","make","re","j","dependency","consistent"],"filtered":["make","re","j","dependency","consistent","parts","hadoop","","seeing","weird","rare","failures","older","versions","maven","appear","related","make","re","j","dependency","consistent"],"features":{"type":0,"size":1000,"indices":[1,143,149,181,199,220,330,343,372,373,388,400,425,525,562,588,650,656,671,674,705,710,760,768,788,954],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines  Making them public should be safe  even if we change internal formats","_c1":"Make DefaultParamsReadable Writable public APIs","document":"Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines  Making them public should be safe  even if we change internal formats Make DefaultParamsReadable Writable public APIs","words":["making","defaultparamswritable","and","defaultparamsreadable","public","will","help","users","who","have","their","own","transformers","save","and","load","pipelines","","making","them","public","should","be","safe","","even","if","we","change","internal","formats","make","defaultparamsreadable","writable","public","apis"],"filtered":["making","defaultparamswritable","defaultparamsreadable","public","help","users","transformers","save","load","pipelines","","making","public","safe","","even","change","internal","formats","make","defaultparamsreadable","writable","public","apis"],"features":{"type":0,"size":1000,"indices":[158,170,217,228,235,258,295,299,333,347,372,406,420,498,520,525,620,642,656,660,665,691,692,714,755,782,842,924,993,996],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Many Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime","_c1":"SparkSession time   a simple timer function","document":"Many Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime SparkSession time   a simple timer function","words":["many","spark","developers","often","want","to","test","the","runtime","of","some","function","in","interactive","debugging","and","testing","","it","d","be","really","useful","to","have","a","simple","spark","time","method","that","can","test","the","runtime","sparksession","time","","","a","simple","timer","function"],"filtered":["many","spark","developers","often","want","test","runtime","function","interactive","debugging","testing","","d","really","useful","simple","spark","time","method","test","runtime","sparksession","time","","","simple","timer","function"],"features":{"type":0,"size":1000,"indices":[42,94,105,157,170,188,272,299,310,313,315,333,343,372,374,388,400,401,445,489,495,528,586,654,656,710,712,738,760,833,942,980],"values":[1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Many users have requirements to use third party R packages in executors workers  but SparkR can not satisfy this requirements elegantly  For example  you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible  I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios     Users can install R packages from CRAN or custom CRAN like repository for each executors     Users can load their local R packages and install them on each executors  To achieve this goal  the first thing is to make SparkR executors support virtualenv like Python conda  I have investigated and found packrat http   rstudio github io packrat   is one of the candidates to support virtualenv for R  Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space  Then SparkR users can install third party packages in the application scope destroy after the application exit  and don t need to bother IT administrators to install these packages manually  I would like to know whether it make sense","_c1":"SparkR executors workers support virtualenv","document":"Many users have requirements to use third party R packages in executors workers  but SparkR can not satisfy this requirements elegantly  For example  you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible  I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios     Users can install R packages from CRAN or custom CRAN like repository for each executors     Users can load their local R packages and install them on each executors  To achieve this goal  the first thing is to make SparkR executors support virtualenv like Python conda  I have investigated and found packrat http   rstudio github io packrat   is one of the candidates to support virtualenv for R  Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space  Then SparkR users can install third party packages in the application scope destroy after the application exit  and don t need to bother IT administrators to install these packages manually  I would like to know whether it make sense SparkR executors workers support virtualenv","words":["many","users","have","requirements","to","use","third","party","r","packages","in","executors","workers","","but","sparkr","can","not","satisfy","this","requirements","elegantly","","for","example","","you","should","to","mess","with","the","it","administrators","of","the","cluster","to","deploy","these","r","packages","on","each","executors","workers","node","which","is","very","inflexible","","i","think","we","should","support","third","party","r","packages","for","sparkr","users","as","what","we","do","for","jar","packages","in","the","following","two","scenarios","","","","","users","can","install","r","packages","from","cran","or","custom","cran","like","repository","for","each","executors","","","","","users","can","load","their","local","r","packages","and","install","them","on","each","executors","","to","achieve","this","goal","","the","first","thing","is","to","make","sparkr","executors","support","virtualenv","like","python","conda","","i","have","investigated","and","found","packrat","http","","","rstudio","github","io","packrat","","","is","one","of","the","candidates","to","support","virtualenv","for","r","","packrat","is","a","dependency","management","system","for","r","and","can","isolate","the","dependent","r","packages","in","its","own","private","package","space","","then","sparkr","users","can","install","third","party","packages","in","the","application","scope","destroy","after","the","application","exit","","and","don","t","need","to","bother","it","administrators","to","install","these","packages","manually","","i","would","like","to","know","whether","it","make","sense","sparkr","executors","workers","support","virtualenv"],"filtered":["many","users","requirements","use","third","party","r","packages","executors","workers","","sparkr","satisfy","requirements","elegantly","","example","","mess","administrators","cluster","deploy","r","packages","executors","workers","node","inflexible","","think","support","third","party","r","packages","sparkr","users","jar","packages","following","two","scenarios","","","","","users","install","r","packages","cran","custom","cran","like","repository","executors","","","","","users","load","local","r","packages","install","executors","","achieve","goal","","first","thing","make","sparkr","executors","support","virtualenv","like","python","conda","","investigated","found","packrat","http","","","rstudio","github","io","packrat","","","one","candidates","support","virtualenv","r","","packrat","dependency","management","system","r","isolate","dependent","r","packages","private","package","space","","sparkr","users","install","third","party","packages","application","scope","destroy","application","exit","","need","bother","administrators","install","packages","manually","","like","know","whether","make","sense","sparkr","executors","workers","support","virtualenv"],"features":{"type":0,"size":1000,"indices":[5,13,18,36,44,64,65,77,82,83,89,91,92,101,163,169,170,174,183,187,188,191,204,207,225,228,235,243,258,266,281,296,299,311,329,330,331,333,343,346,362,372,373,381,388,408,425,436,445,448,461,463,482,489,495,502,510,514,522,525,526,534,537,547,548,564,570,572,588,589,597,602,603,608,612,631,639,650,665,667,675,693,695,704,710,736,755,757,767,777,779,796,807,809,833,838,859,885,906,916,921,924,944,955,979,993],"values":[1.0,1.0,1.0,6.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,3.0,3.0,3.0,4.0,2.0,1.0,1.0,23.0,2.0,1.0,9.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,6.0,9.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,8.0,1.0,5.0,4.0,5.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0]},"cluster_label":10}
{"_c0":"Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull       discussion r","_c1":"Move DT RF GBT Param setter methods to subclasses","document":"Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull       discussion r Move DT RF GBT Param setter methods to subclasses","words":["move","dt","rf","gbt","param","setter","methods","to","subclasses","and","deprecate","these","methods","in","the","model","classes","to","make","them","more","java","friendly","","see","discussion","at","https","","","github","com","apache","spark","pull","","","","","","","discussion","r","move","dt","rf","gbt","param","setter","methods","to","subclasses"],"filtered":["move","dt","rf","gbt","param","setter","methods","subclasses","deprecate","methods","model","classes","make","java","friendly","","see","discussion","https","","","github","com","apache","spark","pull","","","","","","","discussion","r","move","dt","rf","gbt","param","setter","methods","subclasses"],"features":{"type":0,"size":1000,"indices":[2,79,105,129,221,275,282,288,333,372,388,445,461,495,510,515,525,570,597,629,695,709,710,756,778,782,809,857,924,967,998],"values":[2.0,2.0,3.0,3.0,1.0,1.0,2.0,2.0,1.0,9.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Move hbase out of hadoop core  Move its JIRA issues and move it in svn from https   svn apache org repos asf hadoop core trunk src contrib hbase to https   svn apache org repos asf hadoop hbase trunk","_c1":"Move hbase out of hadoop core","document":"Move hbase out of hadoop core  Move its JIRA issues and move it in svn from https   svn apache org repos asf hadoop core trunk src contrib hbase to https   svn apache org repos asf hadoop hbase trunk Move hbase out of hadoop core","words":["move","hbase","out","of","hadoop","core","","move","its","jira","issues","and","move","it","in","svn","from","https","","","svn","apache","org","repos","asf","hadoop","core","trunk","src","contrib","hbase","to","https","","","svn","apache","org","repos","asf","hadoop","hbase","trunk","move","hbase","out","of","hadoop","core"],"filtered":["move","hbase","hadoop","core","","move","jira","issues","move","svn","https","","","svn","apache","org","repos","asf","hadoop","core","trunk","src","contrib","hbase","https","","","svn","apache","org","repos","asf","hadoop","hbase","trunk","move","hbase","hadoop","core"],"features":{"type":0,"size":1000,"indices":[29,56,83,181,228,282,296,333,343,372,388,445,475,495,528,535,537,654,801,821,921,988,998],"values":[4.0,2.0,2.0,4.0,3.0,4.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,3.0,2.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search","_c1":"NetworkTopology is not efficient adding getting removing nodes","document":"NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search NetworkTopology is not efficient adding getting removing nodes","words":["networktoplogy","uses","nodes","with","a","list","of","children","","the","access","to","these","children","is","slow","as","it","s","a","linear","search","networktopology","is","not","efficient","adding","getting","removing","nodes"],"filtered":["networktoplogy","uses","nodes","list","children","","access","children","slow","linear","search","networktopology","efficient","adding","getting","removing","nodes"],"features":{"type":0,"size":1000,"indices":[18,71,111,132,146,170,197,203,224,231,281,329,342,343,361,372,388,460,461,495,572,650,669,710,728,968],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Once   JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit","_c1":"JarFinder getJar should delete the jar file upon destruction of the JVM","document":"Once   JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit JarFinder getJar should delete the jar file upon destruction of the JVM","words":["once","","","jarfinder","getjar","","","","","is","invoked","by","a","client","app","","it","would","be","really","useful","to","destroy","the","generated","jar","after","the","jvm","is","destroyed","by","setting","","","tempjar","deleteonexit","","","","","","in","order","to","preserve","backwards","compatibility","a","configuration","setting","could","be","implemented","","e","g","","","","test","build","dir","purge","on","exit","jarfinder","getjar","should","delete","the","jar","file","upon","destruction","of","the","jvm"],"filtered":["","","jarfinder","getjar","","","","","invoked","client","app","","really","useful","destroy","generated","jar","jvm","destroyed","setting","","","tempjar","deleteonexit","","","","","","order","preserve","backwards","compatibility","configuration","setting","implemented","","e","g","","","","test","build","dir","purge","exit","jarfinder","getjar","delete","jar","file","upon","destruction","jvm"],"features":{"type":0,"size":1000,"indices":[12,30,51,77,82,94,108,135,147,163,170,177,213,223,255,272,281,300,309,310,315,343,372,388,417,422,436,445,495,536,568,586,603,656,659,665,691,692,706,709,710,718,735,834,878,906,953],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,18.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"One thing that the original patch for HADOOP      didn t address is the need for those curated jars to be visible in the final tarball","_c1":"make hadoop client set of curated jars available in a distribution tarball","document":"One thing that the original patch for HADOOP      didn t address is the need for those curated jars to be visible in the final tarball make hadoop client set of curated jars available in a distribution tarball","words":["one","thing","that","the","original","patch","for","hadoop","","","","","","didn","t","address","is","the","need","for","those","curated","jars","to","be","visible","in","the","final","tarball","make","hadoop","client","set","of","curated","jars","available","in","a","distribution","tarball"],"filtered":["one","thing","original","patch","hadoop","","","","","","didn","address","need","curated","jars","visible","final","tarball","make","hadoop","client","set","curated","jars","available","distribution","tarball"],"features":{"type":0,"size":1000,"indices":[36,44,78,81,110,135,137,140,141,170,181,281,343,371,372,388,445,456,525,537,600,656,675,710,760,777,798,813,897,996],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Other people aren t seeing this  yet    but unless you explicitly exclude v     of commons lang  from the azure build  which HADOOP       does   then the dependency declaration of commons lang  v       is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests  I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common","_c1":"explicitly declare the commons lang  dependency as","document":"Other people aren t seeing this  yet    but unless you explicitly exclude v     of commons lang  from the azure build  which HADOOP       does   then the dependency declaration of commons lang  v       is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests  I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common explicitly declare the commons lang  dependency as","words":["other","people","aren","t","seeing","this","","yet","","","","but","unless","you","explicitly","exclude","v","","","","","of","commons","lang","","from","the","azure","build","","which","hadoop","","","","","","","does","","","then","the","dependency","declaration","of","commons","lang","","v","","","","","","","is","creating","a","resolution","conflict","","that","s","a","dependency","only","needed","for","the","local","dynamodb","","","tests","","i","propose","to","fix","this","in","s","guard","by","explicitly","declaring","the","version","used","in","the","tests","to","be","that","of","the","azure","storage","one","","excluding","that","you","get","for","free","","it","doesn","t","impact","anything","shipped","in","production","","but","puts","the","hadoop","build","in","control","of","what","versions","of","commons","lang","are","coming","in","everywhere","by","way","of","the","commons","config","version","declared","in","hadoop","common","explicitly","declare","the","commons","lang","","dependency","as"],"filtered":["people","aren","seeing","","yet","","","","unless","explicitly","exclude","v","","","","","commons","lang","","azure","build","","hadoop","","","","","","","","","dependency","declaration","commons","lang","","v","","","","","","","creating","resolution","conflict","","dependency","needed","local","dynamodb","","","tests","","propose","fix","guard","explicitly","declaring","version","used","tests","azure","storage","one","","excluding","get","free","","doesn","impact","anything","shipped","production","","puts","hadoop","build","control","versions","commons","lang","coming","everywhere","way","commons","config","version","declared","hadoop","common","explicitly","declare","commons","lang","","dependency"],"features":{"type":0,"size":1000,"indices":[6,36,44,73,83,114,116,135,138,158,159,170,181,197,223,244,246,281,310,318,329,343,348,352,372,373,381,388,394,425,436,445,477,486,495,500,526,536,572,580,588,597,605,608,616,619,656,671,674,675,693,698,710,745,755,760,767,777,788,805,810,819,874,897,899,921,930,953,954,959,980,984,994,995],"values":[1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,33.0,2.0,1.0,2.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0]},"cluster_label":3}
{"_c0":"Otherwise  other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish","_c1":"MemorySink should not call DataFrame collect when holding a lock","document":"Otherwise  other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish MemorySink should not call DataFrame collect when holding a lock","words":["otherwise","","other","threads","cannot","query","the","content","in","memorysink","when","","dataframe","collect","","takes","long","time","to","finish","memorysink","should","not","call","dataframe","collect","when","holding","a","lock"],"filtered":["otherwise","","threads","query","content","memorysink","","dataframe","collect","","takes","long","time","finish","memorysink","call","dataframe","collect","holding","lock"],"features":{"type":0,"size":1000,"indices":[9,18,30,76,89,109,146,157,161,170,242,254,372,388,445,594,665,674,710,855,875,904,931,941],"values":[1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Our current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins","_c1":"Rewrite Intersect phyiscal plan using semi join","document":"Our current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins Rewrite Intersect phyiscal plan using semi join","words":["our","current","intersect","physical","operator","simply","delegates","to","rdd","intersect","","we","should","remove","the","intersect","physical","operator","and","simply","transform","a","logical","intersect","into","a","semi","join","","this","way","","we","can","take","advantage","of","all","the","benefits","of","join","implementations","","e","g","","managed","memory","","code","generation","","broadcast","joins","rewrite","intersect","phyiscal","plan","using","semi","join"],"filtered":["current","intersect","physical","operator","simply","delegates","rdd","intersect","","remove","intersect","physical","operator","simply","transform","logical","intersect","semi","join","","way","","take","advantage","benefits","join","implementations","","e","g","","managed","memory","","code","generation","","broadcast","joins","rewrite","intersect","phyiscal","plan","using","semi","join"],"features":{"type":0,"size":1000,"indices":[113,117,123,159,170,179,199,205,220,247,288,319,333,343,372,373,388,417,420,462,533,534,617,624,649,665,684,704,710,738,788,833,855,870,878,891,925,952,968,993,996],"values":[1.0,1.0,1.0,1.0,2.0,5.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Our current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true","_c1":"Deprecate registerTempTable and add dataset createTempView","document":"Our current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true Deprecate registerTempTable and add dataset createTempView","words":["our","current","dataset","registertemptable","does","not","actually","materialize","data","","so","","it","should","be","considered","as","creating","a","temp","view","","we","can","deprecate","it","and","create","a","new","method","called","dataset","createtempview","replaceifexists","","boolean","","","the","default","value","of","replaceifexists","should","be","false","","for","registertemptable","","it","will","call","dataset","createtempview","replaceifexists","","","true","deprecate","registertemptable","and","add","dataset","createtempview"],"filtered":["current","dataset","registertemptable","actually","materialize","data","","","considered","creating","temp","view","","deprecate","create","new","method","called","dataset","createtempview","replaceifexists","","boolean","","","default","value","replaceifexists","false","","registertemptable","","call","dataset","createtempview","replaceifexists","","","true","deprecate","registertemptable","add","dataset","createtempview"],"features":{"type":0,"size":1000,"indices":[18,19,25,36,146,170,171,188,265,275,293,333,343,366,368,369,372,381,398,420,432,447,448,493,495,572,640,654,656,665,678,695,698,704,710,737,767,768,833,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,10.0,1.0,1.0,1.0,1.0,1.0,3.0,4.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Parquet files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector","_c1":"Vectorize parquet decoding using ColumnarBatch","document":"Parquet files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector Vectorize parquet decoding using ColumnarBatch","words":["parquet","files","benefit","from","vectorized","decoding","","columnarbatches","have","been","designed","to","support","this","","this","means","that","a","single","encoded","parquet","column","is","decoded","to","a","single","columnvector","vectorize","parquet","decoding","using","columnarbatch"],"filtered":["parquet","files","benefit","vectorized","decoding","","columnarbatches","designed","support","","means","single","encoded","parquet","column","decoded","single","columnvector","vectorize","parquet","decoding","using","columnarbatch"],"features":{"type":0,"size":1000,"indices":[18,170,172,281,299,372,373,377,388,394,402,426,531,535,541,551,601,624,693,695,742,760,835,906,921,975],"values":[1.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Per   nongli  s suggestions  We should do these things     Remove the non vectorized parquet reader code     Support the remaining types  just big decimals      Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader","_c1":"Cleanup Extend the Vectorized Parquet Reader","document":"Per   nongli  s suggestions  We should do these things     Remove the non vectorized parquet reader code     Support the remaining types  just big decimals      Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader Cleanup Extend the Vectorized Parquet Reader","words":["per","","","nongli","","s","suggestions","","we","should","do","these","things","","","","","remove","the","non","vectorized","parquet","reader","code","","","","","support","the","remaining","types","","just","big","decimals","","","","","","move","the","logic","to","determine","if","our","parquet","reader","can","be","used","to","planning","","only","complex","types","should","fall","back","to","the","parquet","mr","reader","cleanup","extend","the","vectorized","parquet","reader"],"filtered":["per","","","nongli","","suggestions","","things","","","","","remove","non","vectorized","parquet","reader","code","","","","","support","remaining","types","","big","decimals","","","","","","move","logic","determine","parquet","reader","used","planning","","complex","types","fall","back","parquet","mr","reader","cleanup","extend","vectorized","parquet","reader"],"features":{"type":0,"size":1000,"indices":[9,170,172,197,224,272,282,288,304,307,372,388,402,420,430,440,446,461,465,499,534,536,598,605,637,656,665,695,704,710,729,747,833,834,843,899,904,918,970,993],"values":[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,19.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Per discussion on HADOOP        I d like to revert HADOOP        It removes a deprecated API  but the   x line does not have a release with the new replacement API  This places a burden on downstream applications","_c1":"Revert HADOOP       Remove unused TrashPolicy getInstance and initialize code","document":"Per discussion on HADOOP        I d like to revert HADOOP        It removes a deprecated API  but the   x line does not have a release with the new replacement API  This places a burden on downstream applications Revert HADOOP       Remove unused TrashPolicy getInstance and initialize code","words":["per","discussion","on","hadoop","","","","","","","","i","d","like","to","revert","hadoop","","","","","","","","it","removes","a","deprecated","api","","but","the","","","x","line","does","not","have","a","release","with","the","new","replacement","api","","this","places","a","burden","on","downstream","applications","revert","hadoop","","","","","","","remove","unused","trashpolicy","getinstance","and","initialize","code"],"filtered":["per","discussion","hadoop","","","","","","","","d","like","revert","hadoop","","","","","","","","removes","deprecated","api","","","","x","line","release","new","replacement","api","","places","burden","downstream","applications","revert","hadoop","","","","","","","remove","unused","trashpolicy","getinstance","initialize","code"],"features":{"type":0,"size":1000,"indices":[14,18,25,82,83,94,127,158,170,171,181,182,288,299,329,330,333,340,371,372,373,388,420,440,452,455,465,495,620,644,650,695,698,710,742,810,879,924],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,24.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":3}
{"_c0":"Per our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev        mbox   CCA g  F aVRBH WyyK nvBSLCMPtSdUuL Ge  WW DnmnvY SXg mail gmail com  E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size","_c1":"Add support for custom coalescers","document":"Per our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev        mbox   CCA g  F aVRBH WyyK nvBSLCMPtSdUuL Ge  WW DnmnvY SXg mail gmail com  E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size Add support for custom coalescers","words":["per","our","discussion","on","the","mailing","list","","please","see","","here","http","","","mail","archives","apache","org","mod","mbox","","spark","dev","","","","","","","","mbox","","","cca","g","","f","avrbh","wyyk","nvbslcmptsduul","ge","","ww","dnmnvy","sxg","mail","gmail","com","","e","","","it","would","be","nice","to","specify","a","custom","coalescing","policy","as","the","current","","","coalesce","","","","","method","only","allows","the","user","to","specify","the","number","of","partitions","and","we","cannot","really","control","much","","the","need","for","this","feature","popped","up","when","i","wanted","to","merge","small","files","by","coalescing","them","by","size","add","support","for","custom","coalescers"],"filtered":["per","discussion","mailing","list","","please","see","","http","","","mail","archives","apache","org","mod","mbox","","spark","dev","","","","","","","","mbox","","","cca","g","","f","avrbh","wyyk","nvbslcmptsduul","ge","","ww","dnmnvy","sxg","mail","gmail","com","","e","","","nice","specify","custom","coalescing","policy","current","","","coalesce","","","","","method","allows","user","specify","number","partitions","really","control","much","","need","feature","popped","wanted","merge","small","files","coalescing","size","add","support","custom","coalescers"],"features":{"type":0,"size":1000,"indices":[36,44,63,76,82,105,128,135,163,170,192,221,223,227,248,310,329,333,343,346,354,370,372,373,388,395,411,417,425,432,440,455,457,480,495,515,524,535,537,551,567,572,583,612,630,647,654,656,665,695,699,704,710,728,735,736,742,773,789,841,874,878,882,899,906,924,931,983,991,992,993,998],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,26.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"Previously  we use   MB as the default page size  which was way too big for a lot of Spark applications  especially for single node   This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available","_c1":"Pick default page size more intelligently","document":"Previously  we use   MB as the default page size  which was way too big for a lot of Spark applications  especially for single node   This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available Pick default page size more intelligently","words":["previously","","we","use","","","mb","as","the","default","page","size","","which","was","way","too","big","for","a","lot","of","spark","applications","","especially","for","single","node","","","this","patch","changes","it","so","that","the","default","page","size","","if","unset","by","the","user","","is","determined","by","the","number","of","cores","available","and","the","total","execution","memory","available","pick","default","page","size","more","intelligently"],"filtered":["previously","","use","","","mb","default","page","size","","way","big","lot","spark","applications","","especially","single","node","","","patch","changes","default","page","size","","unset","user","","determined","number","cores","available","total","execution","memory","available","pick","default","page","size","intelligently"],"features":{"type":0,"size":1000,"indices":[36,105,137,140,159,170,192,223,234,281,284,333,343,362,363,368,371,372,373,381,489,495,531,571,572,583,597,598,624,629,644,710,735,742,760,764,770,775,788,827,882,993],"values":[2.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,10.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":19}
{"_c0":"Provide API for SVM algorithm for DataFrames  I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation  The API should mimic existing spark ml classification APIs","_c1":"spark ml API for linear SVM","document":"Provide API for SVM algorithm for DataFrames  I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation  The API should mimic existing spark ml classification APIs spark ml API for linear SVM","words":["provide","api","for","svm","algorithm","for","dataframes","","i","would","recommend","using","owl","qn","","rather","than","wrapping","spark","mllib","s","sgd","based","implementation","","the","api","should","mimic","existing","spark","ml","classification","apis","spark","ml","api","for","linear","svm"],"filtered":["provide","api","svm","algorithm","dataframes","","recommend","using","owl","qn","","rather","wrapping","spark","mllib","sgd","based","implementation","","api","mimic","existing","spark","ml","classification","apis","spark","ml","api","linear","svm"],"features":{"type":0,"size":1000,"indices":[5,36,105,162,163,197,215,261,276,288,324,329,371,372,437,521,536,549,624,625,644,665,698,710,723,781,842,883,998],"values":[1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Push down the predicate through the Window operator  In this JIRA  predicates are pushed through Window if and only if the following conditions are satisfied    Predicate involves one and only one column that is part of window partitioning key   Window partitioning key is just a sequence of attributeReferences   i e   none of them is an expression    Predicate must be deterministic","_c1":"Predicate Push Down Through Window Operator","document":"Push down the predicate through the Window operator  In this JIRA  predicates are pushed through Window if and only if the following conditions are satisfied    Predicate involves one and only one column that is part of window partitioning key   Window partitioning key is just a sequence of attributeReferences   i e   none of them is an expression    Predicate must be deterministic Predicate Push Down Through Window Operator","words":["push","down","the","predicate","through","the","window","operator","","in","this","jira","","predicates","are","pushed","through","window","if","and","only","if","the","following","conditions","are","satisfied","","","","predicate","involves","one","and","only","one","column","that","is","part","of","window","partitioning","key","","","window","partitioning","key","is","just","a","sequence","of","attributereferences","","","i","e","","","none","of","them","is","an","expression","","","","predicate","must","be","deterministic","predicate","push","down","through","window","operator"],"filtered":["push","predicate","window","operator","","jira","","predicates","pushed","window","following","conditions","satisfied","","","","predicate","involves","one","one","column","part","window","partitioning","key","","","window","partitioning","key","sequence","attributereferences","","","e","","","none","expression","","","","predicate","must","deterministic","predicate","push","window","operator"],"features":{"type":0,"size":1000,"indices":[23,44,91,138,170,199,217,281,282,307,329,333,343,355,372,373,436,445,476,497,511,543,545,601,607,612,656,710,730,740,752,760,804,821,831,878,899,916,924,940,978],"values":[1.0,2.0,1.0,2.0,3.0,2.0,2.0,3.0,1.0,1.0,1.0,2.0,3.0,2.0,14.0,1.0,1.0,1.0,1.0,1.0,5.0,3.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0]},"cluster_label":19}
{"_c0":"Quantcast has released QFS      http   quantcast github com qfs   a C   distributed filesystem based on Kosmos File System KFS   QFS comes with various feature  performance  and stability improvements over KFS  A hadoop  fs  shim needs be added to support QFS through  qfs     URIs","_c1":"Need to add fs shim to use QFS","document":"Quantcast has released QFS      http   quantcast github com qfs   a C   distributed filesystem based on Kosmos File System KFS   QFS comes with various feature  performance  and stability improvements over KFS  A hadoop  fs  shim needs be added to support QFS through  qfs     URIs Need to add fs shim to use QFS","words":["quantcast","has","released","qfs","","","","","","http","","","quantcast","github","com","qfs","","","a","c","","","distributed","filesystem","based","on","kosmos","file","system","kfs","","","qfs","comes","with","various","feature","","performance","","and","stability","improvements","over","kfs","","a","hadoop","","fs","","shim","needs","be","added","to","support","qfs","through","","qfs","","","","","uris","need","to","add","fs","shim","to","use","qfs"],"filtered":["quantcast","released","qfs","","","","","","http","","","quantcast","github","com","qfs","","","c","","","distributed","filesystem","based","kosmos","file","system","kfs","","","qfs","comes","various","feature","","performance","","stability","improvements","kfs","","hadoop","","fs","","shim","needs","added","support","qfs","","qfs","","","","","uris","need","add","fs","shim","use","qfs"],"features":{"type":0,"size":1000,"indices":[51,82,83,108,135,170,181,214,221,224,307,308,333,352,364,372,384,388,432,435,489,510,537,543,580,625,639,650,656,661,665,666,695,722,736,759,780,954,999],"values":[1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,6.0,2.0,1.0,2.0,1.0,23.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Query     Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct","_c1":"Optimizer should remove unnecessary distincts  in multiple unions","document":"Query     Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct Optimizer should remove unnecessary distincts  in multiple unions","words":["query","","","","","only","one","distinct","should","be","necessary","","this","makes","a","bunch","of","unions","slower","than","a","bunch","of","union","alls","followed","by","a","distinct","optimizer","should","remove","unnecessary","distincts","","in","multiple","unions"],"filtered":["query","","","","","one","distinct","necessary","","makes","bunch","unions","slower","bunch","union","alls","followed","distinct","optimizer","remove","unnecessary","distincts","","multiple","unions"],"features":{"type":0,"size":1000,"indices":[44,170,194,221,223,242,261,288,295,316,343,372,373,399,406,438,445,592,656,665,691,697,845,899],"values":[1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler","_c1":"Add non iterator interface to RandomSampler","document":"RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler Add non iterator interface to RandomSampler","words":["randomsampler","sample","currently","accepts","iterator","as","input","and","output","another","iterator","","this","makes","it","inappropriate","to","use","in","wholestage","codegen","of","sampler","operator","","we","should","add","non","iterator","interface","to","randomsampler","add","non","iterator","interface","to","randomsampler"],"filtered":["randomsampler","sample","currently","accepts","iterator","input","output","another","iterator","","makes","inappropriate","use","wholestage","codegen","sampler","operator","","add","non","iterator","interface","randomsampler","add","non","iterator","interface","randomsampler"],"features":{"type":0,"size":1000,"indices":[0,122,199,224,226,263,297,333,343,372,373,388,432,445,489,495,556,572,583,597,620,663,665,691,763,779,949,993],"values":[1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Read ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html","_c1":"Read ADLS credentials from Credential Provider","document":"Read ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html Read ADLS credentials from Credential Provider","words":["read","adls","credentials","using","hadoop","credentialprovider","api","","see","https","","","hadoop","apache","org","docs","current","hadoop","project","dist","hadoop","common","credentialproviderapi","html","read","adls","credentials","from","credential","provider"],"filtered":["read","adls","credentials","using","hadoop","credentialprovider","api","","see","https","","","hadoop","apache","org","docs","current","hadoop","project","dist","hadoop","common","credentialproviderapi","html","read","adls","credentials","credential","provider"],"features":{"type":0,"size":1000,"indices":[181,256,265,287,372,440,495,515,535,545,624,644,650,652,671,704,710,779,921,954,963,998],"values":[4.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Recently the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too","_c1":"Apply fast serialization on collect limit","document":"Recently the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too Apply fast serialization on collect limit","words":["recently","the","fast","serialization","has","been","introduced","to","collecting","dataframe","dataset","","the","same","technology","can","be","used","on","collect","limit","operator","too","apply","fast","serialization","on","collect","limit"],"filtered":["recently","fast","serialization","introduced","collecting","dataframe","dataset","","technology","used","collect","limit","operator","apply","fast","serialization","collect","limit"],"features":{"type":0,"size":1000,"indices":[34,35,36,82,161,199,222,263,372,388,493,535,580,594,605,644,656,710,713,832,833,843],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Refactor   StaticInvoke      Invoke   and   NewInstance   as    Introduce   InvokeLike   to extract common logic from   StaticInvoke      Invoke   and   NewInstance   to prepare arguments    Remove unneeded null checking and fix nullability of   NewInstance      Modify to short circuit if arguments have   null   when   propageteNull    true","_c1":"Refactor StaticInvoke  Invoke and NewInstance","document":"Refactor   StaticInvoke      Invoke   and   NewInstance   as    Introduce   InvokeLike   to extract common logic from   StaticInvoke      Invoke   and   NewInstance   to prepare arguments    Remove unneeded null checking and fix nullability of   NewInstance      Modify to short circuit if arguments have   null   when   propageteNull    true Refactor StaticInvoke  Invoke and NewInstance","words":["refactor","","","staticinvoke","","","","","","invoke","","","and","","","newinstance","","","as","","","","introduce","","","invokelike","","","to","extract","common","logic","from","","","staticinvoke","","","","","","invoke","","","and","","","newinstance","","","to","prepare","arguments","","","","remove","unneeded","null","checking","and","fix","nullability","of","","","newinstance","","","","","","modify","to","short","circuit","if","arguments","have","","","null","","","when","","","propagetenull","","","","true","refactor","staticinvoke","","invoke","and","newinstance"],"filtered":["refactor","","","staticinvoke","","","","","","invoke","","","","","newinstance","","","","","","introduce","","","invokelike","","","extract","common","logic","","","staticinvoke","","","","","","invoke","","","","","newinstance","","","prepare","arguments","","","","remove","unneeded","null","checking","fix","nullability","","","newinstance","","","","","","modify","short","circuit","arguments","","","null","","","","","propagetenull","","","","true","refactor","staticinvoke","","invoke","newinstance"],"features":{"type":0,"size":1000,"indices":[76,160,164,170,188,193,254,282,288,299,304,318,333,343,372,388,392,445,479,520,544,572,623,644,659,800,921,954,964,989],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,53.0,3.0,3.0,4.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"Remove TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION","_c1":"Removal of TestHiveSharedState","document":"Remove TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION Removal of TestHiveSharedState","words":["remove","testhivesharedstate","","otherwise","","we","are","not","really","testing","the","reflection","logic","based","on","the","setting","of","we","are","not","really","testing","the","reflection","logic","based","on","the","setting","of","catalog","implementation","removal","of","testhivesharedstate"],"filtered":["remove","testhivesharedstate","","otherwise","","really","testing","reflection","logic","based","setting","really","testing","reflection","logic","based","setting","catalog","implementation","removal","testhivesharedstate"],"features":{"type":0,"size":1000,"indices":[18,82,138,288,304,310,343,372,476,489,510,625,651,659,698,710,875,971,993],"values":[2.0,2.0,2.0,1.0,2.0,2.0,3.0,2.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,4.0,1.0,2.0,2.0]},"cluster_label":2}
{"_c0":"Remove useless  databaseName   from  SimpleCatalogRelation","_c1":"Remove databaseName from SimpleCatalogRelation","document":"Remove useless  databaseName   from  SimpleCatalogRelation Remove databaseName from SimpleCatalogRelation","words":["remove","useless","","databasename","","","from","","simplecatalogrelation","remove","databasename","from","simplecatalogrelation"],"filtered":["remove","useless","","databasename","","","","simplecatalogrelation","remove","databasename","simplecatalogrelation"],"features":{"type":0,"size":1000,"indices":[119,288,372,482,906,921],"values":[2.0,2.0,4.0,2.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Right now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition","_c1":"SortMergeJoin and BroadcastHashJoin should support condition","document":"Right now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition SortMergeJoin and BroadcastHashJoin should support condition","words":["right","now","","we","use","a","filter","follow","sortmergejoin","or","broadcasthashjoin","for","conditions","","the","result","projection","of","join","could","be","very","expensive","if","they","generate","lots","of","rows","","could","be","reduce","mostly","by","condition","sortmergejoin","and","broadcasthashjoin","should","support","condition"],"filtered":["right","","use","filter","follow","sortmergejoin","broadcasthashjoin","conditions","","result","projection","join","expensive","generate","lots","rows","","reduce","mostly","condition","sortmergejoin","broadcasthashjoin","support","condition"],"features":{"type":0,"size":1000,"indices":[36,48,62,98,170,187,213,223,333,343,372,389,434,489,497,502,574,612,630,640,656,665,670,695,710,728,822,830,865,944,952,956,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Right now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy  It should be part of constructor of UnsafeRow","_c1":"The numFields of UnsafeRow should not changed by pointTo","document":"Right now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy  It should be part of constructor of UnsafeRow The numFields of UnsafeRow should not changed by pointTo","words":["right","now","","numfields","will","be","passed","in","by","pointto","","","","then","bitsetwidthinbytes","is","calculated","","making","pointto","","","a","little","bit","heavy","","it","should","be","part","of","constructor","of","unsaferow","the","numfields","of","unsaferow","should","not","changed","by","pointto"],"filtered":["right","","numfields","passed","pointto","","","","bitsetwidthinbytes","calculated","","making","pointto","","","little","bit","heavy","","part","constructor","unsaferow","numfields","unsaferow","changed","pointto"],"features":{"type":0,"size":1000,"indices":[18,73,98,122,170,175,223,281,343,353,372,381,391,392,420,445,446,495,533,574,617,656,665,710,740,819,858,996],"values":[1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Right now  we stack a new URLClassLoader when a user add a jar through SQL s add jar command  This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar  For example     In this case  when we lookup class B  we will not be able to find class A because Jar  is the parent of Jar","_c1":"Use a single URLClassLoader for jars added through SQL s  ADD JAR  command","document":"Right now  we stack a new URLClassLoader when a user add a jar through SQL s add jar command  This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar  For example     In this case  when we lookup class B  we will not be able to find class A because Jar  is the parent of Jar Use a single URLClassLoader for jars added through SQL s  ADD JAR  command","words":["right","now","","we","stack","a","new","urlclassloader","when","a","user","add","a","jar","through","sql","s","add","jar","command","","this","approach","can","introduce","issues","caused","by","the","ordering","of","added","jars","when","a","class","of","a","jar","depends","on","another","class","of","another","jar","","for","example","","","","","in","this","case","","when","we","lookup","class","b","","we","will","not","be","able","to","find","class","a","because","jar","","is","the","parent","of","jar","use","a","single","urlclassloader","for","jars","added","through","sql","s","","add","jar","","command"],"filtered":["right","","stack","new","urlclassloader","user","add","jar","sql","add","jar","command","","approach","introduce","issues","caused","ordering","added","jars","class","jar","depends","another","class","another","jar","","example","","","","","case","","lookup","class","b","","able","find","class","jar","","parent","jar","use","single","urlclassloader","jars","added","sql","","add","jar","","command"],"features":{"type":0,"size":1000,"indices":[18,25,36,66,76,82,95,98,110,135,160,170,186,197,223,243,281,298,322,342,343,361,372,373,384,388,420,421,432,445,475,489,492,496,510,531,534,543,574,592,603,656,686,710,778,779,833,882,993],"values":[1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,7.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,12.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,7.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,3.0]},"cluster_label":19}
{"_c0":"Right now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations","_c1":"Remove EmptyRow class","document":"Right now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations Remove EmptyRow class","words":["right","now","internalrow","is","megamorphic","because","it","has","many","different","implementations","","we","should","work","towards","having","only","one","or","at","most","two","internalrow","implementations","remove","emptyrow","class"],"filtered":["right","internalrow","megamorphic","many","different","implementations","","work","towards","one","two","internalrow","implementations","remove","emptyrow","class"],"features":{"type":0,"size":1000,"indices":[44,62,89,98,187,188,263,266,281,288,372,408,421,495,527,534,574,580,618,665,675,756,770,899,925,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Right now S Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option","_c1":"S Credentials should support use of CredentialProvider","document":"Right now S Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option S Credentials should support use of CredentialProvider","words":["right","now","s","credentials","only","works","with","cleartext","passwords","in","configs","","as","a","secret","access","key","or","the","uri","","","the","non","uri","version","should","use","credential","providers","with","a","fallback","to","the","clear","text","option","s","credentials","should","support","use","of","credentialprovider"],"filtered":["right","credentials","works","cleartext","passwords","configs","","secret","access","key","uri","","","non","uri","version","use","credential","providers","fallback","clear","text","option","credentials","support","use","credentialprovider"],"features":{"type":0,"size":1000,"indices":[33,57,79,98,169,170,187,197,222,224,265,287,343,355,363,372,388,395,445,489,572,574,650,665,695,710,716,825,899,920,924,963,995],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"RowEncoder doesn t support UserDefinedType now  We should add the support for it. Database application is important to focus in this issue","_c1":"Add UserDefinedType support to RowEncoder","document":"RowEncoder doesn t support UserDefinedType now  We should add the support for it. Database application is important to focus in this issue Add UserDefinedType support to RowEncoder","words":["rowencoder","doesn","t","support","userdefinedtype","now","","we","should","add","the","support","for","it.","database","application","is","important","to","focus","in","this","issue","add","userdefinedtype","support","to","rowencoder"],"filtered":["rowencoder","doesn","support","userdefinedtype","","add","support","it.","database","application","important","focus","issue","add","userdefinedtype","support","rowencoder"],"features":{"type":0,"size":1000,"indices":[36,98,122,169,281,334,372,373,388,432,445,500,512,537,665,690,695,710,748,777,858,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"SPARK      need to generate random data which follow Weibull distribution","_c1":"Add WeibullGenerator for RandomDataGenerator","document":"SPARK      need to generate random data which follow Weibull distribution Add WeibullGenerator for RandomDataGenerator","words":["spark","","","","","","need","to","generate","random","data","which","follow","weibull","distribution","add","weibullgenerator","for","randomdatagenerator"],"filtered":["spark","","","","","","need","generate","random","data","follow","weibull","distribution","add","weibullgenerator","randomdatagenerator"],"features":{"type":0,"size":1000,"indices":[36,105,372,388,432,537,540,597,695,728,738,750,798,823,830],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"SPARK      uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately","_c1":"Separate configs between shuffle and RPC","document":"SPARK      uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately Separate configs between shuffle and RPC","words":["spark","","","","","","uses","network","module","to","implement","rpc","","however","","there","are","some","configurations","named","with","","spark","shuffle","","prefix","in","the","network","module","","we","should","refactor","them","and","make","sure","the","user","can","control","them","in","shuffle","and","rpc","separately","separate","configs","between","shuffle","and","rpc"],"filtered":["spark","","","","","","uses","network","module","implement","rpc","","however","","configurations","named","","spark","shuffle","","prefix","network","module","","refactor","make","sure","user","control","shuffle","rpc","separately","separate","configs","shuffle","rpc"],"features":{"type":0,"size":1000,"indices":[74,79,105,111,138,173,181,299,333,372,388,400,445,472,481,494,525,568,623,650,665,673,710,718,783,821,831,833,874,882,924,993],"values":[2.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,3.0,10.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"See containing JIRA for details   SPARK","_c1":"ML     QA  Scala APIs audit for evaluation  tuning","document":"See containing JIRA for details   SPARK ML     QA  Scala APIs audit for evaluation  tuning","words":["see","containing","jira","for","details","","","spark","ml","","","","","qa","","scala","apis","audit","for","evaluation","","tuning"],"filtered":["see","containing","jira","details","","","spark","ml","","","","","qa","","scala","apis","audit","evaluation","","tuning"],"features":{"type":0,"size":1000,"indices":[36,105,128,324,372,490,515,533,596,602,723,821,842,856],"values":[2.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"See http   apache spark developers list         n  nabble com Re Should spark ec  get its own repo td      html for more details","_c1":"Move spark ec  from mesos to amplab","document":"See http   apache spark developers list         n  nabble com Re Should spark ec  get its own repo td      html for more details Move spark ec  from mesos to amplab","words":["see","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","re","should","spark","ec","","get","its","own","repo","td","","","","","","html","for","more","details","move","spark","ec","","from","mesos","to","amplab"],"filtered":["see","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","re","spark","ec","","get","repo","td","","","","","","html","details","move","spark","ec","","mesos","amplab"],"features":{"type":0,"size":1000,"indices":[36,105,120,128,221,228,282,296,315,372,388,425,433,495,515,548,629,652,653,655,665,672,728,921,959,965],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"See parent task SPARK","_c1":"python converage ml classification module","document":"See parent task SPARK python converage ml classification module","words":["see","parent","task","spark","python","converage","ml","classification","module"],"filtered":["see","parent","task","spark","python","converage","ml","classification","module"],"features":{"type":0,"size":1000,"indices":[66,105,125,299,324,451,515,549,589],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"See parent task SPARK          bryanc  did this component","_c1":"python coverage ml feature","document":"See parent task SPARK          bryanc  did this component python coverage ml feature","words":["see","parent","task","spark","","","","","","","","","","bryanc","","did","this","component","python","coverage","ml","feature"],"filtered":["see","parent","task","spark","","","","","","","","","","bryanc","","component","python","coverage","ml","feature"],"features":{"type":0,"size":1000,"indices":[66,105,324,372,373,451,515,589,607,609,628,736,803],"values":[1.0,1.0,1.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"SequenceFile s block compression format is too complex and requires   codecs to compress or decompress  It would be good to have a file format that only needs","_c1":"New binary file format","document":"SequenceFile s block compression format is too complex and requires   codecs to compress or decompress  It would be good to have a file format that only needs New binary file format","words":["sequencefile","s","block","compression","format","is","too","complex","and","requires","","","codecs","to","compress","or","decompress","","it","would","be","good","to","have","a","file","format","that","only","needs","new","binary","file","format"],"filtered":["sequencefile","block","compression","format","complex","requires","","","codecs","compress","decompress","","good","file","format","needs","new","binary","file","format"],"features":{"type":0,"size":1000,"indices":[9,25,108,163,168,170,187,197,222,274,281,299,331,333,372,388,441,480,495,511,567,644,656,666,760,812,899,938],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Several classes and methods have been deprecated and are creating lots of build warnings in branch      This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated  Any public use will require a deprecated API  We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc","_c1":"Eliminate MLlib     build warnings from deprecations","document":"Several classes and methods have been deprecated and are creating lots of build warnings in branch      This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated  Any public use will require a deprecated API  We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc Eliminate MLlib     build warnings from deprecations","words":["several","classes","and","methods","have","been","deprecated","and","are","creating","lots","of","build","warnings","in","branch","","","","","","this","issue","is","to","identify","and","fix","those","items","","","","","withsgd","classes","","change","to","make","class","not","deprecated","","object","deprecated","","and","public","class","constructor","deprecated","","any","public","use","will","require","a","deprecated","api","","we","need","to","keep","a","non","deprecated","private","api","since","we","cannot","eliminate","certain","uses","","python","api","","streaming","algs","","and","examples","","","","","use","in","pythonmllibapi","","change","to","using","private","constructors","","","","streaming","algs","","no","warnings","after","we","un","deprecate","the","classes","","","","examples","","deprecate","or","change","ones","which","use","deprecated","apis","","","multiclassmetrics","fields","","precision","","etc","eliminate","mllib","","","","","build","warnings","from","deprecations"],"filtered":["several","classes","methods","deprecated","creating","lots","build","warnings","branch","","","","","","issue","identify","fix","items","","","","","withsgd","classes","","change","make","class","deprecated","","object","deprecated","","public","class","constructor","deprecated","","public","use","require","deprecated","api","","need","keep","non","deprecated","private","api","since","eliminate","certain","uses","","python","api","","streaming","algs","","examples","","","","","use","pythonmllibapi","","change","using","private","constructors","","","","streaming","algs","","warnings","un","deprecate","classes","","","","examples","","deprecate","change","ones","use","deprecated","apis","","","multiclassmetrics","fields","","precision","","etc","eliminate","mllib","","","","","build","warnings","deprecations"],"features":{"type":0,"size":1000,"indices":[4,18,19,73,77,90,91,101,111,129,138,146,157,158,170,187,202,224,245,263,275,281,299,333,343,346,350,357,372,373,388,420,434,445,489,498,502,521,525,534,535,536,537,547,567,575,585,586,589,594,597,599,600,620,624,644,650,667,704,710,748,767,809,811,842,921,931,967,971,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,38.0,1.0,4.0,1.0,1.0,3.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":16}
{"_c0":"Shell java has a hardcoded path to  bin ls which is not correct on all platforms  eg  not on NixOS   see HADOOP       for a similar issue","_c1":"Remove hardcoded absolute path for ls","document":"Shell java has a hardcoded path to  bin ls which is not correct on all platforms  eg  not on NixOS   see HADOOP       for a similar issue Remove hardcoded absolute path for ls","words":["shell","java","has","a","hardcoded","path","to","","bin","ls","which","is","not","correct","on","all","platforms","","eg","","not","on","nixos","","","see","hadoop","","","","","","","for","a","similar","issue","remove","hardcoded","absolute","path","for","ls"],"filtered":["shell","java","hardcoded","path","","bin","ls","correct","platforms","","eg","","nixos","","","see","hadoop","","","","","","","similar","issue","remove","hardcoded","absolute","path","ls"],"features":{"type":0,"size":1000,"indices":[9,18,36,82,116,123,169,170,181,237,281,288,306,344,372,388,515,580,597,668,731,748,749,910,967,968],"values":[1.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,11.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Should we change the hash function for Text to something that handles non ascii characters better  http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup","_c1":"Replace Text hashcode with a better hash function for non ascii strings","document":"Should we change the hash function for Text to something that handles non ascii characters better  http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup Replace Text hashcode with a better hash function for non ascii strings","words":["should","we","change","the","hash","function","for","text","to","something","that","handles","non","ascii","characters","better","","http","","","bailey","svn","sourceforge","net","viewvc","bailey","trunk","src","java","org","apache","bailey","util","hash","java","view","markup","replace","text","hashcode","with","a","better","hash","function","for","non","ascii","strings"],"filtered":["change","hash","function","text","something","handles","non","ascii","characters","better","","http","","","bailey","svn","sourceforge","net","viewvc","bailey","trunk","src","java","org","apache","bailey","util","hash","java","view","markup","replace","text","hashcode","better","hash","function","non","ascii","strings"],"features":{"type":0,"size":1000,"indices":[12,36,37,83,84,158,169,170,224,293,313,372,374,382,388,495,535,537,553,558,650,665,697,699,710,760,787,830,915,941,967,968,988,993,997],"values":[1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Similar to Dataset transform","_c1":"DataFrame transform function","document":"Similar to Dataset transform DataFrame transform function","words":["similar","to","dataset","transform","dataframe","transform","function"],"filtered":["similar","dataset","transform","dataframe","transform","function"],"features":{"type":0,"size":1000,"indices":[161,313,388,493,910,996],"values":[1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Since   spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf    When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf","_c1":"Move spark sql hive thriftServer singleSession to SQLConf","document":"Since   spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf    When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf Move spark sql hive thriftServer singleSession to SQLConf","words":["since","","","spark","sql","hive","thriftserver","singlesession","","","is","a","configuration","of","sql","component","","this","conf","can","be","moved","from","","","sparkconf","","","to","","","staticsqlconf","","","","when","we","introduced","","","spark","sql","hive","thriftserver","singlesession","","","","all","the","sql","configuration","can","be","modified","in","different","sessions","","later","","static","sql","configuration","is","added","","it","is","a","perfect","fit","for","","","spark","sql","hive","thriftserver","singlesession","","","","previously","","we","did","the","same","move","for","","","spark","sql","warehouse","dir","","","from","","","sparkconf","","","to","","","staticsqlconf","move","spark","sql","hive","thriftserver","singlesession","to","sqlconf"],"filtered":["since","","","spark","sql","hive","thriftserver","singlesession","","","configuration","sql","component","","conf","moved","","","sparkconf","","","","","staticsqlconf","","","","introduced","","","spark","sql","hive","thriftserver","singlesession","","","","sql","configuration","modified","different","sessions","","later","","static","sql","configuration","added","","perfect","fit","","","spark","sql","hive","thriftserver","singlesession","","","","previously","","move","","","spark","sql","warehouse","dir","","","","","sparkconf","","","","","staticsqlconf","move","spark","sql","hive","thriftserver","singlesession","sqlconf"],"features":{"type":0,"size":1000,"indices":[36,76,89,105,115,118,138,144,170,281,282,343,372,373,375,384,388,445,466,495,585,596,599,607,609,616,656,686,691,692,709,710,713,764,799,813,833,843,889,921,942,968,993],"values":[2.0,1.0,1.0,5.0,4.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,38.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,4.0,4.0,1.0,1.0,1.0,3.0,8.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0]},"cluster_label":5}
{"_c0":"Since ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy","_c1":"PySpark ml evaluation should support save load","document":"Since ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy PySpark ml evaluation should support save load","words":["since","ml","evaluation","has","supported","save","load","at","scala","side","","supporting","it","at","python","side","is","very","straightforward","and","easy","pyspark","ml","evaluation","should","support","save","load"],"filtered":["since","ml","evaluation","supported","save","load","scala","side","","supporting","python","side","straightforward","easy","pyspark","ml","evaluation","support","save","load"],"features":{"type":0,"size":1000,"indices":[258,281,324,333,372,490,495,496,509,520,580,585,589,593,602,665,673,695,756,760,781,944],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Some codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira","_c1":"Remove unused codes in subexpressionEliminationForWholeStageCodegen","document":"Some codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira Remove unused codes in subexpressionEliminationForWholeStageCodegen","words":["some","codes","in","subexpressioneliminationforwholestagecodegen","are","never","used","actually","","remove","them","using","this","jira","remove","unused","codes","in","subexpressioneliminationforwholestagecodegen"],"filtered":["codes","subexpressioneliminationforwholestagecodegen","never","used","actually","","remove","using","jira","remove","unused","codes","subexpressioneliminationforwholestagecodegen"],"features":{"type":0,"size":1000,"indices":[126,128,138,288,372,373,400,445,447,455,605,624,821,830,924],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Some of the checkstyle checks are not realistic  like the line length   leading to spurious    in precommit  Let s disable","_c1":"Disable spurious checkstyle checks","document":"Some of the checkstyle checks are not realistic  like the line length   leading to spurious    in precommit  Let s disable Disable spurious checkstyle checks","words":["some","of","the","checkstyle","checks","are","not","realistic","","like","the","line","length","","","leading","to","spurious","","","","in","precommit","","let","s","disable","disable","spurious","checkstyle","checks"],"filtered":["checkstyle","checks","realistic","","like","line","length","","","leading","spurious","","","","precommit","","let","disable","disable","spurious","checkstyle","checks"],"features":{"type":0,"size":1000,"indices":[18,138,164,175,182,197,225,287,297,330,343,372,388,400,445,710,745,813,877,881],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,7.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":9}
{"_c0":"Some of the monitoring functions could be moved from YARN to Common for easier sharing","_c1":"Move ResourceCalculatorPlugin from YARN to Common","document":"Some of the monitoring functions could be moved from YARN to Common for easier sharing Move ResourceCalculatorPlugin from YARN to Common","words":["some","of","the","monitoring","functions","could","be","moved","from","yarn","to","common","for","easier","sharing","move","resourcecalculatorplugin","from","yarn","to","common"],"filtered":["monitoring","functions","moved","yarn","common","easier","sharing","move","resourcecalculatorplugin","yarn","common"],"features":{"type":0,"size":1000,"indices":[36,138,213,219,282,343,388,400,474,564,587,656,710,762,820,921,954],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Spark SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum","_c1":"Stddev Variance etc should support columnName as arguments","document":"Spark SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum Stddev Variance etc should support columnName as arguments","words":["spark","sql","aggregate","function","stddev","stddev","pop","stddev","samp","variance","var","pop","var","samp","skewness","kurtosis","collect","list","collect","set","should","support","columnname","as","arguments","like","other","aggregate","function","max","min","count","sum","stddev","variance","etc","should","support","columnname","as","arguments"],"filtered":["spark","sql","aggregate","function","stddev","stddev","pop","stddev","samp","variance","var","pop","var","samp","skewness","kurtosis","collect","list","collect","set","support","columnname","arguments","like","aggregate","function","max","min","count","sum","stddev","variance","etc","support","columnname","arguments"],"features":{"type":0,"size":1000,"indices":[50,101,105,113,275,283,313,330,385,419,434,438,444,452,569,572,594,665,674,686,695,728,799,800,813,939],"values":[1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,4.0]},"cluster_label":13}
{"_c0":"Spark SQL currently falls back to Hive for xpath related functions","_c1":"Implement xpath user defined functions","document":"Spark SQL currently falls back to Hive for xpath related functions Implement xpath user defined functions","words":["spark","sql","currently","falls","back","to","hive","for","xpath","related","functions","implement","xpath","user","defined","functions"],"filtered":["spark","sql","currently","falls","back","hive","xpath","related","functions","implement","xpath","user","defined","functions"],"features":{"type":0,"size":1000,"indices":[36,105,126,199,388,430,472,587,599,604,663,686,763,882],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Spark has an option called   spark localExecution enabled    according to the docs   quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote  This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method   As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark","_c1":"Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled","document":"Spark has an option called   spark localExecution enabled    according to the docs   quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote  This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method   As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled","words":["spark","has","an","option","called","","","spark","localexecution","enabled","","","","according","to","the","docs","","","quote","","enables","spark","to","run","certain","jobs","","such","as","first","","","or","take","","","on","the","driver","","without","sending","tasks","to","the","cluster","","this","can","make","certain","jobs","execute","very","quickly","","but","may","require","shipping","a","whole","partition","of","data","to","the","driver","","","quote","","this","feature","ends","up","adding","quite","a","bit","of","complexity","to","dagscheduler","","especially","in","the","","","runlocallywithinthread","","","method","","but","as","far","as","i","know","nobody","uses","this","feature","","i","searched","the","mailing","list","and","haven","t","seen","any","recent","mentions","of","the","configuration","nor","stacktraces","including","the","runlocally","method","","","as","a","step","towards","scheduler","complexity","reduction","","i","propose","that","we","remove","this","feature","and","all","code","related","to","it","for","spark","remove","dagscheduler","runlocallywithinthread","and","spark","localexecution","enabled"],"filtered":["spark","option","called","","","spark","localexecution","enabled","","","","according","docs","","","quote","","enables","spark","run","certain","jobs","","first","","","take","","","driver","","without","sending","tasks","cluster","","make","certain","jobs","execute","quickly","","may","require","shipping","whole","partition","data","driver","","","quote","","feature","ends","adding","quite","bit","complexity","dagscheduler","","especially","","","runlocallywithinthread","","","method","","far","know","nobody","uses","feature","","searched","mailing","list","haven","seen","recent","mentions","configuration","stacktraces","including","runlocally","method","","","step","towards","scheduler","complexity","reduction","","propose","remove","feature","code","related","spark","remove","dagscheduler","runlocallywithinthread","spark","localexecution","enabled"],"features":{"type":0,"size":1000,"indices":[19,36,45,50,62,82,83,91,105,111,128,135,145,170,183,187,199,222,231,255,272,288,300,329,333,338,341,343,344,359,361,364,372,373,381,388,398,420,439,445,446,455,495,505,522,525,530,545,548,558,572,580,586,587,628,654,665,666,690,691,693,695,710,728,736,746,752,760,765,777,779,817,833,837,842,849,855,884,903,944,954,964,968,978,983,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,29.0,4.0,2.0,6.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":10}
{"_c0":"Spark has configurable L  regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression","_c1":"SparkR spark glm should have configurable regularization parameter","document":"Spark has configurable L  regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression SparkR spark glm should have configurable regularization parameter","words":["spark","has","configurable","l","","regularization","parameter","for","generalized","linear","regression","","it","is","very","important","to","have","them","in","sparkr","so","that","users","can","run","ridge","regression","sparkr","spark","glm","should","have","configurable","regularization","parameter"],"filtered":["spark","configurable","l","","regularization","parameter","generalized","linear","regression","","important","sparkr","users","run","ridge","regression","sparkr","spark","glm","configurable","regularization","parameter"],"features":{"type":0,"size":1000,"indices":[6,24,36,73,105,281,299,329,364,368,372,388,435,445,495,537,580,665,695,755,760,767,833,924,944,964,984,999],"values":[1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Spark s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing   IO performance improvements made in Spark      In order to fix this performance problem  we should re implement those read paths in terms of TextFileFormat","_c1":"Use TextFileFormat in implementation of CSVFileFormat","document":"Spark s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing   IO performance improvements made in Spark      In order to fix this performance problem  we should re implement those read paths in terms of TextFileFormat Use TextFileFormat in implementation of CSVFileFormat","words":["spark","s","csvfileformat","data","source","uses","inefficient","methods","for","reading","files","during","schema","inference","and","does","not","benefit","from","file","listing","","","io","performance","improvements","made","in","spark","","","","","","in","order","to","fix","this","performance","problem","","we","should","re","implement","those","read","paths","in","terms","of","textfileformat","use","textfileformat","in","implementation","of","csvfileformat"],"filtered":["spark","csvfileformat","data","source","uses","inefficient","methods","reading","files","schema","inference","benefit","file","listing","","","io","performance","improvements","made","spark","","","","","","order","fix","performance","problem","","re","implement","read","paths","terms","textfileformat","use","textfileformat","implementation","csvfileformat"],"features":{"type":0,"size":1000,"indices":[18,36,48,55,70,101,105,108,111,129,197,251,277,333,343,344,372,373,388,415,425,437,441,445,472,489,491,541,551,600,650,651,665,695,698,718,759,780,838,850,921,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Spark s style checker should ban the use of Scala s JavaConversions  which provides implicit conversions between Java and Scala collections types  Instead  we should be performing these conversions explicitly using JavaConverters  or forgoing the conversions altogether if they re occurring inside of performance critical code","_c1":"Ban use of JavaConversions and migrate all existing uses to JavaConverters","document":"Spark s style checker should ban the use of Scala s JavaConversions  which provides implicit conversions between Java and Scala collections types  Instead  we should be performing these conversions explicitly using JavaConverters  or forgoing the conversions altogether if they re occurring inside of performance critical code Ban use of JavaConversions and migrate all existing uses to JavaConverters","words":["spark","s","style","checker","should","ban","the","use","of","scala","s","javaconversions","","which","provides","implicit","conversions","between","java","and","scala","collections","types","","instead","","we","should","be","performing","these","conversions","explicitly","using","javaconverters","","or","forgoing","the","conversions","altogether","if","they","re","occurring","inside","of","performance","critical","code","ban","use","of","javaconversions","and","migrate","all","existing","uses","to","javaconverters"],"filtered":["spark","style","checker","ban","use","scala","javaconversions","","provides","implicit","conversions","java","scala","collections","types","","instead","","performing","conversions","explicitly","using","javaconverters","","forgoing","conversions","altogether","re","occurring","inside","performance","critical","code","ban","use","javaconversions","migrate","existing","uses","javaconverters"],"features":{"type":0,"size":1000,"indices":[48,88,104,105,111,170,187,197,224,227,239,333,343,371,372,388,420,425,430,461,463,465,481,489,490,507,558,577,597,609,624,656,665,710,732,759,763,836,863,957,962,967,968,980,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added","_c1":"Open up SparkILoop getAddedJars","document":"SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added Open up SparkILoop getAddedJars","words":["sparkiloop","getaddedjars","is","a","useful","method","to","use","so","we","can","programmatically","get","the","list","of","jars","added","open","up","sparkiloop","getaddedjars"],"filtered":["sparkiloop","getaddedjars","useful","method","use","programmatically","get","list","jars","added","open","sparkiloop","getaddedjars"],"features":{"type":0,"size":1000,"indices":[110,128,141,170,272,275,281,343,354,368,384,388,489,654,710,728,783,833,959,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Starting from Hive       the derby metastore can use a in memory backend  Since our execution hive is a fake metastore  if we use in memory mode  we can reduce the time that is used on creating the execution hive","_c1":"Use in memory for execution hive s derby metastore","document":"Starting from Hive       the derby metastore can use a in memory backend  Since our execution hive is a fake metastore  if we use in memory mode  we can reduce the time that is used on creating the execution hive Use in memory for execution hive s derby metastore","words":["starting","from","hive","","","","","","","the","derby","metastore","can","use","a","in","memory","backend","","since","our","execution","hive","is","a","fake","metastore","","if","we","use","in","memory","mode","","we","can","reduce","the","time","that","is","used","on","creating","the","execution","hive","use","in","memory","for","execution","hive","s","derby","metastore"],"filtered":["starting","hive","","","","","","","derby","metastore","use","memory","backend","","since","execution","hive","fake","metastore","","use","memory","mode","","reduce","time","used","creating","execution","hive","use","memory","execution","hive","derby","metastore"],"features":{"type":0,"size":1000,"indices":[36,76,82,157,170,197,281,284,372,445,477,489,502,585,599,605,616,704,710,736,760,767,775,788,789,833,921,993],"values":[1.0,2.0,1.0,1.0,3.0,1.0,2.0,3.0,9.0,3.0,3.0,3.0,1.0,1.0,4.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0]},"cluster_label":9}
{"_c0":"Support for partitioned  parquet  format in FileStreamSink was added in Spark        now let s add support for partitioned  csv    json    text  format","_c1":"Add support for writing partitioned  csv    json    text  formats in Structured Streaming","document":"Support for partitioned  parquet  format in FileStreamSink was added in Spark        now let s add support for partitioned  csv    json    text  format Add support for writing partitioned  csv    json    text  formats in Structured Streaming","words":["support","for","partitioned","","parquet","","format","in","filestreamsink","was","added","in","spark","","","","","","","","now","let","s","add","support","for","partitioned","","csv","","","","json","","","","text","","format","add","support","for","writing","partitioned","","csv","","","","json","","","","text","","formats","in","structured","streaming"],"filtered":["support","partitioned","","parquet","","format","filestreamsink","added","spark","","","","","","","","let","add","support","partitioned","","csv","","","","json","","","","text","","format","add","support","writing","partitioned","","csv","","","","json","","","","text","","formats","structured","streaming"],"features":{"type":0,"size":1000,"indices":[36,98,105,164,169,172,197,222,234,255,263,313,323,372,384,432,445,517,596,662,695,714],"values":[3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,25.0,1.0,2.0,3.0,1.0,1.0,2.0,3.0,1.0]},"cluster_label":3}
{"_c0":"TaskContext supports task completion callback  which gets called regardless of task failures  However  there is no way for the listener to know if there is an error  This ticket proposes adding a new listener that gets called when a task fails","_c1":"Add a task failure listener to TaskContext","document":"TaskContext supports task completion callback  which gets called regardless of task failures  However  there is no way for the listener to know if there is an error  This ticket proposes adding a new listener that gets called when a task fails Add a task failure listener to TaskContext","words":["taskcontext","supports","task","completion","callback","","which","gets","called","regardless","of","task","failures","","however","","there","is","no","way","for","the","listener","to","know","if","there","is","an","error","","this","ticket","proposes","adding","a","new","listener","that","gets","called","when","a","task","fails","add","a","task","failure","listener","to","taskcontext"],"filtered":["taskcontext","supports","task","completion","callback","","gets","called","regardless","task","failures","","however","","way","listener","know","error","","ticket","proposes","adding","new","listener","gets","called","task","fails","add","task","failure","listener","taskcontext"],"features":{"type":0,"size":1000,"indices":[25,36,76,94,103,138,159,170,231,281,330,333,343,346,372,373,388,393,398,432,443,451,558,576,597,613,673,688,710,752,760,779,831,994,999],"values":[1.0,1.0,1.0,3.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The   FileContext   class currently is annotated as   Evolving    However  at this point we really need to treat it as a   Stable   interface","_c1":"FileContext and AbstractFileSystem should be annotated as a Stable interface","document":"The   FileContext   class currently is annotated as   Evolving    However  at this point we really need to treat it as a   Stable   interface FileContext and AbstractFileSystem should be annotated as a Stable interface","words":["the","","","filecontext","","","class","currently","is","annotated","as","","","evolving","","","","however","","at","this","point","we","really","need","to","treat","it","as","a","","","stable","","","interface","filecontext","and","abstractfilesystem","should","be","annotated","as","a","stable","interface"],"filtered":["","","filecontext","","","class","currently","annotated","","","evolving","","","","however","","point","really","need","treat","","","stable","","","interface","filecontext","abstractfilesystem","annotated","stable","interface"],"features":{"type":0,"size":1000,"indices":[57,170,281,310,333,372,373,382,388,489,495,510,520,534,537,556,572,633,656,665,673,680,710,756,763,993],"values":[1.0,2.0,1.0,1.0,1.0,14.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"The   WriteableRPCEninge   depends on Java s serialization mechanisms for RPC requests  Without proper checks  it has be shown that it can lead to security vulnerabilities such as remote code execution  e g   COLLECTIONS      HADOOP         The current implementation has migrated from   WriteableRPCEngine   to   ProtobufRPCEngine   now  This jira proposes to deprecate   WriteableRPCEngine   in branch   and to remove it in trunk","_c1":"Deprecate WriteableRPCEngine","document":"The   WriteableRPCEninge   depends on Java s serialization mechanisms for RPC requests  Without proper checks  it has be shown that it can lead to security vulnerabilities such as remote code execution  e g   COLLECTIONS      HADOOP         The current implementation has migrated from   WriteableRPCEngine   to   ProtobufRPCEngine   now  This jira proposes to deprecate   WriteableRPCEngine   in branch   and to remove it in trunk Deprecate WriteableRPCEngine","words":["the","","","writeablerpceninge","","","depends","on","java","s","serialization","mechanisms","for","rpc","requests","","without","proper","checks","","it","has","be","shown","that","it","can","lead","to","security","vulnerabilities","such","as","remote","code","execution","","e","g","","","collections","","","","","","hadoop","","","","","","","","","the","current","implementation","has","migrated","from","","","writeablerpcengine","","","to","","","protobufrpcengine","","","now","","this","jira","proposes","to","deprecate","","","writeablerpcengine","","","in","branch","","","and","to","remove","it","in","trunk","deprecate","writeablerpcengine"],"filtered":["","","writeablerpceninge","","","depends","java","serialization","mechanisms","rpc","requests","","without","proper","checks","","shown","lead","security","vulnerabilities","remote","code","execution","","e","g","","","collections","","","","","","hadoop","","","","","","","","","current","implementation","migrated","","","writeablerpcengine","","","","","protobufrpcengine","","","","jira","proposes","deprecate","","","writeablerpcengine","","","branch","","","remove","trunk","deprecate","writeablerpcengine"],"features":{"type":0,"size":1000,"indices":[9,36,64,71,82,83,98,181,197,261,272,275,284,288,298,333,356,372,373,388,417,420,445,495,558,572,575,580,634,656,688,698,710,732,745,760,783,821,833,841,842,843,878,884,909,921,967,987],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,37.0,1.0,4.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":16}
{"_c0":"The   o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed","_c1":"Removed deprecated o a h fs permission AccessControlException","document":"The   o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed Removed deprecated o a h fs permission AccessControlException","words":["the","","","o","a","h","fs","permission","accesscontrolexception","","","has","been","deprecated","for","last","major","releases","and","it","should","be","removed","removed","deprecated","o","a","h","fs","permission","accesscontrolexception"],"filtered":["","","o","h","fs","permission","accesscontrolexception","","","deprecated","last","major","releases","removed","removed","deprecated","o","h","fs","permission","accesscontrolexception"],"features":{"type":0,"size":1000,"indices":[36,135,170,261,269,282,333,372,449,453,495,512,535,580,620,656,665,710,880,989],"values":[1.0,2.0,2.0,2.0,2.0,2.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark      Also  the feature in SPARK      is strictly better than a correct implementation of that feature  We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work","_c1":"Remove references to preferredNodeLocalityData in javadoc and print warning when used","document":"The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark      Also  the feature in SPARK      is strictly better than a correct implementation of that feature  We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work Remove references to preferredNodeLocalityData in javadoc and print warning when used","words":["the","sparkcontext","constructor","that","takes","preferrednodelocalitydata","has","not","worked","since","before","spark","","","","","","also","","the","feature","in","spark","","","","","","is","strictly","better","than","a","correct","implementation","of","that","feature","","we","should","remove","any","documentation","references","to","that","feature","and","print","a","warning","when","it","is","used","saying","it","doesn","t","work","remove","references","to","preferrednodelocalitydata","in","javadoc","and","print","warning","when","used"],"filtered":["sparkcontext","constructor","takes","preferrednodelocalitydata","worked","since","spark","","","","","","also","","feature","spark","","","","","","strictly","better","correct","implementation","feature","","remove","documentation","references","feature","print","warning","used","saying","doesn","work","remove","references","preferrednodelocalitydata","javadoc","print","warning","used"],"features":{"type":0,"size":1000,"indices":[18,36,73,76,91,105,116,125,159,170,254,261,264,274,281,288,333,343,372,388,445,454,495,500,527,546,580,585,605,665,698,710,736,760,777,792,801,886,941,993,995],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,12.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"The StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time  We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well","_c1":"Add API for updateStateByKey to provide batch time as input","document":"The StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time  We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well Add API for updateStateByKey to provide batch time as input","words":["the","statedstream","currently","does","not","provide","the","batch","time","as","input","to","the","state","update","function","","this","is","required","in","cases","where","the","behavior","depends","on","the","batch","start","time","","we","","conviva","","have","been","patching","it","manually","for","the","past","several","spark","versions","but","we","thought","it","might","be","useful","for","others","as","well","add","api","for","updatestatebykey","to","provide","batch","time","as","input"],"filtered":["statedstream","currently","provide","batch","time","input","state","update","function","","required","cases","behavior","depends","batch","start","time","","","conviva","","patching","manually","past","several","spark","versions","thought","might","useful","others","well","add","api","updatestatebykey","provide","batch","time","input"],"features":{"type":0,"size":1000,"indices":[0,18,32,36,82,83,87,103,105,139,157,272,275,281,288,298,299,313,343,349,362,372,373,388,432,445,495,504,505,535,537,547,553,572,576,644,656,671,698,710,735,763,916,985,993,996],"values":[2.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"The Syncable sync   was deprecated in       We should remove it","_c1":"Remove the deprecated Syncable sync   method","document":"The Syncable sync   was deprecated in       We should remove it Remove the deprecated Syncable sync   method","words":["the","syncable","sync","","","was","deprecated","in","","","","","","","we","should","remove","it","remove","the","deprecated","syncable","sync","","","method"],"filtered":["syncable","sync","","","deprecated","","","","","","","remove","remove","deprecated","syncable","sync","","","method"],"features":{"type":0,"size":1000,"indices":[234,288,372,445,495,620,654,665,710,893,919,993],"values":[1.0,2.0,10.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0]},"cluster_label":9}
{"_c0":"The UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients","_c1":"UGI should contain authentication method","document":"The UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients UGI should contain authentication method","words":["the","usergroupinformation","should","contain","authentication","method","in","its","subject","","this","will","be","used","in","hdfs","to","issue","delegation","tokens","only","to","kerberos","authenticated","clients","ugi","should","contain","authentication","method"],"filtered":["usergroupinformation","contain","authentication","method","subject","","used","hdfs","issue","delegation","tokens","kerberos","authenticated","clients","ugi","contain","authentication","method"],"features":{"type":0,"size":1000,"indices":[146,296,301,360,372,373,388,399,420,445,514,605,652,654,656,665,674,710,724,748,805,899,967],"values":[2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The YarnShuffleService  currently just picks a directly in the yarn local dirs to store the leveldb file  YARN added an interface in hadoop     getRecoverPath   to get the location where it should be storing this  We should change to use getRecoveryPath    This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop","_c1":"YarnShuffleService should use YARN getRecoveryPath   for leveldb location","document":"The YarnShuffleService  currently just picks a directly in the yarn local dirs to store the leveldb file  YARN added an interface in hadoop     getRecoverPath   to get the location where it should be storing this  We should change to use getRecoveryPath    This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop YarnShuffleService should use YARN getRecoveryPath   for leveldb location","words":["the","yarnshuffleservice","","currently","just","picks","a","directly","in","the","yarn","local","dirs","to","store","the","leveldb","file","","yarn","added","an","interface","in","hadoop","","","","","getrecoverpath","","","to","get","the","location","where","it","should","be","storing","this","","we","should","change","to","use","getrecoverypath","","","","this","does","mean","we","will","have","to","use","reflection","or","similar","to","check","for","its","existence","though","since","it","doesn","t","exist","before","hadoop","yarnshuffleservice","should","use","yarn","getrecoverypath","","","for","leveldb","location"],"filtered":["yarnshuffleservice","","currently","picks","directly","yarn","local","dirs","store","leveldb","file","","yarn","added","interface","hadoop","","","","","getrecoverpath","","","get","location","storing","","change","use","getrecoverypath","","","","mean","use","reflection","similar","check","existence","though","since","doesn","exist","hadoop","yarnshuffleservice","use","yarn","getrecoverypath","","","leveldb","location"],"features":{"type":0,"size":1000,"indices":[36,108,139,144,148,158,159,170,181,187,188,208,277,296,299,307,372,373,384,388,420,437,445,449,489,495,500,556,564,579,585,589,608,621,651,656,665,698,710,741,743,752,763,777,882,909,910,959,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,14.0,2.0,1.0,5.0,1.0,2.0,3.0,1.0,3.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"The classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes","_c1":"Move o a h record to hadoop streaming","document":"The classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes Move o a h record to hadoop streaming","words":["the","classes","in","o","a","h","record","have","been","deprecated","for","more","than","a","year","and","a","half","","they","should","be","removed","","as","the","first","step","","the","jira","moves","all","these","classes","into","the","hadoop","streaming","project","","which","is","the","only","user","of","these","classes","move","o","a","h","record","to","hadoop","streaming"],"filtered":["classes","o","h","record","deprecated","year","half","","removed","","first","step","","jira","moves","classes","hadoop","streaming","project","","user","classes","move","o","h","record","hadoop","streaming"],"features":{"type":0,"size":1000,"indices":[36,48,170,181,183,261,263,281,282,286,299,333,343,372,388,445,461,512,535,558,572,597,620,629,656,665,671,703,710,780,809,821,880,882,891,899,968],"values":[1.0,1.0,4.0,2.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"The current InternalRow hierarchy makes a difference between immutable and mutable rows  In practice we cannot guarantee that an immutable internal row is immutable  you can always pass a mutable object as an one of its elements   Lets make all internal rows mutable  and reduce the complexity","_c1":"Simplify InternalRow hierarchy","document":"The current InternalRow hierarchy makes a difference between immutable and mutable rows  In practice we cannot guarantee that an immutable internal row is immutable  you can always pass a mutable object as an one of its elements   Lets make all internal rows mutable  and reduce the complexity Simplify InternalRow hierarchy","words":["the","current","internalrow","hierarchy","makes","a","difference","between","immutable","and","mutable","rows","","in","practice","we","cannot","guarantee","that","an","immutable","internal","row","is","immutable","","you","can","always","pass","a","mutable","object","as","an","one","of","its","elements","","","lets","make","all","internal","rows","mutable","","and","reduce","the","complexity","simplify","internalrow","hierarchy"],"filtered":["current","internalrow","hierarchy","makes","difference","immutable","mutable","rows","","practice","guarantee","immutable","internal","row","immutable","","always","pass","mutable","object","one","elements","","","lets","make","internal","rows","mutable","","reduce","complexity","simplify","internalrow","hierarchy"],"features":{"type":0,"size":1000,"indices":[13,19,44,97,170,263,280,281,295,296,329,333,343,372,392,425,445,481,501,502,525,530,541,572,596,650,670,676,691,710,752,760,788,833,921,931,968,993],"values":[1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results","_c1":"Allow custom timing control in microbenchmarks","document":"The current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results Allow custom timing control in microbenchmarks","words":["the","current","benchmark","framework","runs","a","code","block","for","several","iterations","and","reports","statistics","","however","there","is","no","way","to","exclude","per","iteration","setup","time","from","the","overall","results","allow","custom","timing","control","in","microbenchmarks"],"filtered":["current","benchmark","framework","runs","code","block","several","iterations","reports","statistics","","however","way","exclude","per","iteration","setup","time","overall","results","allow","custom","timing","control","microbenchmarks"],"features":{"type":0,"size":1000,"indices":[36,96,106,142,157,159,170,231,281,333,337,346,363,372,388,420,435,440,445,457,511,547,599,620,673,675,695,710,739,792,831,874,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess","_c1":"Considering output for statistics of logical plan","document":"The current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess Considering output for statistics of logical plan","words":["the","current","implementation","of","statistics","of","unarynode","does","not","considering","output","","for","example","","project","","","we","should","considering","it","to","have","a","better","guess","considering","output","for","statistics","of","logical","plan"],"filtered":["current","implementation","statistics","unarynode","considering","output","","example","","project","","","considering","better","guess","considering","output","statistics","logical","plan"],"features":{"type":0,"size":1000,"indices":[18,36,122,123,142,170,243,247,299,343,372,373,388,495,597,665,671,698,710,941,993],"values":[1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The current way we generate these build artifacts is awful  Plus they are ugly and  in the case of release notes  very hard to pick out what is important","_c1":"Rework the changelog and releasenotes","document":"The current way we generate these build artifacts is awful  Plus they are ugly and  in the case of release notes  very hard to pick out what is important Rework the changelog and releasenotes","words":["the","current","way","we","generate","these","build","artifacts","is","awful","","plus","they","are","ugly","and","","in","the","case","of","release","notes","","very","hard","to","pick","out","what","is","important","rework","the","changelog","and","releasenotes"],"filtered":["current","way","generate","build","artifacts","awful","","plus","ugly","","case","release","notes","","hard","pick","important","rework","changelog","releasenotes"],"features":{"type":0,"size":1000,"indices":[48,83,125,138,159,281,318,333,342,343,371,372,388,434,441,445,461,526,536,537,560,654,710,721,830,860,944,948,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided  I have defined multiple local disks defined for a datanode  dfs data dir  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn true When one of those disks breaks and is unmounted then the mountpoint  such as  data    in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed  The only way around this is to alter the configuration and omit that specific disk configuration  To my opinion  It would be more practical to let Hadoop daemons start when at least   disks partition in the provided list is in a usable state  This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing  This might also be configurable that at least X partitions out of he available ones are in OK state","_c1":"Allow daemon startup when at least    or configurable  disk is in an OK state","document":"The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided  I have defined multiple local disks defined for a datanode  dfs data dir  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn true When one of those disks breaks and is unmounted then the mountpoint  such as  data    in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed  The only way around this is to alter the configuration and omit that specific disk configuration  To my opinion  It would be more practical to let Hadoop daemons start when at least   disks partition in the provided list is in a usable state  This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing  This might also be configurable that at least X partitions out of he available ones are in OK state Allow daemon startup when at least    or configurable  disk is in an OK state","words":["the","given","example","is","if","datanode","disk","definitions","but","should","be","applicable","to","all","configuration","where","a","list","of","disks","are","provided","","i","have","defined","multiple","local","disks","defined","for","a","datanode","","dfs","data","dir","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","true","when","one","of","those","disks","breaks","and","is","unmounted","then","the","mountpoint","","such","as","","data","","","","in","this","example","","becomes","a","regular","directory","which","doesn","t","have","the","valid","permissions","and","possible","directory","structure","hadoop","is","expecting","","when","this","situation","happens","","the","datanode","fails","to","restart","because","of","this","while","actually","we","have","enough","disks","in","an","ok","state","to","proceed","","the","only","way","around","this","is","to","alter","the","configuration","and","omit","that","specific","disk","configuration","","to","my","opinion","","it","would","be","more","practical","to","let","hadoop","daemons","start","when","at","least","","","disks","partition","in","the","provided","list","is","in","a","usable","state","","this","prevents","having","to","roll","out","custom","configurations","for","systems","which","have","temporarily","a","disk","","and","therefor","directory","layout","","missing","","this","might","also","be","configurable","that","at","least","x","partitions","out","of","he","available","ones","are","in","ok","state","allow","daemon","startup","when","at","least","","","","or","configurable","","disk","is","in","an","ok","state"],"filtered":["given","example","datanode","disk","definitions","applicable","configuration","list","disks","provided","","defined","multiple","local","disks","defined","datanode","","dfs","data","dir","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","true","one","disks","breaks","unmounted","mountpoint","","","data","","","","example","","becomes","regular","directory","doesn","valid","permissions","possible","directory","structure","hadoop","expecting","","situation","happens","","datanode","fails","restart","actually","enough","disks","ok","state","proceed","","way","around","alter","configuration","omit","specific","disk","configuration","","opinion","","practical","let","hadoop","daemons","start","least","","","disks","partition","provided","list","usable","state","","prevents","roll","custom","configurations","systems","temporarily","disk","","therefor","directory","layout","","missing","","might","also","configurable","least","x","partitions","available","ones","ok","state","allow","daemon","startup","least","","","","configurable","","disk","ok","state"],"features":{"type":0,"size":1000,"indices":[26,36,39,44,50,59,76,79,83,100,109,116,126,138,139,157,159,161,163,164,169,170,173,181,187,188,211,231,243,252,272,275,281,297,299,329,333,343,346,349,364,371,372,373,377,381,388,421,445,447,457,461,467,484,495,498,500,514,525,567,572,576,592,597,600,608,622,629,636,654,656,665,674,675,680,691,692,695,707,710,722,728,731,737,752,756,757,760,764,777,792,794,810,826,829,840,880,892,899,916,919,935,964,968,982,985,992,993,994,996,997],"values":[1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,8.0,6.0,1.0,4.0,1.0,4.0,4.0,2.0,4.0,1.0,1.0,47.0,6.0,1.0,1.0,7.0,1.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,8.0,1.0,7.0,1.0,2.0,2.0,1.0,2.0,3.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0]},"cluster_label":16}
{"_c0":"The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit  This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v  external interface that doesn t involve a complex type hierarchy","_c1":"Simplify accumulators and task metrics","document":"The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit  This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v  external interface that doesn t involve a complex type hierarchy Simplify accumulators and task metrics","words":["the","goal","of","the","ticket","is","to","simplify","both","the","external","interface","and","the","internal","implementation","for","accumulators","and","metrics","","they","are","unnecessarily","convoluted","and","we","should","be","able","to","simplify","them","quite","a","bit","","this","is","an","umbrella","ticket","and","i","will","iteratively","create","new","tasks","as","my","investigation","goes","on","","at","a","high","level","","i","d","would","like","to","create","better","abstractions","for","internal","implementations","","as","well","as","creating","a","simplified","accumulator","v","","external","interface","that","doesn","t","involve","a","complex","type","hierarchy","simplify","accumulators","and","task","metrics"],"filtered":["goal","ticket","simplify","external","interface","internal","implementation","accumulators","metrics","","unnecessarily","convoluted","able","simplify","quite","bit","","umbrella","ticket","iteratively","create","new","tasks","investigation","goes","","high","level","","d","like","create","better","abstractions","internal","implementations","","well","creating","simplified","accumulator","v","","external","interface","doesn","involve","complex","type","hierarchy","simplify","accumulators","task","metrics"],"features":{"type":0,"size":1000,"indices":[9,19,25,36,48,82,89,94,106,116,138,157,163,170,173,210,236,255,265,281,295,329,330,333,340,343,359,372,373,388,392,414,420,443,446,451,472,477,496,500,526,556,572,586,656,665,676,698,710,752,756,760,767,777,848,863,916,924,925,941,977,993],"values":[1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0,2.0,1.0,5.0,1.0,1.0,1.0,6.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for   x","_c1":"Remove hadoop ant from hadoop tools","document":"The hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for   x Remove hadoop ant from hadoop tools","words":["the","hadoop","ant","code","is","an","ancient","kludge","unlikely","to","have","any","users","","still","","we","can","delete","it","from","trunk","as","a","","scream","test","","for","","","x","remove","hadoop","ant","from","hadoop","tools"],"filtered":["hadoop","ant","code","ancient","kludge","unlikely","users","","still","","delete","trunk","","scream","test","","","","x","remove","hadoop","ant","hadoop","tools"],"features":{"type":0,"size":1000,"indices":[36,83,91,110,152,170,181,209,281,288,298,299,372,388,420,495,572,586,710,752,755,800,810,820,833,921,953,970,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The metrics system in the JobTracker is defaulting to every   seconds computing all of the counters for all of the jobs  This work is a substantial amount of work showing up as running in     of the snapshots that I ve seen  I d like to lower the default interval to once every    seconds and make it a low priority thread","_c1":"the metrics system in the job tracker is running too often","document":"The metrics system in the JobTracker is defaulting to every   seconds computing all of the counters for all of the jobs  This work is a substantial amount of work showing up as running in     of the snapshots that I ve seen  I d like to lower the default interval to once every    seconds and make it a low priority thread the metrics system in the job tracker is running too often","words":["the","metrics","system","in","the","jobtracker","is","defaulting","to","every","","","seconds","computing","all","of","the","counters","for","all","of","the","jobs","","this","work","is","a","substantial","amount","of","work","showing","up","as","running","in","","","","","of","the","snapshots","that","i","ve","seen","","i","d","like","to","lower","the","default","interval","to","once","every","","","","seconds","and","make","it","a","low","priority","thread","the","metrics","system","in","the","job","tracker","is","running","too","often"],"filtered":["metrics","system","jobtracker","defaulting","every","","","seconds","computing","counters","jobs","","work","substantial","amount","work","showing","running","","","","","snapshots","ve","seen","","d","like","lower","default","interval","every","","","","seconds","make","low","priority","thread","metrics","system","job","tracker","running","often"],"features":{"type":0,"size":1000,"indices":[0,8,31,36,94,106,115,119,128,147,170,235,281,316,326,329,330,333,343,344,349,351,372,373,381,388,401,445,470,495,525,527,537,572,617,618,639,644,667,710,760,805,817,818,936,963,968],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,11.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,8.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0]},"cluster_label":19}
{"_c0":"The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and  even after HADOOP        is not thread safe  both start and stop are potentially re entrant   It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle  which  for all Yarn services  is the YARN app lifecycle  as implemented in Hadoop common  Making the monitor a subclass of   AbstractService   and moving the init start   stop operations in   serviceInit        serviceStart         serviceStop     methods will fix the concurrency and state model issues  and make it trivial to add as a child to any YARN service which subclasses   CompositeService    most the NM and RM apps  will be able to hook up the monitor simply by creating one in the ctor and adding it as a child","_c1":"Make JvmPauseMonitor an AbstractService","document":"The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and  even after HADOOP        is not thread safe  both start and stop are potentially re entrant   It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle  which  for all Yarn services  is the YARN app lifecycle  as implemented in Hadoop common  Making the monitor a subclass of   AbstractService   and moving the init start   stop operations in   serviceInit        serviceStart         serviceStop     methods will fix the concurrency and state model issues  and make it trivial to add as a child to any YARN service which subclasses   CompositeService    most the NM and RM apps  will be able to hook up the monitor simply by creating one in the ctor and adding it as a child Make JvmPauseMonitor an AbstractService","words":["the","new","jvm","pause","monitor","has","been","written","with","its","own","start","stop","lifecycle","which","has","already","proven","brittle","to","both","ordering","of","operations","and","","even","after","hadoop","","","","","","","","is","not","thread","safe","","both","start","and","stop","are","potentially","re","entrant","","","it","also","requires","every","class","which","supports","the","monitor","to","add","another","field","and","perform","the","lifecycle","operations","in","its","own","lifecycle","","which","","for","all","yarn","services","","is","the","yarn","app","lifecycle","","as","implemented","in","hadoop","common","","making","the","monitor","a","subclass","of","","","abstractservice","","","and","moving","the","init","start","","","stop","operations","in","","","serviceinit","","","","","","","","servicestart","","","","","","","","","servicestop","","","","","methods","will","fix","the","concurrency","and","state","model","issues","","and","make","it","trivial","to","add","as","a","child","to","any","yarn","service","which","subclasses","","","compositeservice","","","","most","the","nm","and","rm","apps","","will","be","able","to","hook","up","the","monitor","simply","by","creating","one","in","the","ctor","and","adding","it","as","a","child","make","jvmpausemonitor","an","abstractservice"],"filtered":["new","jvm","pause","monitor","written","start","stop","lifecycle","already","proven","brittle","ordering","operations","","even","hadoop","","","","","","","","thread","safe","","start","stop","potentially","re","entrant","","","also","requires","every","class","supports","monitor","add","another","field","perform","lifecycle","operations","lifecycle","","","yarn","services","","yarn","app","lifecycle","","implemented","hadoop","common","","making","monitor","subclass","","","abstractservice","","","moving","init","start","","","stop","operations","","","serviceinit","","","","","","","","servicestart","","","","","","","","","servicestop","","","","","methods","fix","concurrency","state","model","issues","","make","trivial","add","child","yarn","service","subclasses","","","compositeservice","","","","nm","rm","apps","","able","hook","monitor","simply","creating","one","ctor","adding","child","make","jvmpausemonitor","abstractservice"],"features":{"type":0,"size":1000,"indices":[18,25,36,44,57,75,77,79,86,89,91,94,105,124,128,129,138,170,177,181,205,214,223,228,231,247,269,281,286,296,300,319,323,333,343,349,356,372,388,406,413,420,425,432,445,475,492,495,496,498,513,525,534,535,545,547,564,572,580,597,629,630,642,650,656,674,710,735,752,753,765,767,770,775,779,792,796,805,809,857,863,936,938,954,968,994,996],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,8.0,2.0,1.0,1.0,50.0,5.0,2.0,1.0,2.0,1.0,2.0,5.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,4.0,10.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,4.0]},"cluster_label":7}
{"_c0":"The new Tungsten execution engine has very robust memory management and speed for simple data types  It does  however  suffer from the following    For user defined aggregates  Hive UDAFs  Dataset typed operators   it is fairly expensive to fit into the Tungsten internal format    For aggregate functions that require complex intermediate data structures  Unsafe  on raw bytes  is not a good programming abstraction due to the lack of structs  The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases  This operator  however  should limit its memory usage to avoid putting too much pressure on GC  e g  falling back to sort based aggregate as soon the number of objects exceeds a very low threshold  Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark","_c1":"Introduce a JVM object based aggregate operator","document":"The new Tungsten execution engine has very robust memory management and speed for simple data types  It does  however  suffer from the following    For user defined aggregates  Hive UDAFs  Dataset typed operators   it is fairly expensive to fit into the Tungsten internal format    For aggregate functions that require complex intermediate data structures  Unsafe  on raw bytes  is not a good programming abstraction due to the lack of structs  The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases  This operator  however  should limit its memory usage to avoid putting too much pressure on GC  e g  falling back to sort based aggregate as soon the number of objects exceeds a very low threshold  Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark Introduce a JVM object based aggregate operator","words":["the","new","tungsten","execution","engine","has","very","robust","memory","management","and","speed","for","simple","data","types","","it","does","","however","","suffer","from","the","following","","","","for","user","defined","aggregates","","hive","udafs","","dataset","typed","operators","","","it","is","fairly","expensive","to","fit","into","the","tungsten","internal","format","","","","for","aggregate","functions","that","require","complex","intermediate","data","structures","","unsafe","","on","raw","bytes","","is","not","a","good","programming","abstraction","due","to","the","lack","of","structs","","the","idea","here","is","to","introduce","a","jvm","object","based","hash","aggregate","operator","that","can","support","the","aforementioned","use","cases","","this","operator","","however","","should","limit","its","memory","usage","to","avoid","putting","too","much","pressure","on","gc","","e","g","","falling","back","to","sort","based","aggregate","as","soon","the","number","of","objects","exceeds","a","very","low","threshold","","internally","at","databricks","we","prototyped","a","version","of","this","for","a","customer","poc","and","have","observed","substantial","speed","ups","over","existing","spark","introduce","a","jvm","object","based","aggregate","operator"],"filtered":["new","tungsten","execution","engine","robust","memory","management","speed","simple","data","types","","","however","","suffer","following","","","","user","defined","aggregates","","hive","udafs","","dataset","typed","operators","","","fairly","expensive","fit","tungsten","internal","format","","","","aggregate","functions","require","complex","intermediate","data","structures","","unsafe","","raw","bytes","","good","programming","abstraction","due","lack","structs","","idea","introduce","jvm","object","based","hash","aggregate","operator","support","aforementioned","use","cases","","operator","","however","","limit","memory","usage","avoid","putting","much","pressure","gc","","e","g","","falling","back","sort","based","aggregate","soon","number","objects","exceeds","low","threshold","","internally","databricks","prototyped","version","customer","poc","observed","substantial","speed","ups","existing","spark","introduce","jvm","object","based","aggregate","operator"],"features":{"type":0,"size":1000,"indices":[9,18,25,36,41,50,62,81,82,91,105,109,110,126,135,140,154,160,168,170,172,183,198,199,222,223,229,242,258,274,281,284,295,296,299,300,316,333,336,343,352,365,371,372,373,374,375,385,388,409,413,417,419,430,437,438,465,485,489,493,495,499,524,525,558,568,572,576,580,583,586,587,599,617,618,625,641,644,650,661,665,673,695,698,710,720,756,760,788,809,832,833,843,851,878,882,889,891,896,911,921,940,942,944,958,980,993,995],"values":[1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,6.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,23.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,7.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":10}
{"_c0":"The removed codes are not reachable  because  InConversion  already resolve the type coercion issues","_c1":"Remove IN type coercion from PromoteStrings","document":"The removed codes are not reachable  because  InConversion  already resolve the type coercion issues Remove IN type coercion from PromoteStrings","words":["the","removed","codes","are","not","reachable","","because","","inconversion","","already","resolve","the","type","coercion","issues","remove","in","type","coercion","from","promotestrings"],"filtered":["removed","codes","reachable","","","inconversion","","already","resolve","type","coercion","issues","remove","type","coercion","promotestrings"],"features":{"type":0,"size":1000,"indices":[18,57,138,221,288,372,421,445,475,490,512,526,621,710,827,830,921,924],"values":[1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP      added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics  As per  RFC      http   www w  org Protocols rfc     rfc     html    HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information    This mechanism is initiated by server sending the      Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI    In case server supports multiple authentication schemes  it may return multiple challenges with a      Authenticate  response  and each challenge may use a different auth scheme    A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge  The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc       Understanding HTTP Authentication https   msdn microsoft com en us library ms         v vs        aspx  On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication  Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags     Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication","_c1":"Support multiple authentication schemes via AuthenticationFilter","document":"The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP      added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics  As per  RFC      http   www w  org Protocols rfc     rfc     html    HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information    This mechanism is initiated by server sending the      Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI    In case server supports multiple authentication schemes  it may return multiple challenges with a      Authenticate  response  and each challenge may use a different auth scheme    A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge  The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc       Understanding HTTP Authentication https   msdn microsoft com en us library ms         v vs        aspx  On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication  Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags     Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication Support multiple authentication schemes via AuthenticationFilter","words":["the","requirement","is","to","support","ldap","based","authentication","scheme","via","hadoop","authenticationfilter","","hadoop","","","","","","added","a","support","to","plug","in","custom","authentication","scheme","","in","addition","to","kerberos","","via","altkerberosauthenticationhandler","class","","but","it","is","based","on","selecting","the","authentication","mechanism","based","on","user","agent","http","header","which","does","not","conform","to","http","protocol","semantics","","as","per","","rfc","","","","","","http","","","www","w","","org","protocols","rfc","","","","","rfc","","","","","html","","","","http","protocol","provides","a","simple","challenge","response","authentication","mechanism","that","can","be","used","by","a","server","to","challenge","a","client","request","and","by","a","client","to","provide","the","necessary","authentication","information","","","","this","mechanism","is","initiated","by","server","sending","the","","","","","","authenticate","","response","with","","www","authenticate","","header","which","includes","at","least","one","challenge","that","indicates","the","authentication","scheme","s","","and","parameters","applicable","to","the","request","uri","","","","in","case","server","supports","multiple","authentication","schemes","","it","may","return","multiple","challenges","with","a","","","","","","authenticate","","response","","and","each","challenge","may","use","a","different","auth","scheme","","","","a","user","agent","must","choose","to","use","the","strongest","auth","scheme","it","understands","and","request","credentials","from","the","user","based","upon","that","challenge","","the","existing","hadoop","authentication","filter","implementation","supports","kerberos","authentication","scheme","and","uses","","negotiate","","as","the","challenge","as","part","of","","www","authenticate","","response","header","","as","per","the","following","documentation","","","negotiate","","challenge","scheme","is","only","applicable","to","kerberos","","and","windows","ntlm","","authentication","schemes","","","spnego","based","kerberos","and","ntlm","http","authentication","http","","","tools","ietf","org","html","rfc","","","","","","","understanding","http","authentication","https","","","msdn","microsoft","com","en","us","library","ms","","","","","","","","","v","vs","","","","","","","","aspx","","on","the","other","hand","for","ldap","authentication","","typically","","basic","","authentication","scheme","is","used","","note","tls","is","mandatory","with","basic","authentication","scheme","","","http","","","httpd","apache","org","docs","trunk","mod","mod","authnz","ldap","html","hence","for","this","feature","","the","idea","would","be","to","provide","a","custom","implementation","of","hadoop","authenticationhandler","and","authenticator","interfaces","which","would","support","both","schemes","","","kerberos","","via","negotiate","auth","challenge","","and","ldap","","via","basic","auth","challenge","","","during","the","authentication","phase","","it","would","send","both","the","challenges","and","let","client","pick","the","appropriate","one","","if","client","responds","with","an","","authorization","","header","tagged","with","","negotiate","","","","it","will","use","kerberos","authentication","","if","client","responds","with","an","","authorization","","header","tagged","with","","basic","","","","it","will","use","ldap","authentication","","note","","","some","http","clients","","e","g","","curl","or","apache","http","java","client","","need","to","be","configured","to","use","one","scheme","over","the","other","e","g","","","","curl","tool","supports","option","to","use","either","kerberos","","via","","","negotiate","flag","","or","username","password","based","authentication","","via","","","basic","and","","u","flags","","","","","apache","httpclient","library","can","be","configured","to","use","specific","authentication","scheme","","http","","","hc","apache","org","httpcomponents","client","ga","tutorial","html","authentication","html","typically","web","browsers","automatically","choose","an","authentication","scheme","based","on","a","notion","of","","strength","","of","security","","e","g","","take","a","look","at","the","","design","of","chrome","browser","for","http","authentication","https","","","www","chromium","org","developers","design","documents","http","authentication","support","multiple","authentication","schemes","via","authenticationfilter"],"filtered":["requirement","support","ldap","based","authentication","scheme","via","hadoop","authenticationfilter","","hadoop","","","","","","added","support","plug","custom","authentication","scheme","","addition","kerberos","","via","altkerberosauthenticationhandler","class","","based","selecting","authentication","mechanism","based","user","agent","http","header","conform","http","protocol","semantics","","per","","rfc","","","","","","http","","","www","w","","org","protocols","rfc","","","","","rfc","","","","","html","","","","http","protocol","provides","simple","challenge","response","authentication","mechanism","used","server","challenge","client","request","client","provide","necessary","authentication","information","","","","mechanism","initiated","server","sending","","","","","","authenticate","","response","","www","authenticate","","header","includes","least","one","challenge","indicates","authentication","scheme","","parameters","applicable","request","uri","","","","case","server","supports","multiple","authentication","schemes","","may","return","multiple","challenges","","","","","","authenticate","","response","","challenge","may","use","different","auth","scheme","","","","user","agent","must","choose","use","strongest","auth","scheme","understands","request","credentials","user","based","upon","challenge","","existing","hadoop","authentication","filter","implementation","supports","kerberos","authentication","scheme","uses","","negotiate","","challenge","part","","www","authenticate","","response","header","","per","following","documentation","","","negotiate","","challenge","scheme","applicable","kerberos","","windows","ntlm","","authentication","schemes","","","spnego","based","kerberos","ntlm","http","authentication","http","","","tools","ietf","org","html","rfc","","","","","","","understanding","http","authentication","https","","","msdn","microsoft","com","en","us","library","ms","","","","","","","","","v","vs","","","","","","","","aspx","","hand","ldap","authentication","","typically","","basic","","authentication","scheme","used","","note","tls","mandatory","basic","authentication","scheme","","","http","","","httpd","apache","org","docs","trunk","mod","mod","authnz","ldap","html","hence","feature","","idea","provide","custom","implementation","hadoop","authenticationhandler","authenticator","interfaces","support","schemes","","","kerberos","","via","negotiate","auth","challenge","","ldap","","via","basic","auth","challenge","","","authentication","phase","","send","challenges","let","client","pick","appropriate","one","","client","responds","","authorization","","header","tagged","","negotiate","","","","use","kerberos","authentication","","client","responds","","authorization","","header","tagged","","basic","","","","use","ldap","authentication","","note","","","http","clients","","e","g","","curl","apache","http","java","client","","need","configured","use","one","scheme","e","g","","","","curl","tool","supports","option","use","either","kerberos","","via","","","negotiate","flag","","username","password","based","authentication","","via","","","basic","","u","flags","","","","","apache","httpclient","library","configured","use","specific","authentication","scheme","","http","","","hc","apache","org","httpcomponents","client","ga","tutorial","html","authentication","html","typically","web","browsers","automatically","choose","authentication","scheme","based","notion","","strength","","security","","e","g","","take","look","","design","chrome","browser","http","authentication","https","","","www","chromium","org","developers","design","documents","http","authentication","support","multiple","authentication","schemes","via","authenticationfilter"],"features":{"type":0,"size":1000,"indices":[3,18,23,31,36,43,44,50,59,80,82,83,84,89,91,92,100,111,118,124,135,146,163,164,170,181,187,189,197,205,208,221,222,223,224,226,229,232,239,261,275,277,281,287,288,314,315,333,342,343,346,352,366,371,372,373,384,388,399,400,409,411,417,418,420,423,428,430,440,441,445,446,452,467,477,489,491,495,499,505,507,523,526,528,534,535,537,540,545,572,592,595,597,605,610,614,615,625,627,634,640,645,646,650,652,653,656,659,665,666,674,695,697,698,704,706,710,722,724,729,735,736,740,752,756,757,760,764,771,788,793,798,800,801,805,808,811,812,825,833,842,848,852,855,863,865,870,878,881,882,885,890,894,899,909,915,921,925,932,939,945,956,957,958,960,967,970,978,979,980,992,994,998,999],"values":[3.0,1.0,1.0,2.0,3.0,2.0,3.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,7.0,25.0,3.0,1.0,14.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,1.0,5.0,1.0,6.0,1.0,2.0,1.0,1.0,11.0,1.0,5.0,2.0,1.0,1.0,1.0,158.0,2.0,2.0,14.0,7.0,1.0,1.0,3.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,7.0,2.0,4.0,2.0,1.0,8.0,1.0,10.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,5.0,3.0,7.0,3.0,3.0,4.0,1.0,2.0,7.0,2.0,1.0,1.0,1.0,1.0,7.0,5.0,1.0,4.0,1.0,14.0,2.0,2.0,4.0,6.0,3.0,5.0,2.0,18.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,9.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,12.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0]},"cluster_label":6}
{"_c0":"The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed","_c1":"Ability to clean up subprocesses spawned by Shell when the process exits","document":"The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed Ability to clean up subprocesses spawned by Shell when the process exits","words":["the","runcommand","code","in","shell","java","can","get","into","a","situation","where","it","will","ignore","interruptedexceptions","and","refuse","to","shutdown","due","to","being","in","i","o","waiting","for","the","return","value","of","the","subprocess","that","was","spawned","","we","need","to","allow","for","the","subprocess","to","be","interrupted","and","killed","when","the","shell","process","gets","killed","","currently","the","jvm","will","shutdown","and","all","of","the","subprocesses","will","be","orphaned","and","not","killed","ability","to","clean","up","subprocesses","spawned","by","shell","when","the","process","exits"],"filtered":["runcommand","code","shell","java","get","situation","ignore","interruptedexceptions","refuse","shutdown","due","o","waiting","return","value","subprocess","spawned","","need","allow","subprocess","interrupted","killed","shell","process","gets","killed","","currently","jvm","shutdown","subprocesses","orphaned","killed","ability","clean","subprocesses","spawned","shell","process","exits"],"features":{"type":0,"size":1000,"indices":[18,22,36,76,113,118,123,128,139,170,218,223,231,234,274,300,313,329,333,340,343,352,372,374,388,420,439,445,495,537,556,568,598,622,656,688,697,710,753,760,763,768,833,859,880,891,911,941,959,967,968,993],"values":[1.0,2.0,2.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,2.0,2.0,1.0,5.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions  Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems","_c1":"Add support for DataFrameStatFunctions in SparkR","document":"The stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions  Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems Add support for DataFrameStatFunctions in SparkR","words":["the","stat","functions","are","defined","in","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframestatfunctions","","currently","only","crosstab","","","is","supported","","functions","to","be","supported","include","","corr","","cov","","freqitems","add","support","for","dataframestatfunctions","in","sparkr"],"filtered":["stat","functions","defined","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframestatfunctions","","currently","crosstab","","","supported","","functions","supported","include","","corr","","cov","","freqitems","add","support","dataframestatfunctions","sparkr"],"features":{"type":0,"size":1000,"indices":[36,105,126,138,281,286,306,307,372,388,401,432,445,490,495,498,535,544,545,552,587,593,597,610,644,652,656,665,686,695,710,763,767,899],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"The system should be able to read in user defined env vars from    hadooprc","_c1":"Add support for  hadooprc","document":"The system should be able to read in user defined env vars from    hadooprc Add support for  hadooprc","words":["the","system","should","be","able","to","read","in","user","defined","env","vars","from","","","","hadooprc","add","support","for","","hadooprc"],"filtered":["system","able","read","user","defined","env","vars","","","","hadooprc","add","support","","hadooprc"],"features":{"type":0,"size":1000,"indices":[36,126,136,372,388,432,445,470,496,537,639,650,656,665,695,710,882,921],"values":[1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame","_c1":"Add toLocalIterator for Dataset","document":"The toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame Add toLocalIterator for Dataset","words":["the","tolocaliterator","of","rdd","is","super","slow","","we","should","have","a","optimized","implementation","for","dataset","dataframe","add","tolocaliterator","for","dataset"],"filtered":["tolocaliterator","rdd","super","slow","","optimized","implementation","dataset","dataframe","add","tolocaliterator","dataset"],"features":{"type":0,"size":1000,"indices":[36,64,161,170,203,281,299,343,372,432,493,501,659,665,698,710,870,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge  Currently  the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing  refer the method mergeApplicationListing fileStatus  FileStatus     The process of replay involves    each line in the event log being read as a string    parsing the string to a Json structure   converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive  Tests show that majority of time spent in replay is in doing this work  When the replay is performed for building the application listing  the only two events that the code really cares for are  SparkListenerApplicationStart  and  SparkListenerApplicationEnd    since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener  This means that when processing an event log file with a huge number  hundreds of thousands  can be more  of events  the work done to deserialize all of these event  and then replay them is not needed  Only two events are what we re interested in  and this can be used to ensure that when replay is performed for the purpose of building the application list  we only make the effort to replay these two events and not others  My tests show that this drastically improves application list load time  For a    MB event log from a user  with over         events  the load time  local on my mac  comes down from about    secs to under   second using this approach  For customers that typically execute applications with large event logs  and thus have multiple large event logs present  this can speed up how soon the history server UI lists the apps considerably  I will be updating a pull request with take at fixing this","_c1":"Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page","document":"There are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge  Currently  the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing  refer the method mergeApplicationListing fileStatus  FileStatus     The process of replay involves    each line in the event log being read as a string    parsing the string to a Json structure   converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive  Tests show that majority of time spent in replay is in doing this work  When the replay is performed for building the application listing  the only two events that the code really cares for are  SparkListenerApplicationStart  and  SparkListenerApplicationEnd    since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener  This means that when processing an event log file with a huge number  hundreds of thousands  can be more  of events  the work done to deserialize all of these event  and then replay them is not needed  Only two events are what we re interested in  and this can be used to ensure that when replay is performed for the purpose of building the application list  we only make the effort to replay these two events and not others  My tests show that this drastically improves application list load time  For a    MB event log from a user  with over         events  the load time  local on my mac  comes down from about    secs to under   second using this approach  For customers that typically execute applications with large event logs  and thus have multiple large event logs present  this can speed up how soon the history server UI lists the apps considerably  I will be updating a pull request with take at fixing this Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page","words":["there","are","known","complaints","cribs","about","history","server","s","application","list","not","updating","quickly","enough","when","the","event","log","files","that","need","replay","are","huge","","currently","","the","fshistoryprovider","design","causes","the","entire","event","log","file","to","be","replayed","when","building","the","initial","application","listing","","refer","the","method","mergeapplicationlisting","filestatus","","filestatus","","","","","the","process","of","replay","involves","","","","each","line","in","the","event","log","being","read","as","a","string","","","","parsing","the","string","to","a","json","structure","","","converting","the","json","to","the","corresponding","scala","classes","with","nested","structures","particularly","the","part","involving","parsing","string","to","json","and","then","to","scala","classes","is","expensive","","tests","show","that","majority","of","time","spent","in","replay","is","in","doing","this","work","","when","the","replay","is","performed","for","building","the","application","listing","","the","only","two","events","that","the","code","really","cares","for","are","","sparklistenerapplicationstart","","and","","sparklistenerapplicationend","","","","since","the","only","listener","attached","to","the","replaylistenerbus","at","that","point","is","the","applicationeventlistener","","this","means","that","when","processing","an","event","log","file","with","a","huge","number","","hundreds","of","thousands","","can","be","more","","of","events","","the","work","done","to","deserialize","all","of","these","event","","and","then","replay","them","is","not","needed","","only","two","events","are","what","we","re","interested","in","","and","this","can","be","used","to","ensure","that","when","replay","is","performed","for","the","purpose","of","building","the","application","list","","we","only","make","the","effort","to","replay","these","two","events","and","not","others","","my","tests","show","that","this","drastically","improves","application","list","load","time","","for","a","","","","mb","event","log","from","a","user","","with","over","","","","","","","","","events","","the","load","time","","local","on","my","mac","","comes","down","from","about","","","","secs","to","under","","","second","using","this","approach","","for","customers","that","typically","execute","applications","with","large","event","logs","","and","thus","have","multiple","large","event","logs","present","","this","can","speed","up","how","soon","the","history","server","ui","lists","the","apps","considerably","","i","will","be","updating","a","pull","request","with","take","at","fixing","this","remove","unneeded","heavy","work","performed","by","fshistoryprovider","for","building","up","the","application","listing","ui","page"],"filtered":["known","complaints","cribs","history","server","application","list","updating","quickly","enough","event","log","files","need","replay","huge","","currently","","fshistoryprovider","design","causes","entire","event","log","file","replayed","building","initial","application","listing","","refer","method","mergeapplicationlisting","filestatus","","filestatus","","","","","process","replay","involves","","","","line","event","log","read","string","","","","parsing","string","json","structure","","","converting","json","corresponding","scala","classes","nested","structures","particularly","part","involving","parsing","string","json","scala","classes","expensive","","tests","show","majority","time","spent","replay","work","","replay","performed","building","application","listing","","two","events","code","really","cares","","sparklistenerapplicationstart","","","sparklistenerapplicationend","","","","since","listener","attached","replaylistenerbus","point","applicationeventlistener","","means","processing","event","log","file","huge","number","","hundreds","thousands","","","events","","work","done","deserialize","event","","replay","needed","","two","events","re","interested","","used","ensure","replay","performed","purpose","building","application","list","","make","effort","replay","two","events","others","","tests","show","drastically","improves","application","list","load","time","","","","","mb","event","log","user","","","","","","","","","","events","","load","time","","local","mac","","comes","","","","secs","","","second","using","approach","","customers","typically","execute","applications","large","event","logs","","thus","multiple","large","event","logs","present","","speed","soon","history","server","ui","lists","apps","considerably","","updating","pull","request","take","fixing","remove","unneeded","heavy","work","performed","fshistoryprovider","building","application","listing","ui","page"],"features":{"type":0,"size":1000,"indices":[3,18,19,22,36,39,40,41,51,60,62,64,74,76,82,87,89,92,94,95,100,108,113,118,126,128,138,149,157,165,169,170,173,182,189,197,198,214,217,219,223,237,244,258,281,288,293,299,310,329,333,334,343,352,353,371,372,373,374,378,381,382,388,394,400,408,411,420,425,437,443,444,445,461,462,490,492,505,511,514,517,520,524,525,526,527,536,537,545,546,551,558,568,572,583,585,587,592,593,597,599,605,608,610,619,624,626,629,631,640,650,654,656,662,684,693,706,707,708,710,728,740,742,752,756,760,763,770,775,776,777,782,809,827,831,833,852,855,866,876,880,882,885,888,899,902,905,916,921,924,942,960,968,983,984,993],"values":[1.0,4.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,3.0,1.0,6.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,6.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,6.0,1.0,1.0,8.0,60.0,7.0,1.0,1.0,2.0,1.0,10.0,1.0,1.0,3.0,2.0,2.0,1.0,3.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,6.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,7.0,26.0,3.0,1.0,1.0,1.0,3.0,8.0,1.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,3.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]},"cluster_label":18}
{"_c0":"There are some duplicated code for options  we should simplify them","_c1":"Cleanup options for DataFrame reader API in Python","document":"There are some duplicated code for options  we should simplify them Cleanup options for DataFrame reader API in Python","words":["there","are","some","duplicated","code","for","options","","we","should","simplify","them","cleanup","options","for","dataframe","reader","api","in","python"],"filtered":["duplicated","code","options","","simplify","cleanup","options","dataframe","reader","api","python"],"features":{"type":0,"size":1000,"indices":[19,36,138,161,333,372,400,420,445,536,589,644,651,665,831,843,924,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There could be same subquery within a single query  we could reuse the result without running it multiple times","_c1":"Reuse subqueries within single query","document":"There could be same subquery within a single query  we could reuse the result without running it multiple times Reuse subqueries within single query","words":["there","could","be","same","subquery","within","a","single","query","","we","could","reuse","the","result","without","running","it","multiple","times","reuse","subqueries","within","single","query"],"filtered":["subquery","within","single","query","","reuse","result","without","running","multiple","times","reuse","subqueries","within","single","query"],"features":{"type":0,"size":1000,"indices":[170,198,213,242,372,412,475,495,531,592,656,673,674,710,831,865,884,963,993],"values":[1.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There have been continuing requests  e g    SPARK        for allowing users to extend and modify MLlib models and algorithms  If you are a user who needs these changes  please comment here about what specifically needs to be modified for your use case","_c1":"Remove final from classes in spark ml trees and ensembles where possible","document":"There have been continuing requests  e g    SPARK        for allowing users to extend and modify MLlib models and algorithms  If you are a user who needs these changes  please comment here about what specifically needs to be modified for your use case Remove final from classes in spark ml trees and ensembles where possible","words":["there","have","been","continuing","requests","","e","g","","","","spark","","","","","","","","for","allowing","users","to","extend","and","modify","mllib","models","and","algorithms","","if","you","are","a","user","who","needs","these","changes","","please","comment","here","about","what","specifically","needs","to","be","modified","for","your","use","case","remove","final","from","classes","in","spark","ml","trees","and","ensembles","where","possible"],"filtered":["continuing","requests","","e","g","","","","spark","","","","","","","","allowing","users","extend","modify","mllib","models","algorithms","","user","needs","changes","","please","comment","specifically","needs","modified","use","case","remove","final","classes","spark","ml","trees","ensembles","possible"],"features":{"type":0,"size":1000,"indices":[36,55,71,100,105,135,138,139,140,170,193,217,235,254,263,288,294,299,317,324,333,342,363,372,388,417,425,445,461,489,517,521,526,535,616,656,666,722,755,758,777,809,831,834,841,878,882,921],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,13.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown  As we upgrade to Parquet       which includes the fix for the pushdown of optional columns  we don t need this metadata now","_c1":"Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown","document":"There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown  As we upgrade to Parquet       which includes the fix for the pushdown of optional columns  we don t need this metadata now Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown","words":["there","is","a","metadata","introduced","before","to","mark","the","optional","columns","in","merged","parquet","schema","for","filter","predicate","pushdown","","as","we","upgrade","to","parquet","","","","","","","which","includes","the","fix","for","the","pushdown","of","optional","columns","","we","don","t","need","this","metadata","now","remove","the","metadata","used","to","mark","optional","columns","in","merged","parquet","schema","for","filter","predicate","pushdown"],"filtered":["metadata","introduced","mark","optional","columns","merged","parquet","schema","filter","predicate","pushdown","","upgrade","parquet","","","","","","","includes","fix","pushdown","optional","columns","","need","metadata","remove","metadata","used","mark","optional","columns","merged","parquet","schema","filter","predicate","pushdown"],"features":{"type":0,"size":1000,"indices":[36,46,98,159,170,172,204,242,281,288,343,372,373,388,441,445,508,509,537,572,597,605,607,609,640,710,713,777,798,831,852,969,993],"values":[3.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,2.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,4.0,1.0,1.0,3.0,1.0,1.0,3.0,2.0]},"cluster_label":9}
{"_c0":"There is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths  We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory  Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives","_c1":"support wildcard in libjars argument","document":"There is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths  We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory  Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives support wildcard in libjars argument","words":["there","is","a","problem","when","a","user","job","adds","too","many","dependency","jars","in","their","command","line","","the","hadoop","classpath","part","can","be","addressed","","including","using","wildcards","","","","","","","but","the","same","cannot","be","done","with","the","","libjars","argument","","today","it","takes","only","fully","specified","file","paths","","we","may","want","to","consider","supporting","wildcards","as","a","way","to","help","users","in","this","situation","","the","idea","is","to","handle","it","the","same","way","the","jvm","does","it","","","","","expands","to","the","list","of","jars","in","that","directory","","it","does","not","traverse","into","any","child","directory","","also","","it","probably","would","be","a","good","idea","to","do","it","only","for","libjars","","i","e","","don","t","do","it","for","","files","and","","archives","support","wildcard","in","libjars","argument"],"filtered":["problem","user","job","adds","many","dependency","jars","command","line","","hadoop","classpath","part","addressed","","including","using","wildcards","","","","","","","done","","libjars","argument","","today","takes","fully","specified","file","paths","","may","want","consider","supporting","wildcards","way","help","users","situation","","idea","handle","way","jvm","","","","","expands","list","jars","directory","","traverse","child","directory","","also","","probably","good","idea","libjars","","e","","","files","","archives","support","wildcard","libjars","argument"],"features":{"type":0,"size":1000,"indices":[18,25,36,48,56,76,83,91,101,108,110,116,135,159,163,168,170,175,181,182,188,200,204,235,254,281,297,300,312,329,333,343,347,360,372,373,388,423,433,445,462,470,478,495,496,534,551,572,588,622,624,644,650,656,666,682,695,698,707,710,712,728,735,740,755,760,777,792,831,833,878,882,891,899,931,954,958,985,992,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,23.0,1.0,5.0,3.0,1.0,4.0,1.0,1.0,1.0,7.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":10}
{"_c0":"There is little to no reason for it to call hadoop daemon sh anymore","_c1":"hadoop daemons sh should just call hdfs directly","document":"There is little to no reason for it to call hadoop daemon sh anymore hadoop daemons sh should just call hdfs directly","words":["there","is","little","to","no","reason","for","it","to","call","hadoop","daemon","sh","anymore","hadoop","daemons","sh","should","just","call","hdfs","directly"],"filtered":["little","reason","call","hadoop","daemon","sh","anymore","hadoop","daemons","sh","call","hdfs","directly"],"features":{"type":0,"size":1000,"indices":[36,146,181,188,211,268,281,307,346,388,391,495,498,665,831,846,850,967],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There is no unit test for KMeansSummary in spark ml  Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime   See PR for  SPARK","_c1":"Unit test for spark ml KMeansSummary","document":"There is no unit test for KMeansSummary in spark ml  Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime   See PR for  SPARK Unit test for spark ml KMeansSummary","words":["there","is","no","unit","test","for","kmeanssummary","in","spark","ml","","other","items","which","could","be","fixed","here","","","","add","since","version","to","kmeanssummary","class","","","modify","clustersizes","method","to","match","gmm","method","","to","be","robust","to","empty","clusters","","in","case","we","support","that","sometime","","","see","pr","for","","spark","unit","test","for","spark","ml","kmeanssummary"],"filtered":["unit","test","kmeanssummary","spark","ml","","items","fixed","","","","add","since","version","kmeanssummary","class","","","modify","clustersizes","method","match","gmm","method","","robust","empty","clusters","","case","support","sometime","","","see","pr","","spark","unit","test","spark","ml","kmeanssummary"],"features":{"type":0,"size":1000,"indices":[36,105,135,150,152,185,199,213,254,281,324,332,335,342,346,367,372,388,432,445,499,500,502,515,534,565,585,586,597,607,654,656,674,695,760,831,993,995],"values":[3.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,11.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"There is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well","_c1":"Add MessageHandler to KinesisUtils createStream similar to Direct Kafka","document":"There is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well Add MessageHandler to KinesisUtils createStream similar to Direct Kafka","words":["there","is","support","for","message","handler","in","direct","kafka","stream","","which","allows","arbitrary","t","to","be","the","output","of","the","stream","instead","of","array","byte","","","this","is","a","very","useful","function","","therefore","should","exist","in","kinesis","as","well","add","messagehandler","to","kinesisutils","createstream","similar","to","direct","kafka"],"filtered":["support","message","handler","direct","kafka","stream","","allows","arbitrary","output","stream","instead","array","byte","","","useful","function","","therefore","exist","kinesis","well","add","messagehandler","kinesisutils","createstream","similar","direct","kafka"],"features":{"type":0,"size":1000,"indices":[36,122,144,150,157,168,170,173,218,272,281,313,343,372,373,388,432,445,480,546,564,572,597,656,665,695,706,710,777,831,847,852,863,910,917,944,969,981],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,4.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"There s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson  test patch script has to verify if the version of FindBugs is correct","_c1":"test patch should verify that FindBugs version used for verification is correct one","document":"There s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson  test patch script has to verify if the version of FindBugs is correct test patch should verify that FindBugs version used for verification is correct one","words":["there","s","a","possibility","that","contributors","and","commiters","might","be","checking","their","patches","against","a","version","of","findbugs","which","differs","from","one","installed","on","hudson","","test","patch","script","has","to","verify","if","the","version","of","findbugs","is","correct","test","patch","should","verify","that","findbugs","version","used","for","verification","is","correct","one"],"filtered":["possibility","contributors","commiters","might","checking","patches","version","findbugs","differs","one","installed","hudson","","test","patch","script","verify","version","findbugs","correct","test","patch","verify","findbugs","version","used","verification","correct","one"],"features":{"type":0,"size":1000,"indices":[36,44,61,82,116,137,170,197,235,281,333,334,342,343,350,372,388,443,464,471,481,580,586,597,605,644,656,665,710,736,741,760,831,921,946,985,991,995],"values":[1.0,2.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0]},"cluster_label":13}
{"_c0":"These two classes should be public  since they are used in public code","_c1":"Make DataFrameHolder and DatasetHolder public","document":"These two classes should be public  since they are used in public code Make DataFrameHolder and DatasetHolder public","words":["these","two","classes","should","be","public","","since","they","are","used","in","public","code","make","dataframeholder","and","datasetholder","public"],"filtered":["two","classes","public","","since","used","public","code","make","dataframeholder","datasetholder","public"],"features":{"type":0,"size":1000,"indices":[4,48,138,333,372,408,420,445,461,498,525,585,605,656,665,718,809],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"These two methods were added to Scala Datasets  but are not available in Python yet","_c1":"Add withWatermark and checkpoint to python dataframe","document":"These two methods were added to Scala Datasets  but are not available in Python yet Add withWatermark and checkpoint to python dataframe","words":["these","two","methods","were","added","to","scala","datasets","","but","are","not","available","in","python","yet","add","withwatermark","and","checkpoint","to","python","dataframe"],"filtered":["two","methods","added","scala","datasets","","available","python","yet","add","withwatermark","checkpoint","python","dataframe"],"features":{"type":0,"size":1000,"indices":[18,83,129,138,161,325,333,348,371,372,384,388,404,408,432,445,461,490,589,907,962],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"They are not very useful  and cause problems with toString due to the order they are mixed in","_c1":"Remove LeafNode  UnaryNode  BinaryNode from TreeNode","document":"They are not very useful  and cause problems with toString due to the order they are mixed in Remove LeafNode  UnaryNode  BinaryNode from TreeNode","words":["they","are","not","very","useful","","and","cause","problems","with","tostring","due","to","the","order","they","are","mixed","in","remove","leafnode","","unarynode","","binarynode","from","treenode"],"filtered":["useful","","cause","problems","tostring","due","order","mixed","remove","leafnode","","unarynode","","binarynode","treenode"],"features":{"type":0,"size":1000,"indices":[18,48,138,142,272,288,299,333,372,388,410,445,634,650,710,718,785,856,870,911,921,942,944],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"They were kept in SQLContext implicits object for binary backward compatibility  in the Spark   x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits","_c1":"Move StringToColumn implicit class into SQLImplicits","document":"They were kept in SQLContext implicits object for binary backward compatibility  in the Spark   x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits Move StringToColumn implicit class into SQLImplicits","words":["they","were","kept","in","sqlcontext","implicits","object","for","binary","backward","compatibility","","in","the","spark","","","x","series","","it","makes","more","sense","for","this","api","to","be","in","sqlimplicits","since","that","s","the","single","class","that","defines","all","the","sql","implicits","move","stringtocolumn","implicit","class","into","sqlimplicits"],"filtered":["kept","sqlcontext","implicits","object","binary","backward","compatibility","","spark","","","x","series","","makes","sense","api","sqlimplicits","since","single","class","defines","sql","implicits","move","stringtocolumn","implicit","class","sqlimplicits"],"features":{"type":0,"size":1000,"indices":[5,36,48,63,105,113,197,282,350,372,373,388,445,451,494,495,531,534,567,571,577,585,629,644,647,650,656,686,691,710,760,810,834,891,908,962,968],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages  Issue brought up by   eronwright   Descriptions below copied from  http   apache spark developers list         n  nabble com Make ML Developer APIs public post     td      html   We plan to make these APIs public in Spark      However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component  Nice to have a consistent format by reusing the trait    ProbabilisticClassifier  Third party components should leverage the complex logic around computing only selected columns  We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL  Users can copy the methods into their own  as needed","_c1":"Make some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier","document":"This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages  Issue brought up by   eronwright   Descriptions below copied from  http   apache spark developers list         n  nabble com Make ML Developer APIs public post     td      html   We plan to make these APIs public in Spark      However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component  Nice to have a consistent format by reusing the trait    ProbabilisticClassifier  Third party components should leverage the complex logic around computing only selected columns  We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL  Users can copy the methods into their own  as needed Make some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier","words":["this","jira","is","for","making","several","ml","apis","public","to","make","it","easier","for","users","to","write","their","own","pipeline","stages","","issue","brought","up","by","","","eronwright","","","descriptions","below","copied","from","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","make","ml","developer","apis","public","post","","","","","td","","","","","","html","","","we","plan","to","make","these","apis","public","in","spark","","","","","","however","","they","will","be","marked","developerapi","and","are","","very","likely","","to","be","broken","in","the","future","","","","vectorudt","","to","define","a","relation","with","a","vector","field","","vectorudt","must","be","instantiated","","","","identifiable","trait","","the","trait","generates","a","unique","identifier","for","the","associated","pipeline","component","","nice","to","have","a","consistent","format","by","reusing","the","trait","","","","probabilisticclassifier","","third","party","components","should","leverage","the","complex","logic","around","computing","only","selected","columns","","we","will","not","yet","make","these","public","","","","schemautils","","third","party","pipeline","components","have","a","need","for","checking","column","types","and","appending","columns","","","","","this","will","probably","be","moved","into","spark","sql","","users","can","copy","the","methods","into","their","own","","as","needed","make","some","ml","apis","public","","vectorudt","","identifiable","","probabilisticclassifier"],"filtered":["jira","making","several","ml","apis","public","make","easier","users","write","pipeline","stages","","issue","brought","","","eronwright","","","descriptions","copied","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","make","ml","developer","apis","public","post","","","","","td","","","","","","html","","","plan","make","apis","public","spark","","","","","","however","","marked","developerapi","","likely","","broken","future","","","","vectorudt","","define","relation","vector","field","","vectorudt","must","instantiated","","","","identifiable","trait","","trait","generates","unique","identifier","associated","pipeline","component","","nice","consistent","format","reusing","trait","","","","probabilisticclassifier","","third","party","components","leverage","complex","logic","around","computing","selected","columns","","yet","make","public","","","","schemautils","","third","party","pipeline","components","need","checking","column","types","appending","columns","","","","","probably","moved","spark","sql","","users","copy","methods","","needed","make","ml","apis","public","","vectorudt","","identifiable","","probabilisticclassifier"],"features":{"type":0,"size":1000,"indices":[9,18,23,36,43,48,55,92,105,113,119,120,123,128,129,138,139,164,170,174,216,221,222,223,228,235,240,244,262,274,281,291,297,299,304,311,315,324,327,333,341,348,354,370,372,373,385,388,392,400,413,420,445,461,465,474,494,495,498,512,525,537,547,572,586,601,603,608,609,617,644,650,652,653,655,656,665,673,686,708,710,728,737,742,748,755,798,814,821,833,842,852,891,899,921,944,954,969,973,993,996,997],"values":[1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,64.0,3.0,1.0,6.0,1.0,1.0,1.0,3.0,5.0,2.0,2.0,1.0,1.0,2.0,5.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":7}
{"_c0":"This JIRA is to address  Jing s comments https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           in HADOOP","_c1":"AsyncCallHandler should use an event driven architecture to handle async calls","document":"This JIRA is to address  Jing s comments https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           in HADOOP AsyncCallHandler should use an event driven architecture to handle async calls","words":["this","jira","is","to","address","","jing","s","comments","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","","","","","","","","","","","in","hadoop","asynccallhandler","should","use","an","event","driven","architecture","to","handle","async","calls"],"filtered":["jira","address","","jing","comments","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","","","","","","","","","","","hadoop","asynccallhandler","use","event","driven","architecture","handle","async","calls"],"features":{"type":0,"size":1000,"indices":[54,56,76,134,154,181,197,221,237,281,294,370,371,372,373,388,445,475,489,495,535,542,577,599,639,662,665,752,821,827,862,866,980,996,998],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,28.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":3}
{"_c0":"This JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    Token init  authenticate and request an identity token  then persist the token in token cache for later reuse    Token display  show the existing token with its info and attributes in the token cache    Token revoke  revoke a token so that the token will no longer be valid and cannot be used later    Token renew  extend the lifecycle of a token before it s expired","_c1":"Hadoop Token Command","document":"This JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    Token init  authenticate and request an identity token  then persist the token in token cache for later reuse    Token display  show the existing token with its info and attributes in the token cache    Token revoke  revoke a token so that the token will no longer be valid and cannot be used later    Token renew  extend the lifecycle of a token before it s expired Hadoop Token Command","words":["this","jira","is","to","define","commands","for","hadoop","token","","the","scope","of","this","task","is","highlighted","as","following","","","","token","init","","authenticate","and","request","an","identity","token","","then","persist","the","token","in","token","cache","for","later","reuse","","","","token","display","","show","the","existing","token","with","its","info","and","attributes","in","the","token","cache","","","","token","revoke","","revoke","a","token","so","that","the","token","will","no","longer","be","valid","and","cannot","be","used","later","","","","token","renew","","extend","the","lifecycle","of","a","token","before","it","s","expired","hadoop","token","command"],"filtered":["jira","define","commands","hadoop","token","","scope","task","highlighted","following","","","","token","init","","authenticate","request","identity","token","","persist","token","token","cache","later","reuse","","","","token","display","","show","existing","token","info","attributes","token","cache","","","","token","revoke","","revoke","token","token","longer","valid","used","later","","","","token","renew","","extend","lifecycle","token","expired","hadoop","token","command"],"features":{"type":0,"size":1000,"indices":[19,36,39,91,92,124,135,159,163,170,174,181,183,197,245,281,296,333,343,346,368,371,372,373,381,388,420,445,449,451,452,466,467,475,482,495,507,528,572,600,605,650,656,674,710,752,756,760,821,834,868,893,931,975,980],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,18.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,14.0,1.0,1.0,1.0,1.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":12}
{"_c0":"This JIRA is to upgrade the derby version from           to           Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade  The upgrade is due to an already disclosed vulnerability  CVE            in derby            We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this  This was raised on the mailing list at http   apache spark developers list         n  nabble com VOTE Release Apache Spark       RC  tp     p      html by Stephen Hellberg and replied to by Sean Owen  I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the     branch  so ideally we d backport this for all impacted Spark releases  I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag","_c1":"Upgrade derby to           from","document":"This JIRA is to upgrade the derby version from           to           Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade  The upgrade is due to an already disclosed vulnerability  CVE            in derby            We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this  This was raised on the mailing list at http   apache spark developers list         n  nabble com VOTE Release Apache Spark       RC  tp     p      html by Stephen Hellberg and replied to by Sean Owen  I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the     branch  so ideally we d backport this for all impacted Spark releases  I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag Upgrade derby to           from","words":["this","jira","is","to","upgrade","the","derby","version","from","","","","","","","","","","","to","","","","","","","","","","","sean","and","i","figured","that","we","only","use","derby","for","tests","and","so","the","initial","pull","request","was","to","not","include","it","in","the","jars","folder","for","spark","","i","now","believe","it","is","required","based","on","comments","for","the","pull","request","and","so","this","is","only","a","dependency","upgrade","","the","upgrade","is","due","to","an","already","disclosed","vulnerability","","cve","","","","","","","","","","","","in","derby","","","","","","","","","","","","we","used","https","","","www","versioneye","com","search","and","will","be","checking","for","any","other","problems","in","a","variety","of","libraries","too","","investigating","if","we","can","set","up","a","jenkins","job","to","check","our","pom","on","a","regular","basis","so","we","can","stay","ahead","of","the","game","for","matters","like","this","","this","was","raised","on","the","mailing","list","at","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","vote","release","apache","spark","","","","","","","rc","","tp","","","","","p","","","","","","html","by","stephen","hellberg","and","replied","to","by","sean","owen","","i","ve","checked","the","impact","to","previous","spark","releases","and","this","particular","version","of","derby","is","the","only","relatively","recent","and","without","vulnerabilities","version","","i","checked","up","to","the","","","","","branch","","so","ideally","we","d","backport","this","for","all","impacted","spark","releases","","i","ve","marked","this","as","critical","and","ticked","the","important","checkbox","as","it","s","going","to","impact","every","user","","there","isn","t","a","security","component","","should","we","add","one","","","and","hence","the","build","tag","upgrade","derby","to","","","","","","","","","","","from"],"filtered":["jira","upgrade","derby","version","","","","","","","","","","","","","","","","","","","","","sean","figured","use","derby","tests","initial","pull","request","include","jars","folder","spark","","believe","required","based","comments","pull","request","dependency","upgrade","","upgrade","due","already","disclosed","vulnerability","","cve","","","","","","","","","","","","derby","","","","","","","","","","","","used","https","","","www","versioneye","com","search","checking","problems","variety","libraries","","investigating","set","jenkins","job","check","pom","regular","basis","stay","ahead","game","matters","like","","raised","mailing","list","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","vote","release","apache","spark","","","","","","","rc","","tp","","","","","p","","","","","","html","stephen","hellberg","replied","sean","owen","","ve","checked","impact","previous","spark","releases","particular","version","derby","relatively","recent","without","vulnerabilities","version","","checked","","","","","branch","","ideally","d","backport","impacted","spark","releases","","ve","marked","critical","ticked","important","checkbox","going","impact","every","user","","isn","security","component","","add","one","","","hence","build","tag","upgrade","derby","","","","","","","","","",""],"features":{"type":0,"size":1000,"indices":[7,18,26,32,36,44,57,76,82,89,91,92,94,98,105,110,113,115,120,128,129,146,157,170,194,197,214,221,223,234,242,246,252,258,276,281,315,329,330,333,343,348,355,360,368,371,372,373,388,390,401,420,432,433,445,453,456,470,489,493,495,511,512,529,536,537,566,572,575,577,588,597,602,605,606,607,609,619,621,625,634,644,652,655,656,665,669,674,704,710,712,728,752,756,760,777,789,793,805,813,821,831,833,842,852,863,870,879,882,884,899,911,918,921,927,936,956,968,975,983,986,987,993,995,998],"values":[1.0,1.0,1.0,1.0,6.0,1.0,1.0,5.0,3.0,2.0,1.0,3.0,1.0,1.0,5.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,2.0,1.0,1.0,5.0,1.0,5.0,1.0,9.0,3.0,1.0,2.0,1.0,4.0,1.0,98.0,7.0,10.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,12.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,3.0,1.0]},"cluster_label":11}
{"_c0":"This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs  Slow RPCs are RPCs that fall at   th percentile  This is useful to troubleshoot why certain services like name node freezes under heavy load","_c1":"RPC Metrics   Add the ability track and log slow RPCs","document":"This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs  Slow RPCs are RPCs that fall at   th percentile  This is useful to troubleshoot why certain services like name node freezes under heavy load RPC Metrics   Add the ability track and log slow RPCs","words":["this","jira","proposes","to","add","a","counter","called","rpcslowcalls","and","also","a","configuration","setting","that","allows","users","to","log","really","slow","rpcs","","slow","rpcs","are","rpcs","that","fall","at","","","th","percentile","","this","is","useful","to","troubleshoot","why","certain","services","like","name","node","freezes","under","heavy","load","rpc","metrics","","","add","the","ability","track","and","log","slow","rpcs"],"filtered":["jira","proposes","add","counter","called","rpcslowcalls","also","configuration","setting","allows","users","log","really","slow","rpcs","","slow","rpcs","rpcs","fall","","","th","percentile","","useful","troubleshoot","certain","services","like","name","node","freezes","heavy","load","rpc","metrics","","","add","ability","track","log","slow","rpcs"],"features":{"type":0,"size":1000,"indices":[1,15,19,39,106,138,170,181,203,208,258,272,281,310,330,333,353,356,362,372,373,388,398,432,480,493,510,525,546,558,631,659,691,710,729,753,755,756,760,780,792,821,924,973],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,6.0,2.0,3.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues","_c1":"Support MetricsSource interface for DecayRpcScheduler Metrics","document":"This allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues Support MetricsSource interface for DecayRpcScheduler Metrics","words":["this","allows","metrics","collector","such","as","ams","to","collect","it","with","metricssink","","the","per","user","rpc","call","counts","","schedule","decisions","and","per","priority","response","time","will","be","useful","to","detect","and","trouble","shoot","hadoop","rpc","server","such","as","namenode","overload","issues","support","metricssource","interface","for","decayrpcscheduler","metrics"],"filtered":["allows","metrics","collector","ams","collect","metricssink","","per","user","rpc","call","counts","","schedule","decisions","per","priority","response","time","useful","detect","trouble","shoot","hadoop","rpc","server","namenode","overload","issues","support","metricssource","interface","decayrpcscheduler","metrics"],"features":{"type":0,"size":1000,"indices":[0,36,106,110,146,157,181,193,207,272,315,333,372,373,388,411,420,423,425,440,445,475,480,495,534,556,557,572,594,650,656,675,695,710,882,919,935,943,971],"values":[1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This bug is reported by Stuti Awasthi  https   www mail archive com user spark apache org msg      html The lossSum has possibility of infinity because we do not standardize the feature before fitting model  we should support feature standardization  Another benefit is that standardization will improve the convergence rate","_c1":"AFTSurvivalRegression should support feature standardization","document":"This bug is reported by Stuti Awasthi  https   www mail archive com user spark apache org msg      html The lossSum has possibility of infinity because we do not standardize the feature before fitting model  we should support feature standardization  Another benefit is that standardization will improve the convergence rate AFTSurvivalRegression should support feature standardization","words":["this","bug","is","reported","by","stuti","awasthi","","https","","","www","mail","archive","com","user","spark","apache","org","msg","","","","","","html","the","losssum","has","possibility","of","infinity","because","we","do","not","standardize","the","feature","before","fitting","model","","we","should","support","feature","standardization","","another","benefit","is","that","standardization","will","improve","the","convergence","rate","aftsurvivalregression","should","support","feature","standardization"],"filtered":["bug","reported","stuti","awasthi","","https","","","www","mail","archive","com","user","spark","apache","org","msg","","","","","","html","losssum","possibility","infinity","standardize","feature","fitting","model","","support","feature","standardization","","another","benefit","standardization","improve","convergence","rate","aftsurvivalregression","support","feature","standardization"],"features":{"type":0,"size":1000,"indices":[18,60,105,159,186,202,221,223,249,253,281,343,368,372,373,380,397,420,421,450,495,522,534,535,541,580,652,665,686,695,699,710,736,760,779,793,828,857,882,940,946,965,993,995,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,10.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This depends on some internal interface of Spark SQL  should be done after merging into Spark","_c1":"DataFrame UDFs in R","document":"This depends on some internal interface of Spark SQL  should be done after merging into Spark DataFrame UDFs in R","words":["this","depends","on","some","internal","interface","of","spark","sql","","should","be","done","after","merging","into","spark","dataframe","udfs","in","r"],"filtered":["depends","internal","interface","spark","sql","","done","merging","spark","dataframe","udfs","r"],"features":{"type":0,"size":1000,"indices":[77,82,105,161,188,189,295,298,343,372,373,400,445,556,570,656,665,686,707,891],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"This is a debug only version of SPARK        for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark","_c1":"Add debug only socket source in Structured Streaming","document":"This is a debug only version of SPARK        for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark Add debug only socket source in Structured Streaming","words":["this","is","a","debug","only","version","of","spark","","","","","","","","for","tutorials","and","debugging","of","streaming","apps","","it","would","be","nice","to","have","a","text","based","socket","source","similar","to","the","one","in","spark","streaming","","it","will","clearly","be","marked","as","debug","only","so","that","users","don","t","try","to","run","it","in","production","applications","","because","this","type","of","source","cannot","provide","ha","without","storing","a","lot","of","state","in","spark","add","debug","only","socket","source","in","structured","streaming"],"filtered":["debug","version","spark","","","","","","","","tutorials","debugging","streaming","apps","","nice","text","based","socket","source","similar","one","spark","streaming","","clearly","marked","debug","users","try","run","production","applications","","type","source","provide","ha","without","storing","lot","state","spark","add","debug","socket","source","structured","streaming"],"features":{"type":0,"size":1000,"indices":[6,36,44,70,105,148,159,163,169,170,204,214,263,281,288,299,313,333,343,349,364,368,370,372,373,388,393,420,421,432,445,495,512,526,572,625,656,658,697,710,725,735,738,742,755,760,777,811,884,899,910,931,995],"values":[1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,10.0,2.0,3.0,2.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This is a follow up jira from HADOOP           Now with the findbug warning     As discussed in HADOOP        bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby       rc  release     Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier","_c1":"Follow on fixups after upgraded mini kdc using Kerby","document":"This is a follow up jira from HADOOP           Now with the findbug warning     As discussed in HADOOP        bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby       rc  release     Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier Follow on fixups after upgraded mini kdc using Kerby","words":["this","is","a","follow","up","jira","from","hadoop","","","","","","","","","","","now","with","the","findbug","warning","","","","","as","discussed","in","hadoop","","","","","","","","bq","","why","was","this","committed","with","a","findbugs","errors","rather","than","adding","the","necessary","plumbing","in","pom","xml","to","make","it","go","away","","we","will","add","the","findbugsexcludefile","xml","and","will","get","rid","of","this","given","kerby","","","","","","","rc","","release","","","","","add","the","kerby","version","hadoop","project","pom","xml","bq","","hadoop","project","pom","xml","contains","the","dependencies","of","all","libraries","used","in","all","modules","of","hadoop","","under","dependencymanagement","","only","here","version","will","be","mentioned","","all","other","hadoop","modules","will","inherit","hadoop","project","","so","all","submodules","will","use","the","same","version","","in","submodule","","version","need","not","be","mentioned","in","pom","xml","","this","will","make","version","management","easier","follow","on","fixups","after","upgraded","mini","kdc","using","kerby"],"filtered":["follow","jira","hadoop","","","","","","","","","","","findbug","warning","","","","","discussed","hadoop","","","","","","","","bq","","committed","findbugs","errors","rather","adding","necessary","plumbing","pom","xml","make","go","away","","add","findbugsexcludefile","xml","get","rid","given","kerby","","","","","","","rc","","release","","","","","add","kerby","version","hadoop","project","pom","xml","bq","","hadoop","project","pom","xml","contains","dependencies","libraries","used","modules","hadoop","","dependencymanagement","","version","mentioned","","hadoop","modules","inherit","hadoop","project","","submodules","use","version","","submodule","","version","need","mentioned","pom","xml","","make","version","management","easier","follow","fixups","upgraded","mini","kdc","using","kerby"],"features":{"type":0,"size":1000,"indices":[17,18,39,48,54,60,77,82,92,98,128,135,158,170,177,181,194,196,200,216,231,234,252,261,281,333,343,360,368,371,372,373,388,420,432,437,445,446,474,489,495,525,537,546,547,562,566,572,605,624,650,656,657,671,674,697,710,726,728,737,751,774,781,809,821,828,899,921,943,959,968,973,991,993,995],"values":[1.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,4.0,1.0,1.0,42.0,4.0,1.0,6.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,3.0,1.0,1.0,6.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,5.0]},"cluster_label":16}
{"_c0":"This is an umbrella ticket to list issues I found with APIs for the     release","_c1":"Spark     SQL API audit","document":"This is an umbrella ticket to list issues I found with APIs for the     release Spark     SQL API audit","words":["this","is","an","umbrella","ticket","to","list","issues","i","found","with","apis","for","the","","","","","release","spark","","","","","sql","api","audit"],"filtered":["umbrella","ticket","list","issues","found","apis","","","","","release","spark","","","","","sql","api","audit"],"features":{"type":0,"size":1000,"indices":[36,105,255,281,329,371,372,373,388,443,475,533,644,650,686,710,728,752,842,955],"values":[1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This is an umbrella ticket to reduce the difference between sql core and sql hive  Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs","_c1":"Merge functionality in Hive module into SQL core module","document":"This is an umbrella ticket to reduce the difference between sql core and sql hive  Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs Merge functionality in Hive module into SQL core module","words":["this","is","an","umbrella","ticket","to","reduce","the","difference","between","sql","core","and","sql","hive","","ultimately","the","only","difference","should","just","be","the","ability","to","run","hive","serdes","as","well","as","udfs","merge","functionality","in","hive","module","into","sql","core","module"],"filtered":["umbrella","ticket","reduce","difference","sql","core","sql","hive","","ultimately","difference","ability","run","hive","serdes","well","udfs","merge","functionality","hive","module","sql","core","module"],"features":{"type":0,"size":1000,"indices":[157,189,228,255,281,299,307,333,364,365,372,373,388,439,443,445,481,501,502,572,599,656,665,686,710,752,753,773,891,899,983],"values":[1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"This is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog","_c1":"Remove Direct Usage of HiveClient in InsertIntoHiveTable","document":"This is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog Remove Direct Usage of HiveClient in InsertIntoHiveTable","words":["this","is","another","step","to","get","rid","of","hiveclient","from","","hivesessionstate","","","all","the","metastore","interactions","should","be","through","","externalcatalog","","interface","","however","","the","existing","implementation","of","","insertintohivetable","","","still","requires","hive","clients","","thus","","we","can","remove","hiveclient","by","moving","the","metastore","interactions","into","","externalcatalog","remove","direct","usage","of","hiveclient","in","insertintohivetable"],"filtered":["another","step","get","rid","hiveclient","","hivesessionstate","","","metastore","interactions","","externalcatalog","","interface","","however","","existing","implementation","","insertintohivetable","","","still","requires","hive","clients","","thus","","remove","hiveclient","moving","metastore","interactions","","externalcatalog","remove","direct","usage","hiveclient","insertintohivetable"],"features":{"type":0,"size":1000,"indices":[17,154,219,223,281,288,343,370,371,372,373,388,445,477,513,543,546,556,558,599,643,656,665,673,684,698,710,724,766,779,800,833,852,891,921,938,959,968,993],"values":[1.0,1.0,3.0,1.0,1.0,2.0,3.0,2.0,1.0,13.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This is for API parity of Scala API  Refer to https   issues apache org jira browse SPARK","_c1":"Add varargs type dropDuplicates   function in SparkR","document":"This is for API parity of Scala API  Refer to https   issues apache org jira browse SPARK Add varargs type dropDuplicates   function in SparkR","words":["this","is","for","api","parity","of","scala","api","","refer","to","https","","","issues","apache","org","jira","browse","spark","add","varargs","type","dropduplicates","","","function","in","sparkr"],"filtered":["api","parity","scala","api","","refer","https","","","issues","apache","org","jira","browse","spark","add","varargs","type","dropduplicates","","","function","sparkr"],"features":{"type":0,"size":1000,"indices":[36,86,105,154,281,313,343,372,373,388,420,432,445,475,490,492,495,511,526,535,644,767,821,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This is just a clean up task  Today there are all these fields in SQLContext that are not organized in any particular way  However  since each SQLContext is a session  many of these fields are actually isolated per session  To minimize the size of these context files and provide a logical grouping that makes more sense  I propose that we move these fields into its own class  called SessionState","_c1":"Refactor  Move SQLContext HiveContext per session state to separate class","document":"This is just a clean up task  Today there are all these fields in SQLContext that are not organized in any particular way  However  since each SQLContext is a session  many of these fields are actually isolated per session  To minimize the size of these context files and provide a logical grouping that makes more sense  I propose that we move these fields into its own class  called SessionState Refactor  Move SQLContext HiveContext per session state to separate class","words":["this","is","just","a","clean","up","task","","today","there","are","all","these","fields","in","sqlcontext","that","are","not","organized","in","any","particular","way","","however","","since","each","sqlcontext","is","a","session","","many","of","these","fields","are","actually","isolated","per","session","","to","minimize","the","size","of","these","context","files","and","provide","a","logical","grouping","that","makes","more","sense","","i","propose","that","we","move","these","fields","into","its","own","class","","called","sessionstate","refactor","","move","sqlcontext","hivecontext","per","session","state","to","separate","class"],"filtered":["clean","task","","today","fields","sqlcontext","organized","particular","way","","however","","since","sqlcontext","session","","many","fields","actually","isolated","per","session","","minimize","size","context","files","provide","logical","grouping","makes","sense","","propose","move","fields","class","","called","sessionstate","refactor","","move","sqlcontext","hivecontext","per","session","state","separate","class"],"features":{"type":0,"size":1000,"indices":[5,18,91,128,138,159,170,186,188,192,228,247,281,282,288,296,307,329,333,340,343,349,372,373,382,388,398,405,440,445,447,451,453,461,462,532,534,551,585,605,621,623,629,667,673,691,693,710,718,760,825,826,831,885,891,968,993],"values":[1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,8.0,1.0,1.0,2.0,1.0,3.0,2.0,2.0,1.0,4.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This is necessary for s3 reads and writes to work correctly with some hadoop versions.","_c1":"Add jets3t dependency to Spark Build","document":"This is necessary for s3 reads and writes to work correctly with some hadoop versions. Add jets3t dependency to Spark Build","words":["this","is","necessary","for","s3","reads","and","writes","to","work","correctly","with","some","hadoop","versions.","add","jets3t","dependency","to","spark","build"],"filtered":["necessary","s3","reads","writes","work","correctly","hadoop","versions.","add","jets3t","dependency","spark","build"],"features":{"type":0,"size":1000,"indices":[36,105,181,281,333,341,373,388,400,432,527,536,588,619,650,697,827,924,945,974],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"This is similar with https   issues apache org jira browse SPARK        Currently    JdbcUtils savePartition   is doing type based dispatch for each row to write appropriate values  So  appropriate writers can be created first according to the schema  and then apply them to each row  This approach is similar with   CatalystWriteSupport","_c1":"Avoid per record type dispatch in JDBC when writing","document":"This is similar with https   issues apache org jira browse SPARK        Currently    JdbcUtils savePartition   is doing type based dispatch for each row to write appropriate values  So  appropriate writers can be created first according to the schema  and then apply them to each row  This approach is similar with   CatalystWriteSupport Avoid per record type dispatch in JDBC when writing","words":["this","is","similar","with","https","","","issues","apache","org","jira","browse","spark","","","","","","","","currently","","","","jdbcutils","savepartition","","","is","doing","type","based","dispatch","for","each","row","to","write","appropriate","values","","so","","appropriate","writers","can","be","created","first","according","to","the","schema","","and","then","apply","them","to","each","row","","this","approach","is","similar","with","","","catalystwritesupport","avoid","per","record","type","dispatch","in","jdbc","when","writing"],"filtered":["similar","https","","","issues","apache","org","jira","browse","spark","","","","","","","","currently","","","","jdbcutils","savepartition","","","type","based","dispatch","row","write","appropriate","values","","","appropriate","writers","created","first","according","schema","","apply","row","","approach","similar","","","catalystwritesupport","avoid","per","record","type","dispatch","jdbc","writing"],"features":{"type":0,"size":1000,"indices":[35,36,76,86,95,100,105,109,113,122,154,183,262,281,286,333,368,372,373,381,388,440,441,445,475,495,502,517,526,535,538,625,646,650,656,710,763,787,788,790,821,833,849,885,905,910,924,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,20.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":17}
{"_c0":"This is the first cut implementation of  trackStateByKey   new improvement state management method in Spark Streaming  See the epic jira for more details   https   issues apache org jira browse SPARK","_c1":"Implement trackStateByKey for improved state management","document":"This is the first cut implementation of  trackStateByKey   new improvement state management method in Spark Streaming  See the epic jira for more details   https   issues apache org jira browse SPARK Implement trackStateByKey for improved state management","words":["this","is","the","first","cut","implementation","of","","trackstatebykey","","","new","improvement","state","management","method","in","spark","streaming","","see","the","epic","jira","for","more","details","","","https","","","issues","apache","org","jira","browse","spark","implement","trackstatebykey","for","improved","state","management"],"filtered":["first","cut","implementation","","trackstatebykey","","","new","improvement","state","management","method","spark","streaming","","see","epic","jira","details","","","https","","","issues","apache","org","jira","browse","spark","implement","trackstatebykey","improved","state","management"],"features":{"type":0,"size":1000,"indices":[25,36,105,128,154,183,263,281,343,349,372,373,415,424,445,472,475,495,515,535,582,629,654,698,710,761,809,821,994,998],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,8.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d   t rWe  x tq e AOfrsM qb  m BRuv fel i PqR  edit usp sharing                    Old description                           Structured streaming doesn t have support for kafka yet  I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka        https   cwiki apache org confluence display KAFKA KIP      Add a time based log index","_c1":"Structured streaming support for consuming from Kafka","document":"This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d   t rWe  x tq e AOfrsM qb  m BRuv fel i PqR  edit usp sharing                    Old description                           Structured streaming doesn t have support for kafka yet  I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka        https   cwiki apache org confluence display KAFKA KIP      Add a time based log index Structured streaming support for consuming from Kafka","words":["this","is","the","parent","jira","to","track","all","the","work","for","the","building","a","kafka","source","for","structured","streaming","","here","is","the","design","doc","for","an","initial","version","of","the","kafka","source","","https","","","docs","google","com","document","d","","","t","rwe","","x","tq","e","aofrsm","qb","","m","bruv","fel","i","pqr","","edit","usp","sharing","","","","","","","","","","","","","","","","","","","","old","description","","","","","","","","","","","","","","","","","","","","","","","","","","","structured","streaming","doesn","t","have","support","for","kafka","yet","","i","personally","feel","like","time","based","indexing","would","make","for","a","much","better","interface","","but","it","s","been","pushed","back","to","kafka","","","","","","","","https","","","cwiki","apache","org","confluence","display","kafka","kip","","","","","","add","a","time","based","log","index","structured","streaming","support","for","consuming","from","kafka"],"filtered":["parent","jira","track","work","building","kafka","source","structured","streaming","","design","doc","initial","version","kafka","source","","https","","","docs","google","com","document","d","","","rwe","","x","tq","e","aofrsm","qb","","m","bruv","fel","pqr","","edit","usp","sharing","","","","","","","","","","","","","","","","","","","","old","description","","","","","","","","","","","","","","","","","","","","","","","","","","","structured","streaming","doesn","support","kafka","yet","","personally","feel","like","time","based","indexing","make","much","better","interface","","pushed","back","kafka","","","","","","","","https","","","cwiki","apache","org","confluence","display","kafka","kip","","","","","","add","time","based","log","index","structured","streaming","support","consuming","kafka"],"features":{"type":0,"size":1000,"indices":[6,36,53,66,70,83,94,108,113,115,122,135,137,157,163,170,189,197,219,221,253,263,281,299,307,313,329,330,343,348,372,373,388,430,432,436,439,441,449,484,493,495,500,513,524,525,527,535,545,556,596,616,625,631,638,666,672,673,679,681,695,710,752,761,777,810,820,821,847,852,878,921,936,941,968,995,998],"values":[1.0,6.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,70.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":11}
{"_c0":"This issue adds  read orc write orc  to SparkR for API parity","_c1":"Add  read orc write orc  to SparkR","document":"This issue adds  read orc write orc  to SparkR for API parity Add  read orc write orc  to SparkR","words":["this","issue","adds","","read","orc","write","orc","","to","sparkr","for","api","parity","add","","read","orc","write","orc","","to","sparkr"],"filtered":["issue","adds","","read","orc","write","orc","","sparkr","api","parity","add","","read","orc","write","orc","","sparkr"],"features":{"type":0,"size":1000,"indices":[36,113,372,373,388,420,432,644,650,748,749,767,985],"values":[1.0,2.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,2.0,1.0]},"cluster_label":2}
{"_c0":"This maven repository is blocked in China  We should get rid of that dependency so people in China can compile Spark","_c1":"Remove dependency on Twitter J repository","document":"This maven repository is blocked in China  We should get rid of that dependency so people in China can compile Spark Remove dependency on Twitter J repository","words":["this","maven","repository","is","blocked","in","china","","we","should","get","rid","of","that","dependency","so","people","in","china","can","compile","spark","remove","dependency","on","twitter","j","repository"],"filtered":["maven","repository","blocked","china","","get","rid","dependency","people","china","compile","spark","remove","dependency","twitter","j","repository"],"features":{"type":0,"size":1000,"indices":[17,82,105,149,281,288,343,368,369,372,373,445,463,486,562,588,591,622,665,760,796,833,959,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"This method survived the code review and it has been there since v       It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly","_c1":"Hide ALS solveLeastSquares","document":"This method survived the code review and it has been there since v       It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly Hide ALS solveLeastSquares","words":["this","method","survived","the","code","review","and","it","has","been","there","since","v","","","","","","","it","exposes","jblas","types","","let","s","remove","it","from","the","public","api","","i","expect","that","no","one","calls","it","directly","hide","als","solveleastsquares"],"filtered":["method","survived","code","review","since","v","","","","","","","exposes","jblas","types","","let","remove","public","api","","expect","one","calls","directly","hide","als","solveleastsquares"],"features":{"type":0,"size":1000,"indices":[44,164,188,197,288,292,329,333,344,346,372,373,420,465,477,495,498,535,580,585,605,644,654,667,710,760,831,843,846,866,921,978],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"This patch adds to fs shell Stat java the missing options of  a and  A  FileStatus already contains the getPermission   method required for returning symbolic permissions  FsPermission contains the method to return the binary short  but nothing to present in standard Octal format  Most UNIX admins base their work on such standard octal permissions  Hence  this patch also introduces one tiny method to translate the toShort   return into octal  Build has already passed unit tests and javadoc","_c1":"Add  A and  a formats for fs  stat command to print permissions","document":"This patch adds to fs shell Stat java the missing options of  a and  A  FileStatus already contains the getPermission   method required for returning symbolic permissions  FsPermission contains the method to return the binary short  but nothing to present in standard Octal format  Most UNIX admins base their work on such standard octal permissions  Hence  this patch also introduces one tiny method to translate the toShort   return into octal  Build has already passed unit tests and javadoc Add  A and  a formats for fs  stat command to print permissions","words":["this","patch","adds","to","fs","shell","stat","java","the","missing","options","of","","a","and","","a","","filestatus","already","contains","the","getpermission","","","method","required","for","returning","symbolic","permissions","","fspermission","contains","the","method","to","return","the","binary","short","","but","nothing","to","present","in","standard","octal","format","","most","unix","admins","base","their","work","on","such","standard","octal","permissions","","hence","","this","patch","also","introduces","one","tiny","method","to","translate","the","toshort","","","return","into","octal","","build","has","already","passed","unit","tests","and","javadoc","add","","a","and","","a","formats","for","fs","","stat","command","to","print","permissions"],"filtered":["patch","adds","fs","shell","stat","java","missing","options","","","","filestatus","already","contains","getpermission","","","method","required","returning","symbolic","permissions","","fspermission","contains","method","return","binary","short","","nothing","present","standard","octal","format","","unix","admins","base","work","standard","octal","permissions","","hence","","patch","also","introduces","one","tiny","method","translate","toshort","","","return","octal","","build","already","passed","unit","tests","javadoc","add","","","formats","fs","","stat","command","print","permissions"],"features":{"type":0,"size":1000,"indices":[7,32,36,44,48,57,82,83,84,118,122,123,135,137,158,170,173,222,235,272,306,333,335,343,352,372,373,388,407,432,445,525,527,536,559,567,580,619,651,652,653,654,680,710,714,716,735,770,792,805,809,886,891,913,945,960,964,967,985,995],"values":[1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,16.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,5.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":17}
{"_c0":"This will make it convenient for R users to use SparkR from their browsers","_c1":"Install and configure RStudio server on Spark EC","document":"This will make it convenient for R users to use SparkR from their browsers Install and configure RStudio server on Spark EC","words":["this","will","make","it","convenient","for","r","users","to","use","sparkr","from","their","browsers","install","and","configure","rstudio","server","on","spark","ec"],"filtered":["make","convenient","r","users","use","sparkr","browsers","install","configure","rstudio","server","spark","ec"],"features":{"type":0,"size":1000,"indices":[36,82,105,193,235,333,373,388,411,420,463,489,495,525,570,614,672,755,757,767,921],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"To track user level connections  How many connections for each user  in busy cluster where so many connections to server","_c1":"Expose  NumOpenConnectionsPerUser  as a metric","document":"To track user level connections  How many connections for each user  in busy cluster where so many connections to server Expose  NumOpenConnectionsPerUser  as a metric","words":["to","track","user","level","connections","","how","many","connections","for","each","user","","in","busy","cluster","where","so","many","connections","to","server","expose","","numopenconnectionsperuser","","as","a","metric"],"filtered":["track","user","level","connections","","many","connections","user","","busy","cluster","many","connections","server","expose","","numopenconnectionsperuser","","metric"],"features":{"type":0,"size":1000,"indices":[36,71,109,139,170,188,236,237,368,372,377,388,411,445,493,522,527,572,882,885,905],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0]},"cluster_label":2}
{"_c0":"Umbrella for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details","_c1":"Umbrella  Dynamic subcommands for hadoop shell scripts","document":"Umbrella for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details Umbrella  Dynamic subcommands for hadoop shell scripts","words":["umbrella","for","converting","hadoop","","hdfs","","mapred","","and","yarn","to","allow","for","dynamic","subcommands","","see","first","comment","for","more","details","umbrella","","dynamic","subcommands","for","hadoop","shell","scripts"],"filtered":["umbrella","converting","hadoop","","hdfs","","mapred","","yarn","allow","dynamic","subcommands","","see","first","comment","details","umbrella","","dynamic","subcommands","hadoop","shell","scripts"],"features":{"type":0,"size":1000,"indices":[36,123,128,149,181,183,231,255,294,333,372,388,515,564,573,629,967,970,988,999],"values":[4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Update LogisticCostAggregator serialization code to make it consistent with LinearRegression","_c1":"Update LogisticCostAggregator serialization code to make it consistent with LinearRegression","document":"Update LogisticCostAggregator serialization code to make it consistent with LinearRegression Update LogisticCostAggregator serialization code to make it consistent with LinearRegression","words":["update","logisticcostaggregator","serialization","code","to","make","it","consistent","with","linearregression","update","logisticcostaggregator","serialization","code","to","make","it","consistent","with","linearregression"],"filtered":["update","logisticcostaggregator","serialization","code","make","consistent","linearregression","update","logisticcostaggregator","serialization","code","make","consistent","linearregression"],"features":{"type":0,"size":1000,"indices":[343,388,420,495,525,650,843,927,954,994],"values":[2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients  We are currently using version       of the SDK  Version       brings some breaking changes  Need to fix code to resolve all these breaking changes and certify that everything works properly","_c1":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients","document":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients  We are currently using version       of the SDK  Version       brings some breaking changes  Need to fix code to resolve all these breaking changes and certify that everything works properly Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients","words":["update","wasb","driver","to","use","the","latest","version","","","","","","","","","of","sdk","for","microsoft","azure","storage","clients","","we","are","currently","using","version","","","","","","","of","the","sdk","","version","","","","","","","brings","some","breaking","changes","","need","to","fix","code","to","resolve","all","these","breaking","changes","and","certify","that","everything","works","properly","update","wasb","driver","to","use","the","latest","version","","","","","","","","","of","sdk","for","microsoft","azure","storage","clients"],"filtered":["update","wasb","driver","use","latest","version","","","","","","","","","sdk","microsoft","azure","storage","clients","","currently","using","version","","","","","","","sdk","","version","","","","","","","brings","breaking","changes","","need","fix","code","resolve","breaking","changes","certify","everything","works","properly","update","wasb","driver","use","latest","version","","","","","","","","","sdk","microsoft","azure","storage","clients"],"features":{"type":0,"size":1000,"indices":[36,74,135,138,225,333,343,344,363,372,388,394,400,409,420,445,461,489,491,498,510,537,624,644,710,724,734,760,763,810,827,920,968,993,995],"values":[2.0,1.0,2.0,1.0,1.0,1.0,5.0,2.0,2.0,31.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0]},"cluster_label":3}
{"_c0":"Update the tachyon client dependency from       to        There are no new dependencies added or Spark facing APIs changed","_c1":"Upgrade Tachyon dependency to","document":"Update the tachyon client dependency from       to        There are no new dependencies added or Spark facing APIs changed Upgrade Tachyon dependency to","words":["update","the","tachyon","client","dependency","from","","","","","","","to","","","","","","","","there","are","no","new","dependencies","added","or","spark","facing","apis","changed","upgrade","tachyon","dependency","to"],"filtered":["update","tachyon","client","dependency","","","","","","","","","","","","","","new","dependencies","added","spark","facing","apis","changed","upgrade","tachyon","dependency"],"features":{"type":0,"size":1000,"indices":[25,105,135,138,187,196,242,343,346,372,384,388,392,416,586,588,710,831,842,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,13.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases","_c1":"Use sqlContext from MLlibTestSparkContext for spark ml test suites","document":"Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases Use sqlContext from MLlibTestSparkContext for spark ml test suites","words":["use","sqlcontext","from","mllibtestsparkcontext","rather","than","creating","new","one","for","spark","ml","test","cases","use","sqlcontext","from","mllibtestsparkcontext","for","spark","ml","test","suites"],"filtered":["use","sqlcontext","mllibtestsparkcontext","rather","creating","new","one","spark","ml","test","cases","use","sqlcontext","mllibtestsparkcontext","spark","ml","test","suites"],"features":{"type":0,"size":1000,"indices":[25,36,44,105,261,324,437,451,489,576,586,641,767,921,931],"values":[1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Use the latest Sparksession to replace the existing SQLContext in MLlib","_c1":"Replace SQLContext with SparkSession in MLlib","document":"Use the latest Sparksession to replace the existing SQLContext in MLlib Replace SQLContext with SparkSession in MLlib","words":["use","the","latest","sparksession","to","replace","the","existing","sqlcontext","in","mllib","replace","sqlcontext","with","sparksession","in","mllib"],"filtered":["use","latest","sparksession","replace","existing","sqlcontext","mllib","replace","sqlcontext","sparksession","mllib"],"features":{"type":0,"size":1000,"indices":[371,388,445,451,489,498,521,650,710,787,942],"values":[1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"We already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day","_c1":"Track current database in SQL HiveContext","document":"We already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day Track current database in SQL HiveContext","words":["we","already","have","internal","apis","for","hive","to","do","this","","we","should","do","it","for","sqlcontext","too","so","we","can","merge","these","code","paths","one","day","track","current","database","in","sql","hivecontext"],"filtered":["already","internal","apis","hive","","sqlcontext","merge","code","paths","one","day","track","current","database","sql","hivecontext"],"features":{"type":0,"size":1000,"indices":[36,44,48,57,295,299,368,372,373,388,420,445,451,453,461,493,495,534,599,605,644,665,686,710,773,833,842,858,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":13}
{"_c0":"We currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark      we want to provide native implementations for DDLs for both SQLContext and HiveContext  The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step   Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API","_c1":"Create native DDL commands","document":"We currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark      we want to provide native implementations for DDLs for both SQLContext and HiveContext  The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step   Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API Create native DDL commands","words":["we","currently","delegate","most","ddls","directly","to","hive","","through","nativeplaceholder","in","hiveql","scala","","in","spark","","","","","","we","want","to","provide","native","implementations","for","ddls","for","both","sqlcontext","and","hivecontext","","the","first","step","is","to","properly","parse","these","ddls","","and","then","create","logical","commands","that","encapsulate","them","","the","actual","implementation","can","still","delegate","to","hivenativecommand","","as","an","example","","we","should","define","a","command","for","renametable","with","the","proper","fields","","and","just","delegate","the","implementation","to","hivenativecommand","","we","might","need","to","track","the","original","sql","query","in","order","to","run","hivenativecommand","","but","we","can","remove","the","sql","query","in","the","future","once","we","do","the","next","step","","","once","we","flush","out","the","internal","persistent","catalog","api","","we","can","then","switch","the","implementation","of","these","newly","added","commands","to","use","the","catalog","api","create","native","ddl","commands"],"filtered":["currently","delegate","ddls","directly","hive","","nativeplaceholder","hiveql","scala","","spark","","","","","","want","provide","native","implementations","ddls","sqlcontext","hivecontext","","first","step","properly","parse","ddls","","create","logical","commands","encapsulate","","actual","implementation","still","delegate","hivenativecommand","","example","","define","command","renametable","proper","fields","","delegate","implementation","hivenativecommand","","might","need","track","original","sql","query","order","run","hivenativecommand","","remove","sql","query","future","next","step","","","flush","internal","persistent","catalog","api","","switch","implementation","newly","added","commands","use","catalog","api","create","native","ddl","commands"],"features":{"type":0,"size":1000,"indices":[36,55,72,78,83,100,105,135,147,170,174,183,187,188,227,242,243,247,265,273,281,288,295,307,313,333,343,364,372,381,384,388,389,441,445,451,453,461,467,489,490,493,495,510,534,537,543,546,558,572,599,640,644,650,654,665,667,686,688,698,710,712,718,749,752,760,763,770,800,817,833,863,909,924,925,953,985,993],"values":[3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,18.0,2.0,1.0,8.0,1.0,1.0,4.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,11.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0]},"cluster_label":0}
{"_c0":"We deprecated  runs  in Spark      SPARK         In      we can either remove  runs  or make it no effect  with warning messages   So we can simplify the implementation  I prefer the latter for better binary compatibility","_c1":"Make  runs  no effect in k means","document":"We deprecated  runs  in Spark      SPARK         In      we can either remove  runs  or make it no effect  with warning messages   So we can simplify the implementation  I prefer the latter for better binary compatibility Make  runs  no effect in k means","words":["we","deprecated","","runs","","in","spark","","","","","","spark","","","","","","","","","in","","","","","","we","can","either","remove","","runs","","or","make","it","no","effect","","with","warning","messages","","","so","we","can","simplify","the","implementation","","i","prefer","the","latter","for","better","binary","compatibility","make","","runs","","no","effect","in","k","means"],"filtered":["deprecated","","runs","","spark","","","","","","spark","","","","","","","","","","","","","","either","remove","","runs","","make","effect","","warning","messages","","","simplify","implementation","","prefer","latter","better","binary","compatibility","make","","runs","","effect","k","means"],"features":{"type":0,"size":1000,"indices":[19,36,96,105,187,257,288,329,346,368,372,394,445,456,474,495,525,546,567,572,620,650,698,710,760,793,833,834,941,993],"values":[1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,28.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0]},"cluster_label":3}
{"_c0":"We don t sufficiently test the  path work well","_c1":"Remove the option to turn off unsafe and codegen","document":"We don t sufficiently test the  path work well Remove the option to turn off unsafe and codegen","words":["we","don","t","sufficiently","test","the","","path","work","well","remove","the","option","to","turn","off","unsafe","and","codegen"],"filtered":["sufficiently","test","","path","work","well","remove","option","turn","unsafe","codegen"],"features":{"type":0,"size":1000,"indices":[157,204,222,242,263,288,333,372,388,441,497,527,586,668,710,777,886,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We generate source code with line numbers for inclusion in the javadoc JARs  Given that there s github and other online viewers  this doesn t seem so useful these days  Disabling the  linkSource  option saves us   MB for the hadoop common javadoc jar","_c1":"Stop bundling HTML source code in javadoc JARs","document":"We generate source code with line numbers for inclusion in the javadoc JARs  Given that there s github and other online viewers  this doesn t seem so useful these days  Disabling the  linkSource  option saves us   MB for the hadoop common javadoc jar Stop bundling HTML source code in javadoc JARs","words":["we","generate","source","code","with","line","numbers","for","inclusion","in","the","javadoc","jars","","given","that","there","s","github","and","other","online","viewers","","this","doesn","t","seem","so","useful","these","days","","disabling","the","","linksource","","option","saves","us","","","mb","for","the","hadoop","common","javadoc","jar","stop","bundling","html","source","code","in","javadoc","jars"],"filtered":["generate","source","code","line","numbers","inclusion","javadoc","jars","","given","github","online","viewers","","doesn","seem","useful","days","","disabling","","linksource","","option","saves","us","","","mb","hadoop","common","javadoc","jar","stop","bundling","html","source","code","javadoc","jars"],"features":{"type":0,"size":1000,"indices":[0,36,44,70,110,117,181,182,197,208,222,252,272,274,333,368,372,373,419,420,445,461,500,510,588,603,650,652,667,674,710,760,769,770,777,796,830,831,858,888,954,993,995],"values":[1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":9}
{"_c0":"We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process  Those name and what they are used for is very similar so I think we can merge them  Also  the original unescapeSQLString method may have a fault  When   u      style character literals are passed to the method  it s not unescaped successfully","_c1":"Merge ParserUtils and ParseUtils","document":"We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process  Those name and what they are used for is very similar so I think we can merge them  Also  the original unescapeSQLString method may have a fault  When   u      style character literals are passed to the method  it s not unescaped successfully Merge ParserUtils and ParseUtils","words":["we","have","parserutils","and","parseutils","which","are","both","utility","collections","for","use","during","the","parsing","process","","those","name","and","what","they","are","used","for","is","very","similar","so","i","think","we","can","merge","them","","also","","the","original","unescapesqlstring","method","may","have","a","fault","","when","","","u","","","","","","style","character","literals","are","passed","to","the","method","","it","s","not","unescaped","successfully","merge","parserutils","and","parseutils"],"filtered":["parserutils","parseutils","utility","collections","use","parsing","process","","name","used","similar","think","merge","","also","","original","unescapesqlstring","method","may","fault","","","","u","","","","","","style","character","literals","passed","method","","unescaped","successfully","merge","parserutils","parseutils"],"features":{"type":0,"size":1000,"indices":[15,18,22,36,48,76,78,122,138,170,197,203,218,277,281,299,329,333,356,368,372,388,406,489,495,524,526,534,564,597,600,605,649,654,666,710,732,773,792,833,863,887,910,924,941,944,957,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,12.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"We introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these  We still plan to implement a full local mode  but it s probably going to be fairly different from what the current iterator based local mode would look like  Let s just remove them for now  and we can always re introduced them in the future by looking at branch","_c1":"Remove org apache spark sql execution local","document":"We introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these  We still plan to implement a full local mode  but it s probably going to be fairly different from what the current iterator based local mode would look like  Let s just remove them for now  and we can always re introduced them in the future by looking at branch Remove org apache spark sql execution local","words":["we","introduced","some","local","operators","in","org","apache","spark","sql","execution","local","package","but","never","fully","wired","the","engine","to","actually","use","these","","we","still","plan","to","implement","a","full","local","mode","","but","it","s","probably","going","to","be","fairly","different","from","what","the","current","iterator","based","local","mode","would","look","like","","let","s","just","remove","them","for","now","","and","we","can","always","re","introduced","them","in","the","future","by","looking","at","branch","remove","org","apache","spark","sql","execution","local"],"filtered":["introduced","local","operators","org","apache","spark","sql","execution","local","package","never","fully","wired","engine","actually","use","","still","plan","implement","full","local","mode","","probably","going","fairly","different","current","iterator","based","local","mode","look","like","","let","remove","","always","re","introduced","future","looking","branch","remove","org","apache","spark","sql","execution","local"],"features":{"type":0,"size":1000,"indices":[13,36,55,83,89,98,105,110,123,126,140,163,164,170,197,223,266,284,288,297,307,315,330,331,333,360,372,385,388,400,425,445,447,461,472,489,493,495,526,535,575,608,625,656,663,686,710,713,756,775,800,833,897,921,924,939,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,2.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0]},"cluster_label":2}
{"_c0":"We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances","_c1":"we need some rpc retry framework","document":"We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances we need some rpc retry framework","words":["we","need","some","mechanism","for","rpc","calls","that","get","exceptions","to","automatically","retry","the","call","under","certain","circumstances","","in","particular","","we","often","end","up","with","calls","to","rpcs","being","wrapped","with","retry","loops","for","timeouts","","we","should","be","able","to","make","a","retrying","proxy","that","will","call","the","rpc","and","retry","in","some","circumstances","we","need","some","rpc","retry","framework"],"filtered":["need","mechanism","rpc","calls","get","exceptions","automatically","retry","call","certain","circumstances","","particular","","often","end","calls","rpcs","wrapped","retry","loops","timeouts","","able","make","retrying","proxy","call","rpc","retry","circumstances","need","rpc","retry","framework"],"features":{"type":0,"size":1000,"indices":[19,36,39,100,128,146,170,181,284,333,372,374,388,400,401,420,428,445,496,505,510,525,537,599,621,638,650,656,665,710,712,717,760,804,866,880,945,959,965,993],"values":[1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,3.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,4.0]},"cluster_label":2}
{"_c0":"We re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless  Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting","_c1":"Eliminate needless uses of FileSystem  exists    isFile    isDirectory","document":"We re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless  Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting Eliminate needless uses of FileSystem  exists    isFile    isDirectory","words":["we","re","cleaning","up","hive","and","spark","s","use","of","filesystem","exists","","because","it","is","often","the","case","we","see","code","of","exists","open","","exists","delete","","when","the","exists","probe","is","needless","","against","object","stores","","expensive","needless","","hadoop","can","set","an","example","here","by","stripping","them","out","","it","will","also","show","where","there","are","opportunities","to","optimise","things","better","and","or","improve","reporting","eliminate","needless","uses","of","filesystem","","exists","","","","isfile","","","","isdirectory"],"filtered":["re","cleaning","hive","spark","use","filesystem","exists","","often","case","see","code","exists","open","","exists","delete","","exists","probe","needless","","object","stores","","expensive","needless","","hadoop","set","example","stripping","","also","show","opportunities","optimise","things","better","improve","reporting","eliminate","needless","uses","filesystem","","exists","","","","isfile","","","","isdirectory"],"features":{"type":0,"size":1000,"indices":[19,21,53,62,76,105,111,128,135,138,139,181,187,197,223,243,245,256,281,333,342,343,348,364,372,382,388,401,417,420,421,425,489,495,515,522,570,590,599,650,654,710,727,752,783,792,813,826,831,833,838,904,924,941,953,993],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,2.0,14.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"We re currently pulling in version       incubating   I think we should upgrade to the latest       incubating","_c1":"Upgrade HTrace version","document":"We re currently pulling in version       incubating   I think we should upgrade to the latest       incubating Upgrade HTrace version","words":["we","re","currently","pulling","in","version","","","","","","","incubating","","","i","think","we","should","upgrade","to","the","latest","","","","","","","incubating","upgrade","htrace","version"],"filtered":["re","currently","pulling","version","","","","","","","incubating","","","think","upgrade","latest","","","","","","","incubating","upgrade","htrace","version"],"features":{"type":0,"size":1000,"indices":[55,173,242,329,372,388,425,445,498,564,665,689,710,763,993,995],"values":[1.0,2.0,2.0,1.0,14.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":17}
{"_c0":"We removed some classes in Spark      If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version","_c1":"Display a better message for not finding classes removed in Spark","document":"We removed some classes in Spark      If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version Display a better message for not finding classes removed in Spark","words":["we","removed","some","classes","in","spark","","","","","","if","the","user","uses","an","incompatible","library","","he","may","see","classnotfoundexception","","it","s","better","to","give","an","instruction","to","ask","people","using","a","correct","version","display","a","better","message","for","not","finding","classes","removed","in","spark"],"filtered":["removed","classes","spark","","","","","","user","uses","incompatible","library","","may","see","classnotfoundexception","","better","give","instruction","ask","people","using","correct","version","display","better","message","finding","classes","removed","spark"],"features":{"type":0,"size":1000,"indices":[9,18,36,105,111,114,116,170,197,198,372,388,400,445,446,449,486,495,512,515,624,666,710,752,783,803,809,882,941,981,993,995,997],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,7.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"We should add a config to disable the  logs endpoint in HttpServer   Listing a directory like this can be dangerous from a security perspective  We can keep it enabled by default for compatibility though","_c1":"Add a config to disable the  logs endpoints","document":"We should add a config to disable the  logs endpoint in HttpServer   Listing a directory like this can be dangerous from a security perspective  We can keep it enabled by default for compatibility though Add a config to disable the  logs endpoints","words":["we","should","add","a","config","to","disable","the","","logs","endpoint","in","httpserver","","","listing","a","directory","like","this","can","be","dangerous","from","a","security","perspective","","we","can","keep","it","enabled","by","default","for","compatibility","though","add","a","config","to","disable","the","","logs","endpoints"],"filtered":["add","config","disable","","logs","endpoint","httpserver","","","listing","directory","like","dangerous","security","perspective","","keep","enabled","default","compatibility","though","add","config","disable","","logs","endpoints"],"features":{"type":0,"size":1000,"indices":[36,170,203,223,297,330,352,372,373,381,388,432,437,445,455,495,504,579,582,594,634,656,665,688,695,710,833,834,876,921,992,993],"values":[1.0,4.0,1.0,1.0,2.0,1.0,2.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"We should add an interface to the GLR summaries in Python for feature parity","_c1":"Python API for Generalized Linear Regression Summary","document":"We should add an interface to the GLR summaries in Python for feature parity Python API for Generalized Linear Regression Summary","words":["we","should","add","an","interface","to","the","glr","summaries","in","python","for","feature","parity","python","api","for","generalized","linear","regression","summary"],"filtered":["add","interface","glr","summaries","python","feature","parity","python","api","generalized","linear","regression","summary"],"features":{"type":0,"size":1000,"indices":[36,126,130,329,341,388,420,432,445,556,589,644,665,695,710,736,752,984,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should add text to DataFrameReader and DataFrameWriter","_c1":"Python API for text data source","document":"We should add text to DataFrameReader and DataFrameWriter Python API for text data source","words":["we","should","add","text","to","dataframereader","and","dataframewriter","python","api","for","text","data","source"],"filtered":["add","text","dataframereader","dataframewriter","python","api","text","data","source"],"features":{"type":0,"size":1000,"indices":[36,70,169,210,256,333,388,432,589,644,665,695,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson  The script would execute the following  and take just the password as an argument","_c1":"Create a test patch script for Hudson","document":"We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson  The script would execute the following  and take just the password as an argument Create a test patch script for Hudson","words":["we","should","create","a","script","that","hudson","uses","to","execute","test","patch","that","is","in","source","control","so","modifications","to","test","patch","sh","arguments","can","be","done","w","o","updating","hudson","","the","script","would","execute","the","following","","and","take","just","the","password","as","an","argument","create","a","test","patch","script","for","hudson"],"filtered":["create","script","hudson","uses","execute","test","patch","source","control","modifications","test","patch","sh","arguments","done","w","o","updating","hudson","","script","execute","following","","take","password","argument","create","test","patch","script","hudson"],"features":{"type":0,"size":1000,"indices":[36,70,91,111,116,137,163,170,209,265,281,307,333,350,368,372,388,445,481,536,572,586,587,656,665,707,710,752,760,800,811,833,846,855,874,880,915,993],"values":[1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We should deprecate ConnectionManager in     before removing it in","_c1":"Deprecate NIO ConnectionManager","document":"We should deprecate ConnectionManager in     before removing it in Deprecate NIO ConnectionManager","words":["we","should","deprecate","connectionmanager","in","","","","","before","removing","it","in","deprecate","nio","connectionmanager"],"filtered":["deprecate","connectionmanager","","","","","removing","deprecate","nio","connectionmanager"],"features":{"type":0,"size":1000,"indices":[96,159,275,372,445,495,665,951,968,993],"values":[2.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get","_c1":"Metrics for codegen size and perf","document":"We should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get Metrics for codegen size and perf","words":["we","should","expose","codahale","metrics","for","the","codegen","source","text","size","and","how","long","it","takes","to","compile","","the","size","is","particularly","interesting","","since","the","jvm","does","have","hard","limits","on","how","large","methods","can","get","metrics","for","codegen","size","and","perf"],"filtered":["expose","codahale","metrics","codegen","source","text","size","long","takes","compile","","size","particularly","interesting","","since","jvm","hard","limits","large","methods","get","metrics","codegen","size","perf"],"features":{"type":0,"size":1000,"indices":[36,70,82,106,109,125,129,169,192,237,254,263,281,299,300,333,372,388,443,463,495,512,585,665,698,710,753,765,782,833,836,904,959,993],"values":[2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We should include in Spark distribution the built source package for SparkR  This will enable help and vignettes when the package is used  Also this source package is what we would release to CRAN","_c1":"R   Include package vignettes and help pages  build source package in Spark distribution","document":"We should include in Spark distribution the built source package for SparkR  This will enable help and vignettes when the package is used  Also this source package is what we would release to CRAN R   Include package vignettes and help pages  build source package in Spark distribution","words":["we","should","include","in","spark","distribution","the","built","source","package","for","sparkr","","this","will","enable","help","and","vignettes","when","the","package","is","used","","also","this","source","package","is","what","we","would","release","to","cran","r","","","include","package","vignettes","and","help","pages","","build","source","package","in","spark","distribution"],"filtered":["include","spark","distribution","built","source","package","sparkr","","enable","help","vignettes","package","used","","also","source","package","release","cran","r","","","include","package","vignettes","help","pages","","build","source","package","spark","distribution"],"features":{"type":0,"size":1000,"indices":[36,70,76,105,163,266,280,281,333,347,371,372,373,388,401,420,445,526,536,570,605,631,665,710,767,792,798,906,993],"values":[1.0,3.0,1.0,2.0,1.0,5.0,1.0,2.0,2.0,2.0,1.0,5.0,3.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":2}
{"_c0":"We should make an effort to clean up the shell env var name space by removing unsafe variables  See comments for list","_c1":"Rename remove non HADOOP    etc from the shell scripts","document":"We should make an effort to clean up the shell env var name space by removing unsafe variables  See comments for list Rename remove non HADOOP    etc from the shell scripts","words":["we","should","make","an","effort","to","clean","up","the","shell","env","var","name","space","by","removing","unsafe","variables","","see","comments","for","list","rename","remove","non","hadoop","","","","etc","from","the","shell","scripts"],"filtered":["make","effort","clean","shell","env","var","name","space","removing","unsafe","variables","","see","comments","list","rename","remove","non","hadoop","","","","etc","shell","scripts"],"features":{"type":0,"size":1000,"indices":[15,36,101,123,126,128,136,181,207,223,224,242,272,275,288,340,372,388,515,525,573,577,665,710,728,752,867,921,968,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices I made these private originally  but they will be useful for users transitioning workloads","_c1":"Make the mllib ml linalg type conversion APIs public","document":"We should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices I made these private originally  but they will be useful for users transitioning workloads Make the mllib ml linalg type conversion APIs public","words":["we","should","open","up","the","apis","for","converting","between","new","","old","linear","algebra","types","","in","spark","mllib","linalg","","","","","vector","asml","","","vectors","fromml","","","same","for","sparse","dense","and","for","matrices","i","made","these","private","originally","","but","they","will","be","useful","for","users","transitioning","workloads","make","the","mllib","ml","linalg","type","conversion","apis","public"],"filtered":["open","apis","converting","new","","old","linear","algebra","types","","spark","mllib","linalg","","","","","vector","asml","","","vectors","fromml","","","sparse","dense","matrices","made","private","originally","","useful","users","transitioning","workloads","make","mllib","ml","linalg","type","conversion","apis","public"],"features":{"type":0,"size":1000,"indices":[18,25,36,48,83,105,114,128,149,167,217,237,272,324,329,333,344,372,420,445,460,461,465,481,498,521,525,526,528,635,656,665,672,704,710,742,755,782,783,842,993,994],"values":[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,11.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":9}
{"_c0":"We should remove methods for variance  stddev  skewness","_c1":"GroupedData should only keep common first order statistics","document":"We should remove methods for variance  stddev  skewness GroupedData should only keep common first order statistics","words":["we","should","remove","methods","for","variance","","stddev","","skewness","groupeddata","should","only","keep","common","first","order","statistics"],"filtered":["remove","methods","variance","","stddev","","skewness","groupeddata","keep","common","first","order","statistics"],"features":{"type":0,"size":1000,"indices":[36,113,129,142,183,288,372,434,594,665,718,899,939,954,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should remove the existing ExpressionOptimizationSuite  and update checkEvaluation to also run the optimizer version","_c1":"ExpressionEvalHelper checkEvaluation should also run the optimizer version","document":"We should remove the existing ExpressionOptimizationSuite  and update checkEvaluation to also run the optimizer version ExpressionEvalHelper checkEvaluation should also run the optimizer version","words":["we","should","remove","the","existing","expressionoptimizationsuite","","and","update","checkevaluation","to","also","run","the","optimizer","version","expressionevalhelper","checkevaluation","should","also","run","the","optimizer","version"],"filtered":["remove","existing","expressionoptimizationsuite","","update","checkevaluation","also","run","optimizer","version","expressionevalhelper","checkevaluation","also","run","optimizer","version"],"features":{"type":0,"size":1000,"indices":[49,196,288,295,333,343,364,371,372,388,665,710,792,930,993,995],"values":[1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"We should slim down the Docker image by removing JDK  now that trunk no longer supports it","_c1":"remove JDK  from Dockerfile","document":"We should slim down the Docker image by removing JDK  now that trunk no longer supports it remove JDK  from Dockerfile","words":["we","should","slim","down","the","docker","image","by","removing","jdk","","now","that","trunk","no","longer","supports","it","remove","jdk","","from","dockerfile"],"filtered":["slim","docker","image","removing","jdk","","trunk","longer","supports","remove","jdk","","dockerfile"],"features":{"type":0,"size":1000,"indices":[83,98,217,223,288,346,372,383,394,466,484,486,495,665,710,760,868,921,968,993,994],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should update to the latest version of Zinc in order to match our SBT version","_c1":"Upgrade Zinc from         to","document":"We should update to the latest version of Zinc in order to match our SBT version Upgrade Zinc from         to","words":["we","should","update","to","the","latest","version","of","zinc","in","order","to","match","our","sbt","version","upgrade","zinc","from","","","","","","","","","to"],"filtered":["update","latest","version","zinc","order","match","sbt","version","upgrade","zinc","","","","","","","",""],"features":{"type":0,"size":1000,"indices":[63,242,343,372,388,445,498,500,629,665,704,710,718,921,993,995],"values":[2.0,1.0,2.0,8.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"We should upgrade snappy java to         across all of our maintenance branches  This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues    for more context  This should be a major help in the Snappy debugging work that I ve been doing","_c1":"Upgrade snappy java to","document":"We should upgrade snappy java to         across all of our maintenance branches  This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues    for more context  This should be a major help in the Snappy debugging work that I ve been doing Upgrade snappy java to","words":["we","should","upgrade","snappy","java","to","","","","","","","","","across","all","of","our","maintenance","branches","","this","release","improves","error","messages","when","attempting","to","deserialize","empty","inputs","using","snappyinputstream","","this","operation","is","always","an","error","","but","the","old","error","messages","made","it","hard","to","distinguish","failures","due","to","empty","streams","from","ones","due","to","reading","invalid","","","corrupted","streams","","","see","https","","","github","com","xerial","snappy","java","issues","","","","for","more","context","","this","should","be","a","major","help","in","the","snappy","debugging","work","that","i","ve","been","doing","upgrade","snappy","java","to"],"filtered":["upgrade","snappy","java","","","","","","","","","across","maintenance","branches","","release","improves","error","messages","attempting","deserialize","empty","inputs","using","snappyinputstream","","operation","always","error","","old","error","messages","made","hard","distinguish","failures","due","empty","streams","ones","due","reading","invalid","","","corrupted","streams","","","see","https","","","github","com","xerial","snappy","java","issues","","","","context","","major","help","snappy","debugging","work","ve","upgrade","snappy","java"],"features":{"type":0,"size":1000,"indices":[13,36,76,83,100,115,125,170,190,199,221,242,257,281,310,320,329,330,333,343,344,345,347,371,372,373,385,388,415,445,449,469,475,488,495,510,514,515,517,527,535,567,624,629,656,665,672,704,710,738,752,760,801,826,833,890,893,911,921,935,967,968,983,993,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,21.0,3.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"We should upgrade to the latest release of MiMa          in order to include my fix for a bug which led to flakiness in the MiMa checks  https   github com typesafehub migration manager issues","_c1":"Upgrade to MiMa","document":"We should upgrade to the latest release of MiMa          in order to include my fix for a bug which led to flakiness in the MiMa checks  https   github com typesafehub migration manager issues Upgrade to MiMa","words":["we","should","upgrade","to","the","latest","release","of","mima","","","","","","","","","","in","order","to","include","my","fix","for","a","bug","which","led","to","flakiness","in","the","mima","checks","","https","","","github","com","typesafehub","migration","manager","issues","upgrade","to","mima"],"filtered":["upgrade","latest","release","mima","","","","","","","","","","order","include","fix","bug","led","flakiness","mima","checks","","https","","","github","com","typesafehub","migration","manager","issues","upgrade","mima"],"features":{"type":0,"size":1000,"indices":[36,170,221,242,249,329,343,371,372,388,401,445,460,475,498,510,526,597,599,641,665,710,718,745,916,950,993,998],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,12.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker","_c1":"Remove MRv  terms from HttpAuthentication md","document":"We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker Remove MRv  terms from HttpAuthentication md","words":["we","should","use","resourcemanager","and","nodemanager","instead","of","jobtracker","and","tasktracker","remove","mrv","","terms","from","httpauthentication","md"],"filtered":["use","resourcemanager","nodemanager","instead","jobtracker","tasktracker","remove","mrv","","terms","httpauthentication","md"],"features":{"type":0,"size":1000,"indices":[8,144,182,251,288,333,343,372,413,489,572,665,713,863,921,993,998],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We use KCL       in the current master  KCL       added integration with Kinesis Producer Library  KPL  and support auto de aggregation  It would be great to upgrade KCL to the latest stable version  Note that the latest version is       and       restored compatibility with dynamodb streams kinesis adapter  which was broken in        See https   github com awslabs amazon kinesis client release notes    tdas    brkyvz  Please recommend a version for upgrade","_c1":"Upgrade Kinesis Client Library to the latest stable version","document":"We use KCL       in the current master  KCL       added integration with Kinesis Producer Library  KPL  and support auto de aggregation  It would be great to upgrade KCL to the latest stable version  Note that the latest version is       and       restored compatibility with dynamodb streams kinesis adapter  which was broken in        See https   github com awslabs amazon kinesis client release notes    tdas    brkyvz  Please recommend a version for upgrade Upgrade Kinesis Client Library to the latest stable version","words":["we","use","kcl","","","","","","","in","the","current","master","","kcl","","","","","","","added","integration","with","kinesis","producer","library","","kpl","","and","support","auto","de","aggregation","","it","would","be","great","to","upgrade","kcl","to","the","latest","stable","version","","note","that","the","latest","version","is","","","","","","","and","","","","","","","restored","compatibility","with","dynamodb","streams","kinesis","adapter","","which","was","broken","in","","","","","","","","see","https","","","github","com","awslabs","amazon","kinesis","client","release","notes","","","","tdas","","","","brkyvz","","please","recommend","a","version","for","upgrade","upgrade","kinesis","client","library","to","the","latest","stable","version"],"filtered":["use","kcl","","","","","","","current","master","","kcl","","","","","","","added","integration","kinesis","producer","library","","kpl","","support","auto","de","aggregation","","great","upgrade","kcl","latest","stable","version","","note","latest","version","","","","","","","","","","","","","restored","compatibility","dynamodb","streams","kinesis","adapter","","broken","","","","","","","","see","https","","","github","com","awslabs","amazon","kinesis","client","release","notes","","","","tdas","","","","brkyvz","","please","recommend","version","upgrade","upgrade","kinesis","client","library","latest","stable","version"],"features":{"type":0,"size":1000,"indices":[20,36,135,163,170,221,234,242,260,270,281,288,294,326,327,333,345,371,372,384,388,434,445,446,460,489,494,495,498,510,515,530,564,597,650,652,656,680,695,710,745,760,780,781,828,834,841,909,948,993,995,998],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,46.0,1.0,3.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,3.0,3.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0]},"cluster_label":16}
{"_c0":"We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r","_c1":"Refactor k means code in SparkRWrappers","document":"We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r Refactor k means code in SparkRWrappers","words":["we","use","a","single","object","","sparkrwrappers","","","https","","","github","com","apache","spark","blob","master","mllib","src","main","scala","org","apache","spark","ml","r","sparkrwrappers","scala","","to","wrap","method","calls","to","glm","and","kmeans","in","sparkr","","this","is","quite","hard","to","maintain","","we","should","refactor","them","into","separate","wrappers","","like","","aftsurvivalregressionwrapper","","and","","naivebayeswrapper","","","the","package","name","should","be","","spakr","ml","r","","instead","of","","spark","ml","api","r","refactor","k","means","code","in","sparkrwrappers"],"filtered":["use","single","object","","sparkrwrappers","","","https","","","github","com","apache","spark","blob","master","mllib","src","main","scala","org","apache","spark","ml","r","sparkrwrappers","scala","","wrap","method","calls","glm","kmeans","sparkr","","quite","hard","maintain","","refactor","separate","wrappers","","like","","aftsurvivalregressionwrapper","","","naivebayeswrapper","","","package","name","","spakr","ml","r","","instead","","spark","ml","api","r","refactor","k","means","code","sparkrwrappers"],"features":{"type":0,"size":1000,"indices":[5,15,105,125,170,208,221,247,255,266,270,281,305,324,330,333,343,372,373,388,394,420,445,456,489,490,495,510,521,524,531,535,570,578,622,623,637,644,650,654,656,665,710,718,726,749,767,863,866,891,924,988,993,998,999],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,17.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":17}
{"_c0":"We want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib  To support the changes we want to make  we should move the implementation from spark mllib to spark ml  We will generalize and modify it  but will also ensure that we do not change the behavior of the old API  There are several steps to this     Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation  The current spark ml tests will ensure that the   implementations learn exactly the same models  Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions     Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation  The spark ml tests will again ensure that we do not change any behavior     Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence  This JIRA is now for step   only  Steps   and   will be in separate JIRAs  After these updates  we can more safely generalize and improve the spark ml implementation","_c1":"Move tree forest implementation from spark mllib to spark ml","document":"We want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib  To support the changes we want to make  we should move the implementation from spark mllib to spark ml  We will generalize and modify it  but will also ensure that we do not change the behavior of the old API  There are several steps to this     Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation  The current spark ml tests will ensure that the   implementations learn exactly the same models  Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions     Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation  The spark ml tests will again ensure that we do not change any behavior     Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence  This JIRA is now for step   only  Steps   and   will be in separate JIRAs  After these updates  we can more safely generalize and improve the spark ml implementation Move tree forest implementation from spark mllib to spark ml","words":["we","want","to","change","and","improve","the","spark","ml","api","for","trees","and","ensembles","","but","we","cannot","change","the","old","api","in","spark","mllib","","to","support","the","changes","we","want","to","make","","we","should","move","the","implementation","from","spark","mllib","to","spark","ml","","we","will","generalize","and","modify","it","","but","will","also","ensure","that","we","do","not","change","the","behavior","of","the","old","api","","there","are","several","steps","to","this","","","","","copy","the","implementation","over","to","spark","ml","and","change","the","spark","ml","classes","to","use","that","implementation","","rather","than","calling","the","spark","mllib","implementation","","the","current","spark","ml","tests","will","ensure","that","the","","","implementations","learn","exactly","the","same","models","","note","","this","should","include","performance","testing","to","make","sure","the","updated","code","does","not","have","any","regressions","","","","","","","update","","","i","have","run","tests","using","spark","perf","","and","there","were","no","regressions","","","","","remove","the","spark","mllib","implementation","","and","make","the","spark","mllib","apis","wrappers","around","the","spark","ml","implementation","","the","spark","ml","tests","will","again","ensure","that","we","do","not","change","any","behavior","","","","","move","the","unit","tests","to","spark","ml","","and","change","the","spark","mllib","unit","tests","to","verify","model","equivalence","","this","jira","is","now","for","step","","","only","","steps","","","and","","","will","be","in","separate","jiras","","after","these","updates","","we","can","more","safely","generalize","and","improve","the","spark","ml","implementation","move","tree","forest","implementation","from","spark","mllib","to","spark","ml"],"filtered":["want","change","improve","spark","ml","api","trees","ensembles","","change","old","api","spark","mllib","","support","changes","want","make","","move","implementation","spark","mllib","spark","ml","","generalize","modify","","also","ensure","change","behavior","old","api","","several","steps","","","","","copy","implementation","spark","ml","change","spark","ml","classes","use","implementation","","rather","calling","spark","mllib","implementation","","current","spark","ml","tests","ensure","","","implementations","learn","exactly","models","","note","","include","performance","testing","make","sure","updated","code","regressions","","","","","","","update","","","run","tests","using","spark","perf","","regressions","","","","","remove","spark","mllib","implementation","","make","spark","mllib","apis","wrappers","around","spark","ml","implementation","","spark","ml","tests","ensure","change","behavior","","","","","move","unit","tests","spark","ml","","change","spark","mllib","unit","tests","verify","model","equivalence","","jira","step","","","","steps","","","","","separate","jiras","","updates","","safely","generalize","improve","spark","ml","implementation","move","tree","forest","implementation","spark","mllib","spark","ml"],"features":{"type":0,"size":1000,"indices":[5,18,36,55,77,83,91,98,105,138,158,167,193,216,254,261,281,282,288,291,299,317,324,326,329,333,335,343,346,352,363,364,372,373,388,401,420,427,437,443,445,461,487,489,490,495,521,522,525,534,547,558,583,619,624,629,640,644,646,656,665,672,674,695,698,710,712,718,735,737,753,759,760,775,792,809,821,831,833,842,857,866,871,899,909,921,925,928,931,952,962,993],"values":[1.0,3.0,2.0,1.0,1.0,2.0,2.0,1.0,18.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,10.0,1.0,1.0,9.0,2.0,2.0,1.0,1.0,1.0,1.0,46.0,3.0,11.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,7.0,2.0,3.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,2.0,2.0,2.0,2.0,1.0,1.0,9.0,21.0,2.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,8.0]},"cluster_label":8}
{"_c0":"We want to rename       to       alpha  for the first alpha release  However  the version number is also encoded outside of the pom xml s  so we need to update these too","_c1":"Change project version from       to       alpha","document":"We want to rename       to       alpha  for the first alpha release  However  the version number is also encoded outside of the pom xml s  so we need to update these too Change project version from       to       alpha","words":["we","want","to","rename","","","","","","","to","","","","","","","alpha","","for","the","first","alpha","release","","however","","the","version","number","is","also","encoded","outside","of","the","pom","xml","s","","so","we","need","to","update","these","too","change","project","version","from","","","","","","","to","","","","","","","alpha"],"filtered":["want","rename","","","","","","","","","","","","","alpha","","first","alpha","release","","however","","version","number","also","encoded","outside","pom","xml","","need","update","change","project","version","","","","","","","","","","","","","alpha"],"features":{"type":0,"size":1000,"indices":[36,92,158,183,197,281,343,360,368,371,372,377,388,461,537,583,644,671,673,710,712,788,792,850,867,921,993,995],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,28.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":3}
{"_c0":"We want to support JSON serialization of vectors in order to support SPARK","_c1":"JSON serialization of Vectors","document":"We want to support JSON serialization of vectors in order to support SPARK JSON serialization of Vectors","words":["we","want","to","support","json","serialization","of","vectors","in","order","to","support","spark","json","serialization","of","vectors"],"filtered":["want","support","json","serialization","vectors","order","support","spark","json","serialization","vectors"],"features":{"type":0,"size":1000,"indices":[105,343,388,445,662,695,712,718,843,993,994],"values":[1.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"What StringIndexerInverse does is not strictly associated with StringIndexer  and the name is not super clear","_c1":"Rename StringIndexerInverse to IndexToString","document":"What StringIndexerInverse does is not strictly associated with StringIndexer  and the name is not super clear Rename StringIndexerInverse to IndexToString","words":["what","stringindexerinverse","does","is","not","strictly","associated","with","stringindexer","","and","the","name","is","not","super","clear","rename","stringindexerinverse","to","indextostring"],"filtered":["stringindexerinverse","strictly","associated","stringindexer","","name","super","clear","rename","stringindexerinverse","indextostring"],"features":{"type":0,"size":1000,"indices":[15,18,264,281,291,333,372,388,461,526,551,650,659,698,710,867,879,924],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"When SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer","_c1":"Remove noop SortOrder in Sort","document":"When SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer Remove noop SortOrder in Sort","words":["when","sortorder","does","not","contain","any","reference","","it","has","no","effect","on","the","sorting","","remove","the","noop","sortorder","in","optimizer","remove","noop","sortorder","in","sort"],"filtered":["sortorder","contain","reference","","effect","sorting","","remove","noop","sortorder","optimizer","remove","noop","sortorder","sort"],"features":{"type":0,"size":1000,"indices":[18,76,82,91,152,288,295,301,346,372,445,495,580,698,706,709,710,720,792,793],"values":[1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0]},"cluster_label":13}
{"_c0":"When a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching  This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels  There are two places where we request serialized bytes from the BlockStore     getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store     the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation  Therefore  I think this is a safe change","_c1":"Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills","document":"When a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching  This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels  There are two places where we request serialized bytes from the BlockStore     getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store     the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation  Therefore  I think this is a safe change Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills","words":["when","a","cached","block","is","spilled","to","disk","and","read","back","in","serialized","form","","i","e","","as","bytes","","","the","current","blockmanager","implementation","will","attempt","to","re","insert","the","serialized","block","into","the","memorystore","even","if","the","block","s","storage","level","requests","deserialized","caching","","this","behavior","adds","some","complexity","to","the","memorystore","but","i","don","t","think","it","offers","many","performance","benefits","and","i","d","like","to","remove","it","in","order","to","simplify","a","larger","refactoring","patch","","therefore","","i","propose","to","change","the","behavior","such","that","disk","store","reads","will","only","cache","bytes","in","the","memory","store","for","blocks","with","serialized","storage","levels","","there","are","two","places","where","we","request","serialized","bytes","from","the","blockstore","","","","","getlocalbytes","","","","which","is","only","called","when","reading","local","copies","of","torrentbroadcast","pieces","","broadcast","pieces","are","always","cached","using","a","serialized","storage","level","","so","this","won","t","lead","to","a","mismatch","in","serialization","forms","if","spilled","bytes","read","from","disk","are","cached","as","bytes","in","the","memory","store","","","","","the","non","shuffle","block","branch","in","getblockdata","","","","which","is","only","called","by","the","nettyblockrpcserver","when","responding","to","requests","to","read","remote","blocks","","caching","the","serialized","bytes","in","memory","will","only","benefit","us","if","those","cached","bytes","are","read","before","they","re","evicted","and","the","likelihood","of","that","happening","seems","low","since","the","frequency","of","remote","reads","of","non","broadcast","cached","blocks","seems","very","low","","caching","these","bytes","when","they","have","a","low","probability","of","being","read","is","bad","if","it","risks","the","eviction","of","blocks","which","are","cached","in","their","expected","serialized","deserialized","forms","","since","those","blocks","seem","more","likely","to","be","read","in","local","computation","","therefore","","i","think","this","is","a","safe","change","don","t","cache","memory","and","disk","blocks","as","bytes","in","memory","store","when","reading","spills"],"filtered":["cached","block","spilled","disk","read","back","serialized","form","","e","","bytes","","","current","blockmanager","implementation","attempt","re","insert","serialized","block","memorystore","even","block","storage","level","requests","deserialized","caching","","behavior","adds","complexity","memorystore","think","offers","many","performance","benefits","d","like","remove","order","simplify","larger","refactoring","patch","","therefore","","propose","change","behavior","disk","store","reads","cache","bytes","memory","store","blocks","serialized","storage","levels","","two","places","request","serialized","bytes","blockstore","","","","","getlocalbytes","","","","called","reading","local","copies","torrentbroadcast","pieces","","broadcast","pieces","always","cached","using","serialized","storage","level","","won","lead","mismatch","serialization","forms","spilled","bytes","read","disk","cached","bytes","memory","store","","","","","non","shuffle","block","branch","getblockdata","","","","called","nettyblockrpcserver","responding","requests","read","remote","blocks","","caching","serialized","bytes","memory","benefit","us","cached","bytes","read","re","evicted","likelihood","happening","seems","low","since","frequency","remote","reads","non","broadcast","cached","blocks","seems","low","","caching","bytes","low","probability","read","bad","risks","eviction","blocks","cached","expected","serialized","deserialized","forms","","since","blocks","seem","likely","read","local","computation","","therefore","","think","safe","change","cache","memory","disk","blocks","bytes","memory","store","reading","spills"],"features":{"type":0,"size":1000,"indices":[8,13,16,19,20,36,48,64,71,76,83,86,92,94,109,127,137,138,139,140,150,158,159,162,163,164,165,170,179,188,197,204,208,223,224,232,235,236,272,274,281,288,299,329,330,333,343,356,368,372,373,374,381,388,394,398,400,406,408,415,420,425,430,445,450,461,495,497,511,520,530,533,541,564,568,572,575,585,588,591,597,600,608,617,618,621,624,629,642,650,656,689,693,698,710,718,735,736,739,740,743,759,760,777,783,788,792,813,821,831,843,878,891,899,910,921,936,939,944,945,960,963,975,978,985,990,993,994,997],"values":[1.0,1.0,1.0,1.0,6.0,1.0,3.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,9.0,5.0,1.0,1.0,5.0,1.0,4.0,6.0,1.0,1.0,29.0,3.0,1.0,2.0,10.0,4.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,10.0,6.0,3.0,3.0,2.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,2.0,1.0,1.0,3.0,3.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,7.0,1.0,3.0,1.0,1.0,16.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,2.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0]},"cluster_label":14}
{"_c0":"When data need to be reprocessed in the database  there is currently no manual method to reload the chukwa sequence files into database  A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this","_c1":"Add utilities to load chukwa sequence file to database","document":"When data need to be reprocessed in the database  there is currently no manual method to reload the chukwa sequence files into database  A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this Add utilities to load chukwa sequence file to database","words":["when","data","need","to","be","reprocessed","in","the","database","","there","is","currently","no","manual","method","to","reload","the","chukwa","sequence","files","into","database","","a","few","minor","tweaks","to","metricsdataloader","should","be","possible","to","create","a","command","line","utility","to","do","this","add","utilities","to","load","chukwa","sequence","file","to","database"],"filtered":["data","need","reprocessed","database","","currently","manual","method","reload","chukwa","sequence","files","database","","minor","tweaks","metricsdataloader","possible","create","command","line","utility","add","utilities","load","chukwa","sequence","file","database"],"features":{"type":0,"size":1000,"indices":[8,76,100,108,125,135,170,182,258,265,281,282,332,346,372,373,388,400,430,432,445,473,534,537,551,553,608,638,649,654,656,665,695,710,763,831,858,891],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0]},"cluster_label":2}
{"_c0":"When generated code accesses a   ColumnarBatch   object  it is possible to get values of each column from   ColumnVector   instead of calling   getRow","_c1":"Direct consume ColumnVector in generated code when ColumnarBatch is used","document":"When generated code accesses a   ColumnarBatch   object  it is possible to get values of each column from   ColumnVector   instead of calling   getRow Direct consume ColumnVector in generated code when ColumnarBatch is used","words":["when","generated","code","accesses","a","","","columnarbatch","","","object","","it","is","possible","to","get","values","of","each","column","from","","","columnvector","","","instead","of","calling","","","getrow","direct","consume","columnvector","in","generated","code","when","columnarbatch","is","used"],"filtered":["generated","code","accesses","","","columnarbatch","","","object","","possible","get","values","column","","","columnvector","","","instead","calling","","","getrow","direct","consume","columnvector","generated","code","columnarbatch","used"],"features":{"type":0,"size":1000,"indices":[76,86,100,170,281,315,343,372,388,420,445,453,455,495,583,601,605,650,742,852,863,885,921,959,962,975],"values":[2.0,1.0,1.0,1.0,2.0,2.0,2.0,11.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"When o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar","_c1":"Remove bin rcc script","document":"When o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar Remove bin rcc script","words":["when","o","a","h","record","was","moved","","bin","rcc","was","never","updated","to","pull","those","classes","from","the","streaming","jar","remove","bin","rcc","script"],"filtered":["o","h","record","moved","","bin","rcc","never","updated","pull","classes","streaming","jar","remove","bin","rcc","script"],"features":{"type":0,"size":1000,"indices":[76,126,138,170,234,237,244,263,282,286,288,350,372,388,490,597,600,603,710,809,880,921],"values":[1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"When setting up a compression codec in an MR job the full class name of the codec must be used  To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip    instead their full codec class name  Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs","_c1":"Add capability to resolve compression codec based on codec name","document":"When setting up a compression codec in an MR job the full class name of the codec must be used  To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip    instead their full codec class name  Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs Add capability to resolve compression codec based on codec name","words":["when","setting","up","a","compression","codec","in","an","mr","job","the","full","class","name","of","the","codec","must","be","used","","to","ease","usability","","compression","codecs","should","be","resolved","by","their","codec","name","","ie","","gzip","","","","deflate","","","","zlib","","","","bzip","","","","instead","their","full","codec","class","name","","besides","easy","of","use","for","hadoop","users","who","would","use","the","codec","alias","instead","the","full","codec","class","name","","it","could","simplify","how","hbase","resolves","loads","the","codecs","add","capability","to","resolve","compression","codec","based","on","codec","name"],"filtered":["setting","compression","codec","mr","job","full","class","name","codec","must","used","","ease","usability","","compression","codecs","resolved","codec","name","","ie","","gzip","","","","deflate","","","","zlib","","","","bzip","","","","instead","full","codec","class","name","","besides","easy","use","hadoop","users","use","codec","alias","instead","full","codec","class","name","","simplify","hbase","resolves","loads","codecs","add","capability","resolve","compression","codec","based","codec","name"],"features":{"type":0,"size":1000,"indices":[15,19,23,29,36,62,76,81,82,128,163,170,176,181,213,217,223,235,237,272,287,323,336,343,372,388,402,432,441,445,470,489,495,534,578,591,605,625,656,659,662,665,710,731,752,755,760,787,812,827,863,897,956],"values":[5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,8.0,2.0,18.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,3.0,1.0]},"cluster_label":17}
{"_c0":"When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation","_c1":"Data Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider","document":"When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation Data Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider","words":["when","users","try","to","implement","a","data","source","api","with","extending","only","relationprovider","and","creatablerelationprovider","","they","will","hit","an","error","when","resolving","the","relation","data","source","apis","","extending","relationprovider","and","creatablerelationprovider","without","schemarelationprovider"],"filtered":["users","try","implement","data","source","api","extending","relationprovider","creatablerelationprovider","","hit","error","resolving","relation","data","source","apis","","extending","relationprovider","creatablerelationprovider","without","schemarelationprovider"],"features":{"type":0,"size":1000,"indices":[48,70,76,159,170,280,333,372,379,388,420,472,609,644,650,695,708,710,752,755,842,882,884,899,946],"values":[1.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"When we use the   jdbc   in pyspark  if we check the lowerBound and upperBound  we can give a more friendly suggestion","_c1":"Check the lowerBound and upperBound whether equal None in jdbc API","document":"When we use the   jdbc   in pyspark  if we check the lowerBound and upperBound  we can give a more friendly suggestion Check the lowerBound and upperBound whether equal None in jdbc API","words":["when","we","use","the","","","jdbc","","","in","pyspark","","if","we","check","the","lowerbound","and","upperbound","","we","can","give","a","more","friendly","suggestion","check","the","lowerbound","and","upperbound","whether","equal","none","in","jdbc","api"],"filtered":["use","","","jdbc","","","pyspark","","check","lowerbound","upperbound","","give","friendly","suggestion","check","lowerbound","upperbound","whether","equal","none","jdbc","api"],"features":{"type":0,"size":1000,"indices":[76,135,170,333,372,402,445,489,509,538,629,643,644,709,710,783,790,807,831,833,882,993],"values":[1.0,2.0,2.0,2.0,6.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0]},"cluster_label":2}
{"_c0":"When you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass    However  when reflect on this we try and load   org mycompany project MyClass","_c1":"Support for classes defined in package objects","document":"When you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass    However  when reflect on this we try and load   org mycompany project MyClass Support for classes defined in package objects","words":["when","you","define","a","class","inside","of","a","package","object","","the","name","ends","up","being","something","like","","","org","mycompany","project","package","myclass","","","","however","","when","reflect","on","this","we","try","and","load","","","org","mycompany","project","myclass","support","for","classes","defined","in","package","objects"],"filtered":["define","class","inside","package","object","","name","ends","something","like","","","org","mycompany","project","package","myclass","","","","however","","reflect","try","load","","","org","mycompany","project","myclass","support","classes","defined","package","objects"],"features":{"type":0,"size":1000,"indices":[15,36,76,82,122,126,128,159,170,174,256,258,266,330,333,336,338,343,372,373,374,425,445,463,534,535,553,650,671,673,695,710,809,993,999],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"When you have a key value class that s non Writable and you forget to attach io serializers for the same  an NPE is thrown by the tasks with no information on why or what s missing and what led to it  I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones","_c1":"When a serializer class is missing  return null  not throw an NPE","document":"When you have a key value class that s non Writable and you forget to attach io serializers for the same  an NPE is thrown by the tasks with no information on why or what s missing and what led to it  I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones When a serializer class is missing  return null  not throw an NPE","words":["when","you","have","a","key","value","class","that","s","non","writable","and","you","forget","to","attach","io","serializers","for","the","same","","an","npe","is","thrown","by","the","tasks","with","no","information","on","why","or","what","s","missing","and","what","led","to","it","","i","think","a","better","exception","can","be","thrown","by","serializationfactory","instead","of","an","npe","when","a","class","is","not","found","accepted","by","any","of","the","loaded","ones","when","a","serializer","class","is","missing","","return","null","","not","throw","an","npe"],"filtered":["key","value","class","non","writable","forget","attach","io","serializers","","npe","thrown","tasks","information","missing","led","","think","better","exception","thrown","serializationfactory","instead","npe","class","found","accepted","loaded","ones","serializer","class","missing","","return","null","","throw","npe"],"features":{"type":0,"size":1000,"indices":[5,18,36,76,82,91,118,129,170,187,191,193,197,223,224,267,281,299,329,333,343,345,346,355,359,372,388,398,425,495,525,526,534,560,564,567,593,650,651,656,691,710,752,760,768,769,831,833,838,863,941,955,973,978],"values":[1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,2.0,3.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"While running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below   Stack trace   org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java     org xerial snappy SnappyInputStream rawRead SnappyInputStream java      org xerial snappy SnappyInputStream read SnappyInputStream java      java io DataInputStream readFully DataInputStream java      java io DataInputStream readLong DataInputStream java      org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java     org apache spark util collection unsafe sort UnsafeSorterSpillMerger   loadNext UnsafeSorterSpillMerger java     org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to use with non JNI based System arrayCopy method in this case","_c1":"Significant amount of CPU is being consumed in SnappyNative arrayCopy method","document":"While running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below   Stack trace   org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java     org xerial snappy SnappyInputStream rawRead SnappyInputStream java      org xerial snappy SnappyInputStream read SnappyInputStream java      java io DataInputStream readFully DataInputStream java      java io DataInputStream readLong DataInputStream java      org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java     org apache spark util collection unsafe sort UnsafeSorterSpillMerger   loadNext UnsafeSorterSpillMerger java     org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to use with non JNI based System arrayCopy method in this case Significant amount of CPU is being consumed in SnappyNative arrayCopy method","words":["while","running","a","spark","job","which","is","spilling","a","lot","of","data","in","reduce","phase","","we","see","that","significant","amount","of","cpu","is","being","consumed","in","native","snappy","arraycopy","method","","please","see","the","stack","trace","below","","","stack","trace","","","org","xerial","snappy","snappynative","","","yjp","","arraycopy","native","method","","org","xerial","snappy","snappynative","arraycopy","snappynative","java","","org","xerial","snappy","snappy","arraycopy","snappy","java","","","","","org","xerial","snappy","snappyinputstream","rawread","snappyinputstream","java","","","","","","org","xerial","snappy","snappyinputstream","read","snappyinputstream","java","","","","","","java","io","datainputstream","readfully","datainputstream","java","","","","","","java","io","datainputstream","readlong","datainputstream","java","","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillreader","loadnext","unsafesorterspillreader","java","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillmerger","","","loadnext","unsafesorterspillmerger","java","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","the","reason","for","that","is","the","spillreader","does","a","lot","of","small","reads","from","the","underlying","snappy","compressed","stream","and","snappyinputstream","invokes","native","jni","arraycopy","method","to","copy","the","data","","which","is","expensive","","we","should","fix","snappy","","java","to","use","with","non","jni","based","system","arraycopy","method","in","this","case","significant","amount","of","cpu","is","being","consumed","in","snappynative","arraycopy","method"],"filtered":["running","spark","job","spilling","lot","data","reduce","phase","","see","significant","amount","cpu","consumed","native","snappy","arraycopy","method","","please","see","stack","trace","","","stack","trace","","","org","xerial","snappy","snappynative","","","yjp","","arraycopy","native","method","","org","xerial","snappy","snappynative","arraycopy","snappynative","java","","org","xerial","snappy","snappy","arraycopy","snappy","java","","","","","org","xerial","snappy","snappyinputstream","rawread","snappyinputstream","java","","","","","","org","xerial","snappy","snappyinputstream","read","snappyinputstream","java","","","","","","java","io","datainputstream","readfully","datainputstream","java","","","","","","java","io","datainputstream","readlong","datainputstream","java","","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillreader","loadnext","unsafesorterspillreader","java","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillmerger","","","loadnext","unsafesorterspillmerger","java","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","reason","spillreader","lot","small","reads","underlying","snappy","compressed","stream","snappyinputstream","invokes","native","jni","arraycopy","method","copy","data","","expensive","","fix","snappy","","java","use","non","jni","based","system","arraycopy","method","case","significant","amount","cpu","consumed","snappynative","arraycopy","method"],"features":{"type":0,"size":1000,"indices":[36,62,101,105,155,159,169,170,187,191,197,215,216,218,224,230,240,242,268,273,281,284,317,320,333,342,343,372,373,374,388,445,469,470,489,490,495,502,515,517,526,528,535,537,597,602,603,625,637,639,650,654,665,686,692,695,698,699,707,710,720,735,742,760,772,778,803,838,841,855,889,921,945,963,967,993],"values":[1.0,1.0,2.0,5.0,2.0,1.0,1.0,3.0,3.0,2.0,4.0,4.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,5.0,2.0,1.0,10.0,1.0,2.0,4.0,62.0,1.0,2.0,3.0,5.0,5.0,1.0,1.0,2.0,4.0,1.0,2.0,5.0,4.0,1.0,9.0,2.0,2.0,1.0,7.0,1.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,5.0,2.0,3.0,1.0,2.0,2.0,2.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,13.0,2.0]},"cluster_label":4}
{"_c0":"With HADOOP       now committed  we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts  converting them to use the new   slaves option  Additionally  the documentation should be updated to reflect these new command options","_c1":"Update sbin commands and documentation to use new   slaves option","document":"With HADOOP       now committed  we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts  converting them to use the new   slaves option  Additionally  the documentation should be updated to reflect these new command options Update sbin commands and documentation to use new   slaves option","words":["with","hadoop","","","","","","","now","committed","","we","need","to","remove","usages","of","yarn","daemons","sh","and","hadoop","daemons","sh","from","the","start","and","stop","scripts","","converting","them","to","use","the","new","","","slaves","option","","additionally","","the","documentation","should","be","updated","to","reflect","these","new","command","options","update","sbin","commands","and","documentation","to","use","new","","","slaves","option"],"filtered":["hadoop","","","","","","","committed","","need","remove","usages","yarn","daemons","sh","hadoop","daemons","sh","start","stop","scripts","","converting","use","new","","","slaves","option","","additionally","","documentation","updated","reflect","new","command","options","update","sbin","commands","documentation","use","new","","","slaves","option"],"features":{"type":0,"size":1000,"indices":[25,98,122,127,135,149,177,181,222,288,333,343,344,372,383,388,461,467,489,490,498,537,564,573,605,631,650,651,656,665,710,796,846,921,924,993,996],"values":[3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,2.0,1.0,14.0,2.0,4.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"With the commit of HADOOP        the CHANGES txt files are now EOLed  We should remove them","_c1":"Remove all of the CHANGES txt files","document":"With the commit of HADOOP        the CHANGES txt files are now EOLed  We should remove them Remove all of the CHANGES txt files","words":["with","the","commit","of","hadoop","","","","","","","","the","changes","txt","files","are","now","eoled","","we","should","remove","them","remove","all","of","the","changes","txt","files"],"filtered":["commit","hadoop","","","","","","","","changes","txt","files","eoled","","remove","remove","changes","txt","files"],"features":{"type":0,"size":1000,"indices":[98,138,181,288,343,363,372,551,650,665,670,709,710,759,924,968,993],"values":[1.0,1.0,1.0,2.0,2.0,2.0,8.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"add a metadata file giving the FS impl of swift  remove the entry from core default xml","_c1":"swift FS to add a service load metadata file","document":"add a metadata file giving the FS impl of swift  remove the entry from core default xml swift FS to add a service load metadata file","words":["add","a","metadata","file","giving","the","fs","impl","of","swift","","remove","the","entry","from","core","default","xml","swift","fs","to","add","a","service","load","metadata","file"],"filtered":["add","metadata","file","giving","fs","impl","swift","","remove","entry","core","default","xml","swift","fs","add","service","load","metadata","file"],"features":{"type":0,"size":1000,"indices":[92,108,135,170,228,245,258,288,343,372,381,388,432,555,609,710,749,753,887,921],"values":[1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"cc   nongli    please attach the design doc","_c1":"bucketed table support","document":"cc   nongli    please attach the design doc bucketed table support","words":["cc","","","nongli","","","","please","attach","the","design","doc","bucketed","table","support"],"filtered":["cc","","","nongli","","","","please","attach","design","doc","bucketed","table","support"],"features":{"type":0,"size":1000,"indices":[25,189,372,605,681,695,710,769,837,841,918],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"currently in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame  However  collect   currently can t collect data of nested types from a DataFrame because     The serializer in JVM backend does not support nested types     collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector","_c1":"Improve the implementation of collect   on DataFrame in SparkR","document":"currently in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame  However  collect   currently can t collect data of nested types from a DataFrame because     The serializer in JVM backend does not support nested types     collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector Improve the implementation of collect   on DataFrame in SparkR","words":["currently","in","sparkr","","collect","","","on","a","dataframe","collects","the","data","within","the","dataframe","into","a","local","data","frame","","r","users","are","used","to","using","data","frame","","however","","collect","","","currently","can","t","collect","data","of","nested","types","from","a","dataframe","because","","","","","the","serializer","in","jvm","backend","does","not","support","nested","types","","","","","collect","","","in","r","side","assumes","each","column","is","of","simple","atomic","type","that","can","be","combinded","into","a","atomic","vector","improve","the","implementation","of","collect","","","on","dataframe","in","sparkr"],"filtered":["currently","sparkr","","collect","","","dataframe","collects","data","within","dataframe","local","data","frame","","r","users","used","using","data","frame","","however","","collect","","","currently","collect","data","nested","types","dataframe","","","","","serializer","jvm","backend","support","nested","types","","","","","collect","","","r","side","assumes","column","simple","atomic","type","combinded","atomic","vector","improve","implementation","collect","","","dataframe","sparkr"],"features":{"type":0,"size":1000,"indices":[5,18,82,101,138,161,170,281,300,343,372,388,412,421,445,465,522,526,570,593,594,601,605,608,624,656,673,695,698,710,742,755,760,763,767,777,781,789,833,852,885,891,921,953,980],"values":[1.0,1.0,4.0,1.0,1.0,4.0,4.0,1.0,1.0,3.0,20.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,2.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"deprecate DistCpV  and Logalyzer  which are no longer used","_c1":"Deprecate DistCpV  and Logalyzer","document":"deprecate DistCpV  and Logalyzer  which are no longer used Deprecate DistCpV  and Logalyzer","words":["deprecate","distcpv","","and","logalyzer","","which","are","no","longer","used","deprecate","distcpv","","and","logalyzer"],"filtered":["deprecate","distcpv","","logalyzer","","longer","used","deprecate","distcpv","","logalyzer"],"features":{"type":0,"size":1000,"indices":[25,138,178,275,333,346,372,597,605,868],"values":[2.0,1.0,2.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"distcpv  is pretty much unsupported  we should just remove it","_c1":"Remove DistCpV  and Logalyzer","document":"distcpv  is pretty much unsupported  we should just remove it Remove DistCpV  and Logalyzer","words":["distcpv","","is","pretty","much","unsupported","","we","should","just","remove","it","remove","distcpv","","and","logalyzer"],"filtered":["distcpv","","pretty","much","unsupported","","remove","remove","distcpv","","logalyzer"],"features":{"type":0,"size":1000,"indices":[25,178,281,288,307,333,372,495,524,624,665,993],"values":[2.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"distinct   and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns","_c1":"Implement dropDuplicates   method of DataFrame in SparkR","document":"distinct   and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns Implement dropDuplicates   method of DataFrame in SparkR","words":["distinct","","","and","unique","","","drop","duplicated","rows","on","all","columns","","while","dropduplicates","","","can","drop","duplicated","rows","on","selected","columns","implement","dropduplicates","","","method","of","dataframe","in","sparkr"],"filtered":["distinct","","","unique","","","drop","duplicated","rows","columns","","dropduplicates","","","drop","duplicated","rows","selected","columns","implement","dropduplicates","","","method","dataframe","sparkr"],"features":{"type":0,"size":1000,"indices":[43,82,161,333,343,372,373,438,445,472,511,577,654,670,707,767,833,968,969],"values":[1.0,2.0,1.0,3.0,1.0,9.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":9}
{"_c0":"guice     doesn t work with lambda statement  https   github com google guice issues     We should upgrade it to     which includes the fix","_c1":"JDK   Update guice version to","document":"guice     doesn t work with lambda statement  https   github com google guice issues     We should upgrade it to     which includes the fix JDK   Update guice version to","words":["guice","","","","","doesn","t","work","with","lambda","statement","","https","","","github","com","google","guice","issues","","","","","we","should","upgrade","it","to","","","","","which","includes","the","fix","jdk","","","update","guice","version","to"],"filtered":["guice","","","","","doesn","work","lambda","statement","","https","","","github","com","google","guice","issues","","","","","upgrade","","","","","includes","fix","jdk","","","update","guice","version"],"features":{"type":0,"size":1000,"indices":[211,221,234,242,341,343,372,388,445,475,486,495,500,510,527,596,597,650,665,710,777,852,993,995,998],"values":[1.0,1.0,1.0,1.0,3.0,1.0,17.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"hadoop common src main bin hadoop config sh needs to be updated post MR   eg the layout of mapred home has changed","_c1":"hadoop config sh needs to be updated post MR","document":"hadoop common src main bin hadoop config sh needs to be updated post MR   eg the layout of mapred home has changed hadoop config sh needs to be updated post MR","words":["hadoop","common","src","main","bin","hadoop","config","sh","needs","to","be","updated","post","mr","","","eg","the","layout","of","mapred","home","has","changed","hadoop","config","sh","needs","to","be","updated","post","mr"],"filtered":["hadoop","common","src","main","bin","hadoop","config","sh","needs","updated","post","mr","","","eg","layout","mapred","home","changed","hadoop","config","sh","needs","updated","post","mr"],"features":{"type":0,"size":1000,"indices":[9,181,237,272,343,352,365,372,388,392,490,580,637,656,666,710,846,954,973,982,988],"values":[1.0,3.0,1.0,2.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"hadoop common src main bin hadoop config sh needs to be updated post mavenization  eg it still refers to build classes etc","_c1":"hadoop config sh needs to be updated post mavenization","document":"hadoop common src main bin hadoop config sh needs to be updated post mavenization  eg it still refers to build classes etc hadoop config sh needs to be updated post mavenization","words":["hadoop","common","src","main","bin","hadoop","config","sh","needs","to","be","updated","post","mavenization","","eg","it","still","refers","to","build","classes","etc","hadoop","config","sh","needs","to","be","updated","post","mavenization"],"filtered":["hadoop","common","src","main","bin","hadoop","config","sh","needs","updated","post","mavenization","","eg","still","refers","build","classes","etc","hadoop","config","sh","needs","updated","post","mavenization"],"features":{"type":0,"size":1000,"indices":[9,101,181,237,352,372,388,490,495,536,637,656,666,752,800,809,846,954,973,988],"values":[1.0,1.0,3.0,2.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations  Additionally  the token files that are created use Java serializations which are hard impossible to deal with in other languages  It should be replaced with a better utility in common that can read write protobuf based token files  has enough flexibility to be used with other services  and offers key functionality such as append and rename  The old version file format should still be supported for backward compatibility  but will be effectively deprecated  A follow on JIRA will deprecrate fetchdt","_c1":"Updated utility to create modify token files","document":"hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations  Additionally  the token files that are created use Java serializations which are hard impossible to deal with in other languages  It should be replaced with a better utility in common that can read write protobuf based token files  has enough flexibility to be used with other services  and offers key functionality such as append and rename  The old version file format should still be supported for backward compatibility  but will be effectively deprecated  A follow on JIRA will deprecrate fetchdt Updated utility to create modify token files","words":["hdfs","fetchdt","is","missing","some","critical","features","and","is","geared","almost","exclusively","towards","hdfs","operations","","additionally","","the","token","files","that","are","created","use","java","serializations","which","are","hard","impossible","to","deal","with","in","other","languages","","it","should","be","replaced","with","a","better","utility","in","common","that","can","read","write","protobuf","based","token","files","","has","enough","flexibility","to","be","used","with","other","services","","and","offers","key","functionality","such","as","append","and","rename","","the","old","version","file","format","should","still","be","supported","for","backward","compatibility","","but","will","be","effectively","deprecated","","a","follow","on","jira","will","deprecrate","fetchdt","updated","utility","to","create","modify","token","files"],"filtered":["hdfs","fetchdt","missing","critical","features","geared","almost","exclusively","towards","hdfs","operations","","additionally","","token","files","created","use","java","serializations","hard","impossible","deal","languages","","replaced","better","utility","common","read","write","protobuf","based","token","files","","enough","flexibility","used","services","","offers","key","functionality","append","rename","","old","version","file","format","still","supported","backward","compatibility","","effectively","deprecated","","follow","jira","deprecrate","fetchdt","updated","utility","create","modify","token","files"],"features":{"type":0,"size":1000,"indices":[5,20,36,62,82,83,96,108,113,120,125,138,170,197,222,254,262,265,272,281,326,333,344,355,356,365,372,385,388,394,400,411,420,433,445,486,489,490,493,495,525,528,551,572,580,593,597,605,609,620,625,649,650,651,656,665,672,674,687,710,728,735,755,760,800,821,833,834,850,867,880,941,954,967,995],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,4.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0]},"cluster_label":9}
{"_c0":"is missing in pom xml  That way   mvn versions set   does not work for the project","_c1":"Missing hadoop cloud storage project module in pom xml","document":"is missing in pom xml  That way   mvn versions set   does not work for the project Missing hadoop cloud storage project module in pom xml","words":["is","missing","in","pom","xml","","that","way","","","mvn","versions","set","","","does","not","work","for","the","project","missing","hadoop","cloud","storage","project","module","in","pom","xml"],"filtered":["missing","pom","xml","","way","","","mvn","versions","set","","","work","project","missing","hadoop","cloud","storage","project","module","pom","xml"],"features":{"type":0,"size":1000,"indices":[18,36,91,92,159,181,240,281,299,360,372,394,445,525,527,671,698,710,760,813],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"kms dt currently does not have its own token identifier class to de","_c1":"Support decoding KMS Delegation Token with its own Identifier","document":"kms dt currently does not have its own token identifier class to de Support decoding KMS Delegation Token with its own Identifier","words":["kms","dt","currently","does","not","have","its","own","token","identifier","class","to","de","support","decoding","kms","delegation","token","with","its","own","identifier"],"filtered":["kms","dt","currently","token","identifier","class","de","support","decoding","kms","delegation","token","identifier"],"features":{"type":0,"size":1000,"indices":[18,228,281,296,299,360,388,494,528,534,615,650,695,698,763,782,906],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"kms was not rewritten to use the new shell framework  It should be reworked to take advantage of it","_c1":"Rewrite kms to use new shell framework","document":"kms was not rewritten to use the new shell framework  It should be reworked to take advantage of it Rewrite kms to use new shell framework","words":["kms","was","not","rewritten","to","use","the","new","shell","framework","","it","should","be","reworked","to","take","advantage","of","it","rewrite","kms","to","use","new","shell","framework"],"filtered":["kms","rewritten","use","new","shell","framework","","reworked","take","advantage","rewrite","kms","use","new","shell","framework"],"features":{"type":0,"size":1000,"indices":[3,18,25,123,234,343,372,388,489,495,534,599,615,656,665,710,738,855,928],"values":[1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"make MultilayerPerceptronClassifier layers and weights public","_c1":"make MultilayerPerceptronClassifier layers and weights public","document":"make MultilayerPerceptronClassifier layers and weights public make MultilayerPerceptronClassifier layers and weights public","words":["make","multilayerperceptronclassifier","layers","and","weights","public","make","multilayerperceptronclassifier","layers","and","weights","public"],"filtered":["make","multilayerperceptronclassifier","layers","weights","public","make","multilayerperceptronclassifier","layers","weights","public"],"features":{"type":0,"size":1000,"indices":[333,359,415,498,504,525],"values":[2.0,2.0,2.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"runs  introduces extra complexity and overhead in MLlib s k means implementation  I haven t seen much usage with  runs  not equal to      We can deprecate this method in      and remove or void it in      It helps us simplify the implementation","_c1":"Deprecate  runs  in k means","document":"runs  introduces extra complexity and overhead in MLlib s k means implementation  I haven t seen much usage with  runs  not equal to      We can deprecate this method in      and remove or void it in      It helps us simplify the implementation Deprecate  runs  in k means","words":["runs","","introduces","extra","complexity","and","overhead","in","mllib","s","k","means","implementation","","i","haven","t","seen","much","usage","with","","runs","","not","equal","to","","","","","","we","can","deprecate","this","method","in","","","","","","and","remove","or","void","it","in","","","","","","it","helps","us","simplify","the","implementation","deprecate","","runs","","in","k","means"],"filtered":["runs","","introduces","extra","complexity","overhead","mllib","k","means","implementation","","haven","seen","much","usage","","runs","","equal","","","","","","deprecate","method","","","","","","remove","void","","","","","","helps","us","simplify","implementation","deprecate","","runs","","k","means"],"features":{"type":0,"size":1000,"indices":[8,18,19,96,114,154,158,187,197,208,275,288,327,329,333,372,373,388,394,402,445,456,495,521,524,530,650,654,698,710,736,777,817,833,964,993],"values":[1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,21.0,1.0,1.0,2.0,1.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"simplify Parsers implementation   add map and reduce side to the demux   add dynamic link between RecordType and Parsers using configuration file and alias   encapsulate data files creation location and naming convention inside the core demux classes   sort all data by TimePartition Machine Timestamp by default","_c1":"Update Chukwa Demux process","document":"simplify Parsers implementation   add map and reduce side to the demux   add dynamic link between RecordType and Parsers using configuration file and alias   encapsulate data files creation location and naming convention inside the core demux classes   sort all data by TimePartition Machine Timestamp by default Update Chukwa Demux process","words":["simplify","parsers","implementation","","","add","map","and","reduce","side","to","the","demux","","","add","dynamic","link","between","recordtype","and","parsers","using","configuration","file","and","alias","","","encapsulate","data","files","creation","location","and","naming","convention","inside","the","core","demux","classes","","","sort","all","data","by","timepartition","machine","timestamp","by","default","update","chukwa","demux","process"],"filtered":["simplify","parsers","implementation","","","add","map","reduce","side","demux","","","add","dynamic","link","recordtype","parsers","using","configuration","file","alias","","","encapsulate","data","files","creation","location","naming","convention","inside","core","demux","classes","","","sort","data","timepartition","machine","timestamp","default","update","chukwa","demux","process"],"features":{"type":0,"size":1000,"indices":[19,20,22,49,66,87,108,140,160,202,223,228,287,333,343,372,381,388,389,432,463,481,484,502,551,624,638,691,695,698,710,720,741,748,781,809,852,968,999],"values":[1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,8.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":9}
{"_c0":"spark logit is added in      We need to update spark vignettes to reflect the changes  This is part of SparkR QA work","_c1":"Update spark logit in sparkr vignettes","document":"spark logit is added in      We need to update spark vignettes to reflect the changes  This is part of SparkR QA work Update spark logit in sparkr vignettes","words":["spark","logit","is","added","in","","","","","","we","need","to","update","spark","vignettes","to","reflect","the","changes","","this","is","part","of","sparkr","qa","work","update","spark","logit","in","sparkr","vignettes"],"filtered":["spark","logit","added","","","","","","need","update","spark","vignettes","reflect","changes","","part","sparkr","qa","work","update","spark","logit","sparkr","vignettes"],"features":{"type":0,"size":1000,"indices":[105,122,281,318,343,363,372,373,384,388,445,526,527,537,710,740,767,856,993],"values":[3.0,1.0,2.0,2.0,3.0,1.0,6.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"the share hadoop  component  template directories are from when RPM and such were built as part of the build system  that no longer happens and now those files cause more harm than good since they are in the classpath  let s remove them","_c1":"Remove vestigal templates directories creation","document":"the share hadoop  component  template directories are from when RPM and such were built as part of the build system  that no longer happens and now those files cause more harm than good since they are in the classpath  let s remove them Remove vestigal templates directories creation","words":["the","share","hadoop","","component","","template","directories","are","from","when","rpm","and","such","were","built","as","part","of","the","build","system","","that","no","longer","happens","and","now","those","files","cause","more","harm","than","good","since","they","are","in","the","classpath","","let","s","remove","them","remove","vestigal","templates","directories","creation"],"filtered":["share","hadoop","","component","","template","directories","rpm","built","part","build","system","","longer","happens","files","cause","harm","good","since","classpath","","let","remove","remove","vestigal","templates","directories","creation"],"features":{"type":0,"size":1000,"indices":[48,76,98,138,140,151,164,168,177,181,197,261,272,288,333,343,346,372,418,445,470,536,551,572,585,593,600,609,629,639,682,710,740,760,854,856,868,906,921,924,935,962],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming  it let us no choice but to implement one for ourself  this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only here is how you can use it  endpoint root   streaming api v     Endpoint    Meaning      statistics Statistics information of stream    receivers A list of all receiver streams    receivers   stream id   Details of the given receiver stream    batches A list of all retained batches    batches   batch id   Details of the given batch    batches   batch id   operations A list of all output operations of the given batch    batches   batch id   operations   operation id   Details of the given operation  given batch","_c1":"Add a REST api to spark streaming","document":"trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming  it let us no choice but to implement one for ourself  this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only here is how you can use it  endpoint root   streaming api v     Endpoint    Meaning      statistics Statistics information of stream    receivers A list of all receiver streams    receivers   stream id   Details of the given receiver stream    batches A list of all retained batches    batches   batch id   Details of the given batch    batches   batch id   operations A list of all output operations of the given batch    batches   batch id   operations   operation id   Details of the given operation  given batch Add a REST api to spark streaming","words":["trying","to","monitoring","our","streaming","application","using","spark","rest","interface","and","found","out","that","there","is","no","api","for","streaming","","it","let","us","no","choice","but","to","implement","one","for","ourself","","this","api","should","cover","exceptly","the","same","amount","of","information","as","you","can","get","from","the","web","interface","the","implementation","is","base","on","the","current","rest","implementation","of","spark","core","and","will","be","available","for","running","applications","only","here","is","how","you","can","use","it","","endpoint","root","","","streaming","api","v","","","","","endpoint","","","","meaning","","","","","","statistics","statistics","information","of","stream","","","","receivers","a","list","of","all","receiver","streams","","","","receivers","","","stream","id","","","details","of","the","given","receiver","stream","","","","batches","a","list","of","all","retained","batches","","","","batches","","","batch","id","","","details","of","the","given","batch","","","","batches","","","batch","id","","","operations","a","list","of","all","output","operations","of","the","given","batch","","","","batches","","","batch","id","","","operations","","","operation","id","","","details","of","the","given","operation","","given","batch","add","a","rest","api","to","spark","streaming"],"filtered":["trying","monitoring","streaming","application","using","spark","rest","interface","found","api","streaming","","let","us","choice","implement","one","ourself","","api","cover","exceptly","amount","information","get","web","interface","implementation","base","current","rest","implementation","spark","core","available","running","applications","use","","endpoint","root","","","streaming","api","v","","","","","endpoint","","","","meaning","","","","","","statistics","statistics","information","stream","","","","receivers","list","receiver","streams","","","","receivers","","","stream","id","","","details","given","receiver","stream","","","","batches","list","retained","batches","","","","batches","","","batch","id","","","details","given","batch","","","","batches","","","batch","id","","","operations","list","output","operations","given","batch","","","","batches","","","batch","id","","","operations","","","operation","id","","","details","given","operation","","given","batch","add","rest","api","spark","streaming"],"features":{"type":0,"size":1000,"indices":[36,44,82,83,88,103,105,112,122,128,135,142,164,169,170,173,178,188,208,213,218,228,237,252,263,264,281,296,333,343,345,346,371,372,373,388,420,425,430,432,472,477,489,495,504,537,556,572,594,624,644,654,656,665,683,698,704,708,710,728,735,742,760,764,781,801,820,831,833,899,921,955,959,963,968,978],"values":[3.0,1.0,1.0,1.0,1.0,6.0,3.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,5.0,4.0,5.0,3.0,2.0,2.0,10.0,1.0,2.0,1.0,56.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,9.0,3.0,3.0,1.0,1.0,1.0,5.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0]},"cluster_label":7}
{"_c0":"typeId is not needed in columnar cache  it s confusing to having them","_c1":"Remove typeId in columnar cache","document":"typeId is not needed in columnar cache  it s confusing to having them Remove typeId in columnar cache","words":["typeid","is","not","needed","in","columnar","cache","","it","s","confusing","to","having","them","remove","typeid","in","columnar","cache"],"filtered":["typeid","needed","columnar","cache","","confusing","remove","typeid","columnar","cache"],"features":{"type":0,"size":1000,"indices":[18,163,197,244,250,281,288,372,388,445,495,659,675,728,924],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"we have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation","_c1":"test patch s issue matching regex should be configurable","document":"we have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation test patch s issue matching regex should be configurable","words":["we","have","the","issue","matching","regex","configurable","via","altering","the","default","","add","in","a","cli","arg","to","update","it","on","invocation","test","patch","s","issue","matching","regex","should","be","configurable"],"filtered":["issue","matching","regex","configurable","via","altering","default","","add","cli","arg","update","invocation","test","patch","issue","matching","regex","configurable"],"features":{"type":0,"size":1000,"indices":[82,94,137,170,197,241,299,343,372,381,388,432,445,495,500,510,586,595,656,665,710,734,748,897,964,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
