{"_c1":"Add R API for stddev variance","document":"Add R API for stddev variance","words":["add","r","api","for","stddev","variance"],"filtered":["add","r","api","stddev","variance"],"features":{"type":0,"size":1000,"indices":[36,113,432,570,644,939],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Break SQLQuerySuite out into smaller test suites","document":"Break SQLQuerySuite out into smaller test suites","words":["break","sqlquerysuite","out","into","smaller","test","suites"],"filtered":["break","sqlquerysuite","smaller","test","suites"],"features":{"type":0,"size":1000,"indices":[301,586,648,654,891,931,974],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Deprecate metrics v","document":"Deprecate metrics v","words":["deprecate","metrics","v"],"filtered":["deprecate","metrics","v"],"features":{"type":0,"size":1000,"indices":[106,275,477],"values":[1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Execute multiple Python UDFs in single batch","document":"Execute multiple Python UDFs in single batch","words":["execute","multiple","python","udfs","in","single","batch"],"filtered":["execute","multiple","python","udfs","single","batch"],"features":{"type":0,"size":1000,"indices":[103,189,445,531,587,589,592],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Expose event time time stats through StreamingQueryProgress","document":"Expose event time time stats through StreamingQueryProgress","words":["expose","event","time","time","stats","through","streamingqueryprogress"],"filtered":["expose","event","time","time","stats","streamingqueryprogress"],"features":{"type":0,"size":1000,"indices":[109,157,371,543,642,691],"values":[1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Implement drop method for DataFrame in SparkR","document":"Implement drop method for DataFrame in SparkR","words":["implement","drop","method","for","dataframe","in","sparkr"],"filtered":["implement","drop","method","dataframe","sparkr"],"features":{"type":0,"size":1000,"indices":[36,161,445,472,577,654,767],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"QueryPlan expressions should always include all expressions","document":"QueryPlan expressions should always include all expressions","words":["queryplan","expressions","should","always","include","all","expressions"],"filtered":["queryplan","expressions","always","include","expressions"],"features":{"type":0,"size":1000,"indices":[13,29,401,665,863,968],"values":[1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c1":"Refactor R mllib for easier ml implementations","document":"Refactor R mllib for easier ml implementations","words":["refactor","r","mllib","for","easier","ml","implementations"],"filtered":["refactor","r","mllib","easier","ml","implementations"],"features":{"type":0,"size":1000,"indices":[36,324,474,521,570,623,925],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove OpenHashSet for the old aggregate","document":"Remove OpenHashSet for the old aggregate","words":["remove","openhashset","for","the","old","aggregate"],"filtered":["remove","openhashset","old","aggregate"],"features":{"type":0,"size":1000,"indices":[36,288,420,438,672,710],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove implicit conversion from Expression to Column","document":"Remove implicit conversion from Expression to Column","words":["remove","implicit","conversion","from","expression","to","column"],"filtered":["remove","implicit","conversion","expression","column"],"features":{"type":0,"size":1000,"indices":[288,388,420,577,601,804,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Remove redundant Experimental annotations in sql streaming package","document":"Remove redundant Experimental annotations in sql streaming package","words":["remove","redundant","experimental","annotations","in","sql","streaming","package"],"filtered":["remove","redundant","experimental","annotations","sql","streaming","package"],"features":{"type":0,"size":1000,"indices":[263,266,288,411,445,458,686,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Removed calling size  length in while condition to avoid extra JVM call","document":"Removed calling size  length in while condition to avoid extra JVM call","words":["removed","calling","size","","length","in","while","condition","to","avoid","extra","jvm","call"],"filtered":["removed","calling","size","","length","condition","avoid","extra","jvm","call"],"features":{"type":0,"size":1000,"indices":[8,109,146,192,300,372,388,389,445,512,583,707,813],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Rewrite sls rumen to use new shell framework","document":"Rewrite sls rumen to use new shell framework","words":["rewrite","sls","rumen","to","use","new","shell","framework"],"filtered":["rewrite","sls","rumen","use","new","shell","framework"],"features":{"type":0,"size":1000,"indices":[25,123,256,363,388,489,599,738],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support UnsafeRow in LocalTableScan","document":"Support UnsafeRow in LocalTableScan","words":["support","unsaferow","in","localtablescan"],"filtered":["support","unsaferow","localtablescan"],"features":{"type":0,"size":1000,"indices":[175,445,494,695],"values":[1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support UnsafeRow in MapPartitions MapGroups CoGroup","document":"Support UnsafeRow in MapPartitions MapGroups CoGroup","words":["support","unsaferow","in","mappartitions","mapgroups","cogroup"],"filtered":["support","unsaferow","mappartitions","mapgroups","cogroup"],"features":{"type":0,"size":1000,"indices":[155,175,445,553,695,997],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support intersect except in Hive SQL","document":"Support intersect except in Hive SQL","words":["support","intersect","except","in","hive","sql"],"filtered":["support","intersect","except","hive","sql"],"features":{"type":0,"size":1000,"indices":[179,445,513,599,686,695],"values":[1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"Support window functions in SQLContext","document":"Support window functions in SQLContext","words":["support","window","functions","in","sqlcontext"],"filtered":["support","window","functions","sqlcontext"],"features":{"type":0,"size":1000,"indices":[445,451,511,587,695],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"make SubqueryHolder an inner class","document":"make SubqueryHolder an inner class","words":["make","subqueryholder","an","inner","class"],"filtered":["make","subqueryholder","inner","class"],"features":{"type":0,"size":1000,"indices":[101,525,534,752,800],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"move hive hack for data source table into HiveExternalCatalog","document":"move hive hack for data source table into HiveExternalCatalog","words":["move","hive","hack","for","data","source","table","into","hiveexternalcatalog"],"filtered":["move","hive","hack","data","source","table","hiveexternalcatalog"],"features":{"type":0,"size":1000,"indices":[36,70,130,282,599,695,760,837,891],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"remove GenericInternalRowWithSchema","document":"remove GenericInternalRowWithSchema","words":["remove","genericinternalrowwithschema"],"filtered":["remove","genericinternalrowwithschema"],"features":{"type":0,"size":1000,"indices":[288,861],"values":[1.0,1.0]},"cluster_label":13}
{"_c1":"remove MaxOf and MinOf","document":"remove MaxOf and MinOf","words":["remove","maxof","and","minof"],"filtered":["remove","maxof","minof"],"features":{"type":0,"size":1000,"indices":[44,288,333,796],"values":[1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"remove OverwriteOptions","document":"remove OverwriteOptions","words":["remove","overwriteoptions"],"filtered":["remove","overwriteoptions"],"features":{"type":0,"size":1000,"indices":[210,288],"values":[1.0,1.0]},"cluster_label":13}
{"_c1":"remove catalog table type INDEX","document":"remove catalog table type INDEX","words":["remove","catalog","table","type","index"],"filtered":["remove","catalog","table","type","index"],"features":{"type":0,"size":1000,"indices":[288,307,510,526,837],"values":[1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"remove the createCode and createStructCode  and replace the usage of them by createStructCode","document":"remove the createCode and createStructCode  and replace the usage of them by createStructCode","words":["remove","the","createcode","and","createstructcode","","and","replace","the","usage","of","them","by","createstructcode"],"filtered":["remove","createcode","createstructcode","","replace","usage","createstructcode"],"features":{"type":0,"size":1000,"indices":[122,154,223,288,333,343,372,710,787,889,924],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c1":"remove trait Queryable","document":"remove trait Queryable","words":["remove","trait","queryable"],"filtered":["remove","trait","queryable"],"features":{"type":0,"size":1000,"indices":[288,821,852],"values":[1.0,1.0,1.0]},"cluster_label":13}
{"_c1":"use StructType in CatalogTable and remove CatalogColumn","document":"use StructType in CatalogTable and remove CatalogColumn","words":["use","structtype","in","catalogtable","and","remove","catalogcolumn"],"filtered":["use","structtype","catalogtable","remove","catalogcolumn"],"features":{"type":0,"size":1000,"indices":[22,288,333,375,376,445,489],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"A user was moving from      to      and was invoking randomwriter with a config on the command line like  bin hadoop jar hadoop   examples jar randomwriter output conf xml which worked in       but in      it ignores the conf xml without complaining  The equivalent is bin hadoop jar hadoop   examples jar randomwriter  conf conf xml output","_c1":"randomwriter should complain if there are too many arguments","document":"A user was moving from      to      and was invoking randomwriter with a config on the command line like  bin hadoop jar hadoop   examples jar randomwriter output conf xml which worked in       but in      it ignores the conf xml without complaining  The equivalent is bin hadoop jar hadoop   examples jar randomwriter  conf conf xml output randomwriter should complain if there are too many arguments","words":["a","user","was","moving","from","","","","","","to","","","","","","and","was","invoking","randomwriter","with","a","config","on","the","command","line","like","","bin","hadoop","jar","hadoop","","","examples","jar","randomwriter","output","conf","xml","which","worked","in","","","","","","","but","in","","","","","","it","ignores","the","conf","xml","without","complaining","","the","equivalent","is","bin","hadoop","jar","hadoop","","","examples","jar","randomwriter","","conf","conf","xml","output","randomwriter","should","complain","if","there","are","too","many","arguments"],"filtered":["user","moving","","","","","","","","","","","invoking","randomwriter","config","command","line","like","","bin","hadoop","jar","hadoop","","","examples","jar","randomwriter","output","conf","xml","worked","","","","","","","","","","","","ignores","conf","xml","without","complaining","","equivalent","bin","hadoop","jar","hadoop","","","examples","jar","randomwriter","","conf","conf","xml","output","randomwriter","complain","many","arguments"],"features":{"type":0,"size":1000,"indices":[20,82,83,92,122,135,138,170,181,182,188,234,237,281,330,333,350,352,372,388,445,495,513,533,597,603,644,647,650,665,709,710,737,800,801,831,882,884,921,949,991],"values":[1.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,4.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,28.0,1.0,2.0,1.0,1.0,4.0,1.0,4.0,1.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"According to the discussion in HADOOP       we should remove   io native lib available   from trunk  and always use native libraries if they exist","_c1":"Remove io native lib available","document":"According to the discussion in HADOOP       we should remove   io native lib available   from trunk  and always use native libraries if they exist Remove io native lib available","words":["according","to","the","discussion","in","hadoop","","","","","","","we","should","remove","","","io","native","lib","available","","","from","trunk","","and","always","use","native","libraries","if","they","exist","remove","io","native","lib","available"],"filtered":["according","discussion","hadoop","","","","","","","remove","","","io","native","lib","available","","","trunk","","always","use","native","libraries","exist","remove","io","native","lib","available"],"features":{"type":0,"size":1000,"indices":[11,13,48,83,144,170,181,187,288,333,371,372,388,445,489,566,665,695,710,838,849,921,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,11.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Add Java friendly API for StreamingListener","_c1":"Add JavaStreamingListener","document":"Add Java friendly API for StreamingListener Add JavaStreamingListener","words":["add","java","friendly","api","for","streaminglistener","add","javastreaminglistener"],"filtered":["add","java","friendly","api","streaminglistener","add","javastreaminglistener"],"features":{"type":0,"size":1000,"indices":[36,325,432,468,644,709,967],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Add Python API  user guide and example for ml feature CountVectorizerModel","_c1":"Add Python API for ml feature CountVectorizer","document":"Add Python API  user guide and example for ml feature CountVectorizerModel Add Python API for ml feature CountVectorizer","words":["add","python","api","","user","guide","and","example","for","ml","feature","countvectorizermodel","add","python","api","for","ml","feature","countvectorizer"],"filtered":["add","python","api","","user","guide","example","ml","feature","countvectorizermodel","add","python","api","ml","feature","countvectorizer"],"features":{"type":0,"size":1000,"indices":[36,241,243,318,324,333,372,432,589,644,675,736,882],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Add a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise    h   Example If   df x       is TRUE  then return    otherwise return","_c1":"Add  ifelse  Column function to SparkR","document":"Add a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise    h   Example If   df x       is TRUE  then return    otherwise return Add  ifelse  Column function to SparkR","words":["add","a","column","function","on","a","dataframe","like","","ifelse","","in","r","to","sparkr","","i","guess","we","could","implement","it","with","a","combination","with","","","when","","","and","","","otherwise","","","","h","","","example","if","","","df","x","","","","","","","is","true","","then","return","","","","otherwise","return","add","","ifelse","","column","function","to","sparkr"],"filtered":["add","column","function","dataframe","like","","ifelse","","r","sparkr","","guess","implement","combination","","","","","","","otherwise","","","","h","","","example","","","df","x","","","","","","","true","","return","","","","otherwise","return","add","","ifelse","","column","function","sparkr"],"features":{"type":0,"size":1000,"indices":[14,76,82,118,145,161,170,188,213,243,281,282,286,313,329,330,333,372,381,388,432,445,472,495,570,597,601,650,767,810,875,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,28.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":1}
{"_c0":"Add a new API method called gapplyCollect   for SparkDataFrame  It does gapply on a SparkDataFrame and collect the result back to R  Compared to gapply     collect    gapplyCollect   offers performance optimization as well as programming convenience  as no schema is needed to be provided  This is similar to dapplyCollect","_c1":"add gapplyCollect   for SparkDataFrame","document":"Add a new API method called gapplyCollect   for SparkDataFrame  It does gapply on a SparkDataFrame and collect the result back to R  Compared to gapply     collect    gapplyCollect   offers performance optimization as well as programming convenience  as no schema is needed to be provided  This is similar to dapplyCollect add gapplyCollect   for SparkDataFrame","words":["add","a","new","api","method","called","gapplycollect","","","for","sparkdataframe","","it","does","gapply","on","a","sparkdataframe","and","collect","the","result","back","to","r","","compared","to","gapply","","","","","collect","","","","gapplycollect","","","offers","performance","optimization","as","well","as","programming","convenience","","as","no","schema","is","needed","to","be","provided","","this","is","similar","to","dapplycollect","add","gapplycollect","","","for","sparkdataframe"],"filtered":["add","new","api","method","called","gapplycollect","","","sparkdataframe","","gapply","sparkdataframe","collect","result","back","r","","compared","gapply","","","","","collect","","","","gapplycollect","","","offers","performance","optimization","well","programming","convenience","","schema","needed","provided","","similar","dapplycollect","add","gapplycollect","","","sparkdataframe"],"features":{"type":0,"size":1000,"indices":[25,36,45,50,82,157,170,202,207,244,281,333,346,372,373,388,394,398,430,432,441,495,565,570,572,594,642,644,654,656,698,710,731,754,759,783,865,910],"values":[1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,17.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Add feature interaction as a transformer  which takes a list of vector double columns  and generate a single vector column that contains the interactions  multiplication  among them with proper handling of feature names","_c1":"Add feature interaction as a transformer","document":"Add feature interaction as a transformer  which takes a list of vector double columns  and generate a single vector column that contains the interactions  multiplication  among them with proper handling of feature names Add feature interaction as a transformer","words":["add","feature","interaction","as","a","transformer","","which","takes","a","list","of","vector","double","columns","","and","generate","a","single","vector","column","that","contains","the","interactions","","multiplication","","among","them","with","proper","handling","of","feature","names","add","feature","interaction","as","a","transformer"],"filtered":["add","feature","interaction","transformer","","takes","list","vector","double","columns","","generate","single","vector","column","contains","interactions","","multiplication","","among","proper","handling","feature","names","add","feature","interaction","transformer"],"features":{"type":0,"size":1000,"indices":[48,100,170,200,254,276,333,343,370,372,405,432,531,572,597,601,614,650,688,710,728,736,742,760,825,830,922,924,969],"values":[1.0,1.0,4.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Adding an option to   FsShell stat   to get a file s block location information will be very useful  we can print the block location information in this format  blockID XXXXX byte range YYYY ZZZZ location dn  dn   blockID XXXXX byte range YYYY ZZZZ location dn  dn","_c1":"Add a command to   FsShell stat   to get a file s block location information","document":"Adding an option to   FsShell stat   to get a file s block location information will be very useful  we can print the block location information in this format  blockID XXXXX byte range YYYY ZZZZ location dn  dn   blockID XXXXX byte range YYYY ZZZZ location dn  dn Add a command to   FsShell stat   to get a file s block location information","words":["adding","an","option","to","","","fsshell","stat","","","to","get","a","file","s","block","location","information","will","be","very","useful","","we","can","print","the","block","location","information","in","this","format","","blockid","xxxxx","byte","range","yyyy","zzzz","location","dn","","dn","","","blockid","xxxxx","byte","range","yyyy","zzzz","location","dn","","dn","add","a","command","to","","","fsshell","stat","","","to","get","a","file","s","block","location","information"],"filtered":["adding","option","","","fsshell","stat","","","get","file","block","location","information","useful","","print","block","location","information","format","","blockid","xxxxx","byte","range","yyyy","zzzz","location","dn","","dn","","","blockid","xxxxx","byte","range","yyyy","zzzz","location","dn","","dn","add","command","","","fsshell","stat","","","get","file","block","location","information"],"features":{"type":0,"size":1000,"indices":[95,108,135,168,170,197,222,231,272,306,372,373,388,420,432,445,459,511,514,609,656,710,741,745,752,766,833,845,886,944,959,978,993],"values":[2.0,2.0,1.0,2.0,3.0,2.0,2.0,1.0,1.0,2.0,14.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,4.0,2.0,1.0,1.0,5.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0]},"cluster_label":17}
{"_c0":"After DistCp copies a file  it calls   getFileStatus   to get the   FileStatus   from the destination so that it can compare to the source and update metadata if necessary  If the DistCp command was run without the option to preserve metadata attributes  then this additional   getFileStatus   call is wasteful","_c1":"In DistCp  prevent unnecessary getFileStatus call when not preserving metadata","document":"After DistCp copies a file  it calls   getFileStatus   to get the   FileStatus   from the destination so that it can compare to the source and update metadata if necessary  If the DistCp command was run without the option to preserve metadata attributes  then this additional   getFileStatus   call is wasteful In DistCp  prevent unnecessary getFileStatus call when not preserving metadata","words":["after","distcp","copies","a","file","","it","calls","","","getfilestatus","","","to","get","the","","","filestatus","","","from","the","destination","so","that","it","can","compare","to","the","source","and","update","metadata","if","necessary","","if","the","distcp","command","was","run","without","the","option","to","preserve","metadata","attributes","","then","this","additional","","","getfilestatus","","","call","is","wasteful","in","distcp","","prevent","unnecessary","getfilestatus","call","when","not","preserving","metadata"],"filtered":["distcp","copies","file","","calls","","","getfilestatus","","","get","","","filestatus","","","destination","compare","source","update","metadata","necessary","","distcp","command","run","without","option","preserve","metadata","attributes","","additional","","","getfilestatus","","","call","wasteful","distcp","","prevent","unnecessary","getfilestatus","call","preserving","metadata"],"features":{"type":0,"size":1000,"indices":[17,18,64,70,76,77,108,135,146,170,183,222,234,242,281,300,333,343,364,368,372,373,381,388,399,403,422,445,494,495,531,571,609,697,710,760,833,866,884,909,921,959,960],"values":[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,16.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"After SPARK        we should add Python API for MaxAbsScaler","_c1":"Python API for MaxAbsScaler","document":"After SPARK        we should add Python API for MaxAbsScaler Python API for MaxAbsScaler","words":["after","spark","","","","","","","","we","should","add","python","api","for","maxabsscaler","python","api","for","maxabsscaler"],"filtered":["spark","","","","","","","","add","python","api","maxabsscaler","python","api","maxabsscaler"],"features":{"type":0,"size":1000,"indices":[36,77,105,372,432,589,644,663,665,993],"values":[2.0,1.0,1.0,7.0,1.0,2.0,2.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"After SPARK        we should add Python API for generalized linear regression","_c1":"Python API for GeneralizedLinearRegression","document":"After SPARK        we should add Python API for generalized linear regression Python API for GeneralizedLinearRegression","words":["after","spark","","","","","","","","we","should","add","python","api","for","generalized","linear","regression","python","api","for","generalizedlinearregression"],"filtered":["spark","","","","","","","","add","python","api","generalized","linear","regression","python","api","generalizedlinearregression"],"features":{"type":0,"size":1000,"indices":[36,77,105,316,329,372,432,589,644,665,695,984,993],"values":[2.0,1.0,1.0,1.0,1.0,7.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"After acquiring allocations from YARN and launching containers  Spark currently waits for   seconds for executors to connect to the driver  On Spark standalone  nothing like this happens  I m wondering whether we can just remove this sleep entirely  Is there a reason I m missing why YARN is different than standalone in this regard  At the least we could do something smarter like wait until all executors have registered","_c1":"Remove   second sleep before starting app on YARN","document":"After acquiring allocations from YARN and launching containers  Spark currently waits for   seconds for executors to connect to the driver  On Spark standalone  nothing like this happens  I m wondering whether we can just remove this sleep entirely  Is there a reason I m missing why YARN is different than standalone in this regard  At the least we could do something smarter like wait until all executors have registered Remove   second sleep before starting app on YARN","words":["after","acquiring","allocations","from","yarn","and","launching","containers","","spark","currently","waits","for","","","seconds","for","executors","to","connect","to","the","driver","","on","spark","standalone","","nothing","like","this","happens","","i","m","wondering","whether","we","can","just","remove","this","sleep","entirely","","is","there","a","reason","i","m","missing","why","yarn","is","different","than","standalone","in","this","regard","","at","the","least","we","could","do","something","smarter","like","wait","until","all","executors","have","registered","remove","","","second","sleep","before","starting","app","on","yarn"],"filtered":["acquiring","allocations","yarn","launching","containers","","spark","currently","waits","","","seconds","executors","connect","driver","","spark","standalone","","nothing","like","happens","","m","wondering","whether","remove","sleep","entirely","","reason","m","missing","yarn","different","standalone","regard","","least","something","smarter","like","wait","executors","registered","remove","","","second","sleep","starting","app","yarn"],"features":{"type":0,"size":1000,"indices":[36,77,82,84,89,94,105,135,143,155,159,170,213,261,268,281,288,299,307,325,329,330,333,353,372,373,388,413,436,445,477,514,520,525,534,547,553,554,564,609,616,638,667,710,745,756,757,763,807,831,833,866,897,905,921,935,961,968,973,993],"values":[2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,10.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand","_c1":"AggregateFunction should not ImplicitCastInputTypes","document":"AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand AggregateFunction should not ImplicitCastInputTypes","words":["aggregatefunction","","","currently","implements","","","implicitcastinputtypes","","","","which","enables","implicit","input","type","casting","","","this","can","lead","to","unexpected","results","","and","should","only","be","enabled","when","it","is","suitable","for","the","function","at","hand","aggregatefunction","should","not","implicitcastinputtypes"],"filtered":["aggregatefunction","","","currently","implements","","","implicitcastinputtypes","","","","enables","implicit","input","type","casting","","","lead","unexpected","results","","enabled","suitable","function","hand","aggregatefunction","implicitcastinputtypes"],"features":{"type":0,"size":1000,"indices":[0,18,36,52,76,145,281,313,333,356,363,372,373,388,390,423,455,487,495,526,577,597,656,665,710,756,763,800,833,844,889,899],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,10.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Aliyun OSS is widely used among China s cloud users  but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application  because of no original support for OSS in Hadoop  This work aims to integrate Aliyun OSS with Hadoop  By simple configuration  Spark Hadoop applications can read write data from OSS without any code change  Narrowing the gap between user s APP and data storage  like what have been done for S  in Hadoop","_c1":"Incorporate Aliyun OSS file system implementation","document":"Aliyun OSS is widely used among China s cloud users  but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application  because of no original support for OSS in Hadoop  This work aims to integrate Aliyun OSS with Hadoop  By simple configuration  Spark Hadoop applications can read write data from OSS without any code change  Narrowing the gap between user s APP and data storage  like what have been done for S  in Hadoop Incorporate Aliyun OSS file system implementation","words":["aliyun","oss","is","widely","used","among","china","s","cloud","users","","but","currently","it","is","not","easy","to","access","data","laid","on","oss","storage","from","user","s","hadoop","spark","application","","because","of","no","original","support","for","oss","in","hadoop","","this","work","aims","to","integrate","aliyun","oss","with","hadoop","","by","simple","configuration","","spark","hadoop","applications","can","read","write","data","from","oss","without","any","code","change","","narrowing","the","gap","between","user","s","app","and","data","storage","","like","what","have","been","done","for","s","","in","hadoop","incorporate","aliyun","oss","file","system","implementation"],"filtered":["aliyun","oss","widely","used","among","china","cloud","users","","currently","easy","access","data","laid","oss","storage","user","hadoop","spark","application","","original","support","oss","hadoop","","work","aims","integrate","aliyun","oss","hadoop","","simple","configuration","","spark","hadoop","applications","read","write","data","oss","without","code","change","","narrowing","gap","user","app","data","storage","","like","done","","hadoop","incorporate","aliyun","oss","file","system","implementation"],"features":{"type":0,"size":1000,"indices":[18,26,36,78,82,83,91,94,105,108,113,158,164,169,181,197,223,224,281,299,330,333,343,346,369,372,373,388,394,405,420,421,445,453,481,495,526,527,530,535,605,639,650,682,691,695,698,707,710,742,755,760,763,833,844,882,884,901,921,922,980],"values":[1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,5.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,8.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Along the same vein as HADOOP        there are now several remaining usages of guava APIs that are now incompatible with a more recent version  e g       This JIRA proposes eliminating those usages  With this  the hadoop base compatible with guava","_c1":"Remove some uses of obsolete guava APIs from the hadoop codebase","document":"Along the same vein as HADOOP        there are now several remaining usages of guava APIs that are now incompatible with a more recent version  e g       This JIRA proposes eliminating those usages  With this  the hadoop base compatible with guava Remove some uses of obsolete guava APIs from the hadoop codebase","words":["along","the","same","vein","as","hadoop","","","","","","","","there","are","now","several","remaining","usages","of","guava","apis","that","are","now","incompatible","with","a","more","recent","version","","e","g","","","","","","","this","jira","proposes","eliminating","those","usages","","with","this","","the","hadoop","base","compatible","with","guava","remove","some","uses","of","obsolete","guava","apis","from","the","hadoop","codebase"],"filtered":["along","vein","hadoop","","","","","","","","several","remaining","usages","guava","apis","incompatible","recent","version","","e","g","","","","","","","jira","proposes","eliminating","usages","","","hadoop","base","compatible","guava","remove","uses","obsolete","guava","apis","hadoop","codebase"],"features":{"type":0,"size":1000,"indices":[98,111,127,138,146,170,173,181,208,288,329,343,372,373,400,403,417,435,476,522,547,558,572,600,629,650,656,710,747,760,803,821,831,842,878,921,995],"values":[2.0,1.0,2.0,2.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,16.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Amongst other things  JUnit   has better support for class wide set up and tear down  via  BeforeClass and  AfterClass annotations   and more flexible assertions  http   junit sourceforge net doc ReleaseNotes    html   It would be nice to be able to take advantage of these features in tests we write  JUnit   can run tests written for JUnit       without any changes","_c1":"Upgrade to JUnit","document":"Amongst other things  JUnit   has better support for class wide set up and tear down  via  BeforeClass and  AfterClass annotations   and more flexible assertions  http   junit sourceforge net doc ReleaseNotes    html   It would be nice to be able to take advantage of these features in tests we write  JUnit   can run tests written for JUnit       without any changes Upgrade to JUnit","words":["amongst","other","things","","junit","","","has","better","support","for","class","wide","set","up","and","tear","down","","via","","beforeclass","and","","afterclass","annotations","","","and","more","flexible","assertions","","http","","","junit","sourceforge","net","doc","releasenotes","","","","html","","","it","would","be","nice","to","be","able","to","take","advantage","of","these","features","in","tests","we","write","","junit","","","can","run","tests","written","for","junit","","","","","","","without","any","changes","upgrade","to","junit"],"filtered":["amongst","things","","junit","","","better","support","class","wide","set","tear","","via","","beforeclass","","afterclass","annotations","","","flexible","assertions","","http","","","junit","sourceforge","net","doc","releasenotes","","","","html","","","nice","able","take","advantage","features","tests","write","","junit","","","run","tests","written","junit","","","","","","","without","changes","upgrade","junit"],"features":{"type":0,"size":1000,"indices":[36,68,91,113,128,163,177,217,242,333,343,363,364,370,372,388,434,445,461,477,483,495,496,534,580,595,607,619,629,652,656,665,674,681,695,755,795,813,815,833,836,855,884,888,904,915,941,968,993,998],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,25.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Apache Parquet       is released officially last week on    Jan  This issue aims to bump Parquet version to       since it includes many fixes  https   lists apache org thread html af c   f          a   d  ec  b bbeecaea  aa ef  f   c      Cdev parquet apache org  E","_c1":"Upgrade Parquet to","document":"Apache Parquet       is released officially last week on    Jan  This issue aims to bump Parquet version to       since it includes many fixes  https   lists apache org thread html af c   f          a   d  ec  b bbeecaea  aa ef  f   c      Cdev parquet apache org  E Upgrade Parquet to","words":["apache","parquet","","","","","","","is","released","officially","last","week","on","","","","jan","","this","issue","aims","to","bump","parquet","version","to","","","","","","","since","it","includes","many","fixes","","https","","","lists","apache","org","thread","html","af","c","","","f","","","","","","","","","","a","","","d","","ec","","b","bbeecaea","","aa","ef","","f","","","c","","","","","","cdev","parquet","apache","org","","e","upgrade","parquet","to"],"filtered":["apache","parquet","","","","","","","released","officially","last","week","","","","jan","","issue","aims","bump","parquet","version","","","","","","","since","includes","many","fixes","","https","","","lists","apache","org","thread","html","af","c","","","f","","","","","","","","","","","","d","","ec","","b","bbeecaea","","aa","ef","","f","","","c","","","","","","cdev","parquet","apache","org","","e","upgrade","parquet"],"features":{"type":0,"size":1000,"indices":[82,94,122,124,166,170,172,188,189,240,242,248,281,352,361,372,373,388,399,444,495,498,535,585,634,643,652,672,722,748,805,844,852,878,895,989,995,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,44.0,1.0,3.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":5}
{"_c0":"As Hadoop widespreads and matures the number of tools and utilities for users keeps growing  Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie  Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other  The lack of a common foundation creates issues for developers and users","_c1":"Common foundation for Hadoop client tools","document":"As Hadoop widespreads and matures the number of tools and utilities for users keeps growing  Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie  Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other  The lack of a common foundation creates issues for developers and users Common foundation for Hadoop client tools","words":["as","hadoop","widespreads","and","matures","the","number","of","tools","and","utilities","for","users","keeps","growing","","some","of","them","are","bundled","with","hadoop","core","","some","with","hadoop","contrib","","some","on","their","own","","some","are","full","fledged","servers","on","their","own","","for","example","","just","to","name","a","few","","distcp","","streaming","","pipes","","har","","pig","","hive","","oozie","","today","there","is","no","standard","mechanism","for","making","these","tools","available","to","users","","neither","there","is","a","standard","mechanism","for","these","tools","to","integrate","and","distributed","them","with","each","other","","the","lack","of","a","common","foundation","creates","issues","for","developers","and","users","common","foundation","for","hadoop","client","tools"],"filtered":["hadoop","widespreads","matures","number","tools","utilities","users","keeps","growing","","bundled","hadoop","core","","hadoop","contrib","","","full","fledged","servers","","example","","name","","distcp","","streaming","","pipes","","har","","pig","","hive","","oozie","","today","standard","mechanism","making","tools","available","users","","neither","standard","mechanism","tools","integrate","distributed","","lack","common","foundation","creates","issues","developers","users","common","foundation","hadoop","client","tools"],"features":{"type":0,"size":1000,"indices":[15,26,36,58,82,116,118,135,138,170,181,228,235,243,263,281,307,315,333,343,346,347,352,371,372,388,400,409,427,461,462,475,487,494,513,521,572,583,599,608,650,653,661,674,706,710,755,761,769,801,831,885,897,922,924,945,954,967,970,996],"values":[1.0,1.0,6.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,4.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,16.0,3.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,4.0,1.0]},"cluster_label":11}
{"_c0":"As a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults  To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review","_c1":"Use reference counting to prevent blocks from being evicted during reads","document":"As a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults  To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review Use reference counting to prevent blocks from being evicted during reads","words":["as","a","pre","requisite","to","off","heap","caching","of","blocks","","we","need","a","mechanism","to","prevent","pages","","","blocks","from","being","evicted","while","they","are","being","read","","with","on","heap","objects","","evicting","a","block","while","it","is","being","read","merely","leads","to","memory","accounting","problems","","because","we","assume","that","an","evicted","block","is","a","candidate","for","garbage","collection","","which","will","not","be","true","during","a","read","","","but","with","off","heap","memory","this","will","lead","to","either","data","corruption","or","segmentation","faults","","to","address","this","","we","should","add","a","reference","counting","mechanism","to","track","which","blocks","pages","are","being","read","in","order","to","prevent","them","from","being","evicted","prematurely","","i","propose","to","do","this","in","two","phases","","first","","add","a","safe","","conservative","approach","in","which","all","blockmanager","get","","","","calls","implicitly","increment","the","reference","count","of","blocks","and","where","tasks","","references","are","automatically","freed","upon","task","completion","","this","will","be","correct","but","may","have","adverse","performance","impacts","because","it","will","prevent","legitimate","block","evictions","","in","phase","two","","we","should","incrementally","add","release","","","calls","in","order","to","fix","the","eviction","of","unreferenced","blocks","","the","latter","change","may","need","to","touch","many","different","components","","which","is","why","i","propose","to","do","it","separately","in","order","to","make","the","changes","easier","to","reason","about","and","review","use","reference","counting","to","prevent","blocks","from","being","evicted","during","reads"],"filtered":["pre","requisite","heap","caching","blocks","","need","mechanism","prevent","pages","","","blocks","evicted","read","","heap","objects","","evicting","block","read","merely","leads","memory","accounting","problems","","assume","evicted","block","candidate","garbage","collection","","true","read","","","heap","memory","lead","either","data","corruption","segmentation","faults","","address","","add","reference","counting","mechanism","track","blocks","pages","read","order","prevent","evicted","prematurely","","propose","two","phases","","first","","add","safe","","conservative","approach","blockmanager","get","","","","calls","implicitly","increment","reference","count","blocks","tasks","","references","automatically","freed","upon","task","completion","","correct","may","adverse","performance","impacts","prevent","legitimate","block","evictions","","phase","two","","incrementally","add","release","","","calls","order","fix","eviction","unreferenced","blocks","","latter","change","may","need","touch","many","different","components","","propose","separately","order","make","changes","easier","reason","review","use","reference","counting","prevent","blocks","evicted","reads"],"features":{"type":0,"size":1000,"indices":[8,18,20,30,36,48,78,82,83,89,92,95,116,125,138,139,158,170,183,187,188,211,230,236,251,268,272,277,281,294,299,303,329,330,333,336,343,344,356,359,363,371,372,373,374,388,393,399,403,408,420,421,432,445,451,452,474,489,493,494,495,497,505,511,520,525,528,534,537,551,572,590,594,597,604,621,622,642,650,656,664,665,666,689,693,695,707,709,710,718,735,739,752,759,760,777,788,813,866,870,908,914,921,924,936,945,959,963,968,973,993,996],"values":[4.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,7.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,26.0,6.0,6.0,14.0,1.0,1.0,4.0,3.0,4.0,2.0,3.0,7.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,6.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,3.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,4.0,1.0]},"cluster_label":1}
{"_c0":"As discussed with   aw    In AVRO      a docker based solution was created to setup all the tools for doing a full build  This enables much easier reproduction of any issues and getting up and running for new developers  This issue is to  copy port  that setup into the hadoop project in preparation for the bug squash","_c1":"Make setting up the build environment easier","document":"As discussed with   aw    In AVRO      a docker based solution was created to setup all the tools for doing a full build  This enables much easier reproduction of any issues and getting up and running for new developers  This issue is to  copy port  that setup into the hadoop project in preparation for the bug squash Make setting up the build environment easier","words":["as","discussed","with","","","aw","","","","in","avro","","","","","","a","docker","based","solution","was","created","to","setup","all","the","tools","for","doing","a","full","build","","this","enables","much","easier","reproduction","of","any","issues","and","getting","up","and","running","for","new","developers","","this","issue","is","to","","copy","port","","that","setup","into","the","hadoop","project","in","preparation","for","the","bug","squash","make","setting","up","the","build","environment","easier"],"filtered":["discussed","","","aw","","","","avro","","","","","","docker","based","solution","created","setup","tools","full","build","","enables","much","easier","reproduction","issues","getting","running","new","developers","","issue","","copy","port","","setup","hadoop","project","preparation","bug","squash","make","setting","build","environment","easier"],"features":{"type":0,"size":1000,"indices":[25,36,40,91,100,128,132,145,170,181,216,234,249,262,281,315,333,337,343,372,373,388,404,445,466,474,475,524,525,536,572,625,650,659,671,710,726,748,760,781,823,891,897,901,920,958,963,968,970],"values":[1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,14.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"As hadoop tools grows bigger and bigger  it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows  Let s rework this to be smarter","_c1":"Rework hadoop tools","document":"As hadoop tools grows bigger and bigger  it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows  Let s rework this to be smarter Rework hadoop tools","words":["as","hadoop","tools","grows","bigger","and","bigger","","it","s","becoming","evident","that","having","a","single","directory","that","gets","sucked","in","is","starting","to","become","a","big","burden","as","the","number","of","tools","grows","","let","s","rework","this","to","be","smarter","rework","hadoop","tools"],"filtered":["hadoop","tools","grows","bigger","bigger","","becoming","evident","single","directory","gets","sucked","starting","become","big","burden","number","tools","grows","","let","rework","smarter","rework","hadoop","tools"],"features":{"type":0,"size":1000,"indices":[13,164,170,181,197,249,281,318,333,343,372,373,388,445,465,495,520,531,572,579,583,598,616,656,675,688,702,710,760,871,900,970,992],"values":[1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0]},"cluster_label":0}
{"_c0":"As of Spark      Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API","_c1":"Native database table system catalog","document":"As of Spark      Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API Native database table system catalog","words":["as","of","spark","","","","","","spark","sql","internally","has","only","a","limited","catalog","","and","does","not","support","any","of","the","ddls","","this","is","an","umbrella","ticket","to","introduce","an","internal","api","for","a","system","catalog","","and","the","associated","ddl","implementations","using","this","api","native","database","table","system","catalog"],"filtered":["spark","","","","","","spark","sql","internally","limited","catalog","","support","ddls","","umbrella","ticket","introduce","internal","api","system","catalog","","associated","ddl","implementations","using","api","native","database","table","system","catalog"],"features":{"type":0,"size":1000,"indices":[18,36,91,105,160,170,187,255,281,291,295,333,343,362,372,373,388,443,495,510,572,580,624,639,641,644,686,695,698,710,752,837,858,899,925],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,8.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"As part of HADOOP       java execution across many different shell bits were consolidated down to  effectively  two routines  Prior to calling those two routines  the CLASSPATH is exported  This export should really be getting handled in the exec function and not in the individual shell bits  Additionally  it would be good if there was     so that bash  x would show the content of the classpath or even a    debug classpath  option that would echo the classpath to the screen prior to java exec to help with debugging","_c1":"CLASSPATH handling should be consolidated  debuggable","document":"As part of HADOOP       java execution across many different shell bits were consolidated down to  effectively  two routines  Prior to calling those two routines  the CLASSPATH is exported  This export should really be getting handled in the exec function and not in the individual shell bits  Additionally  it would be good if there was     so that bash  x would show the content of the classpath or even a    debug classpath  option that would echo the classpath to the screen prior to java exec to help with debugging CLASSPATH handling should be consolidated  debuggable","words":["as","part","of","hadoop","","","","","","","java","execution","across","many","different","shell","bits","were","consolidated","down","to","","effectively","","two","routines","","prior","to","calling","those","two","routines","","the","classpath","is","exported","","this","export","should","really","be","getting","handled","in","the","exec","function","and","not","in","the","individual","shell","bits","","additionally","","it","would","be","good","if","there","was","","","","","so","that","bash","","x","would","show","the","content","of","the","classpath","or","even","a","","","","debug","classpath","","option","that","would","echo","the","classpath","to","the","screen","prior","to","java","exec","to","help","with","debugging","classpath","handling","should","be","consolidated","","debuggable"],"filtered":["part","hadoop","","","","","","","java","execution","across","many","different","shell","bits","consolidated","","effectively","","two","routines","","prior","calling","two","routines","","classpath","exported","","export","really","getting","handled","exec","function","individual","shell","bits","","additionally","","good","","","","","bash","","x","show","content","classpath","even","","","","debug","classpath","","option","echo","classpath","screen","prior","java","exec","help","debugging","classpath","handling","consolidated","","debuggable"],"features":{"type":0,"size":1000,"indices":[18,19,34,68,89,120,123,132,146,163,168,170,181,187,188,217,222,234,243,281,284,310,313,333,343,344,347,367,368,372,373,388,389,406,408,445,453,476,495,572,583,591,600,650,656,661,665,682,710,738,740,760,771,810,811,825,831,855,883,920,962,967],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,23.0,1.0,5.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,5.0,7.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":11}
{"_c0":"As part of the work to implement SPARK        it would be nice to have the network library efficiently stream data over a connection  Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately  Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug","_c1":"Support streaming data using network library","document":"As part of the work to implement SPARK        it would be nice to have the network library efficiently stream data over a connection  Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately  Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug Support streaming data using network library","words":["as","part","of","the","work","to","implement","spark","","","","","","","","it","would","be","nice","to","have","the","network","library","efficiently","stream","data","over","a","connection","","currently","all","it","has","is","the","shuffle","data","protocol","","which","is","not","very","efficient","for","large","files","","it","requires","the","whole","file","to","be","buffered","on","the","receiver","side","before","the","receiver","can","do","anything","","for","large","files","","that","comes","at","a","huge","cost","in","memory","","you","can","chunk","large","files","but","that","requires","the","client","to","ask","for","each","chunk","separately","","instead","","a","similar","approach","but","allowing","the","data","to","be","processed","as","it","arrives","would","be","a","lot","more","efficient","","and","make","it","easier","to","implement","the","file","server","in","the","referenced","bug","support","streaming","data","using","network","library"],"filtered":["part","work","implement","spark","","","","","","","","nice","network","library","efficiently","stream","data","connection","","currently","shuffle","data","protocol","","efficient","large","files","","requires","whole","file","buffered","receiver","side","receiver","anything","","large","files","","comes","huge","cost","memory","","chunk","large","files","requires","client","ask","chunk","separately","","instead","","similar","approach","allowing","data","processed","arrives","lot","efficient","","make","easier","implement","file","server","referenced","bug","support","streaming","data","using","network","library"],"features":{"type":0,"size":1000,"indices":[18,36,51,52,74,82,83,95,105,108,114,135,154,159,163,170,188,198,218,235,249,263,281,299,333,343,352,370,372,388,411,425,445,446,460,472,474,490,494,495,525,527,534,551,567,568,572,580,597,624,629,656,662,695,710,722,735,740,756,760,763,781,782,788,833,863,885,903,910,938,944,957,968,993],"values":[1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,16.0,6.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,5.0,10.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":15}
{"_c0":"As work continues on HADOOP        it s become evident that we need better hooks to start daemons as specifically configured users  Via the  command   subcommand  USER environment variables in   x  we actually have a standardized way to do that  This in turn means we can make the sbin scripts super functional with a bit of updating    Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users","_c1":"Update scripts to be smarter when running with privilege","document":"As work continues on HADOOP        it s become evident that we need better hooks to start daemons as specifically configured users  Via the  command   subcommand  USER environment variables in   x  we actually have a standardized way to do that  This in turn means we can make the sbin scripts super functional with a bit of updating    Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users Update scripts to be smarter when running with privilege","words":["as","work","continues","on","hadoop","","","","","","","","it","s","become","evident","that","we","need","better","hooks","to","start","daemons","as","specifically","configured","users","","via","the","","command","","","subcommand","","user","environment","variables","in","","","x","","we","actually","have","a","standardized","way","to","do","that","","this","in","turn","means","we","can","make","the","sbin","scripts","super","functional","with","a","bit","of","updating","","","","consolidate","start","dfs","sh","and","start","secure","dns","sh","into","one","script","","","make","start","","","","sh","and","stop","","","","sh","know","how","to","switch","users","when","run","as","root","","","undeprecate","start","stop","all","sh","so","that","it","could","be","used","as","root","for","production","purposes","and","as","a","single","user","for","non","production","users","update","scripts","to","be","smarter","when","running","with","privilege"],"filtered":["work","continues","hadoop","","","","","","","","become","evident","need","better","hooks","start","daemons","specifically","configured","users","","via","","command","","","subcommand","","user","environment","variables","","","x","","actually","standardized","way","","turn","means","make","sbin","scripts","super","functional","bit","updating","","","","consolidate","start","dfs","sh","start","secure","dns","sh","one","script","","","make","start","","","","sh","stop","","","","sh","know","switch","users","run","root","","","undeprecate","start","stop","sh","used","root","production","purposes","single","user","non","production","users","update","scripts","smarter","running","privilege"],"features":{"type":0,"size":1000,"indices":[6,36,44,48,76,82,135,159,170,181,197,213,224,229,237,249,272,275,299,313,333,340,343,350,364,368,372,373,388,394,441,445,446,447,482,495,498,520,525,527,531,534,536,537,540,572,573,579,595,605,615,631,650,656,657,658,659,660,710,733,755,758,760,779,781,796,810,820,833,846,882,886,891,904,941,963,968,993,996],"values":[2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,29.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,5.0]},"cluster_label":1}
{"_c0":"Bagel has been deprecated and we haven t done any changes to it  There is no need to run those tests","_c1":"Remove Bagel test suites","document":"Bagel has been deprecated and we haven t done any changes to it  There is no need to run those tests Remove Bagel test suites","words":["bagel","has","been","deprecated","and","we","haven","t","done","any","changes","to","it","","there","is","no","need","to","run","those","tests","remove","bagel","test","suites"],"filtered":["bagel","deprecated","haven","done","changes","","need","run","tests","remove","bagel","test","suites"],"features":{"type":0,"size":1000,"indices":[91,281,288,333,346,363,364,372,388,495,535,537,539,580,586,600,619,620,707,777,831,931,964,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Based on discussion at YETUS      this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff","_c1":"Incorporate checkcompatibility script which runs Java API Compliance Checker","document":"Based on discussion at YETUS      this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff Incorporate checkcompatibility script which runs Java API Compliance Checker","words":["based","on","discussion","at","yetus","","","","","","this","code","can","t","go","there","","but","it","s","still","very","useful","for","release","managers","","a","similar","variant","of","this","script","has","been","used","for","a","while","by","apache","hbase","and","apache","kudu","","and","imo","jacc","output","is","easier","to","understand","than","jdiff","incorporate","checkcompatibility","script","which","runs","java","api","compliance","checker"],"filtered":["based","discussion","yetus","","","","","","code","go","","still","useful","release","managers","","similar","variant","script","used","apache","hbase","apache","kudu","","imo","jacc","output","easier","understand","jdiff","incorporate","checkcompatibility","script","runs","java","api","compliance","checker"],"features":{"type":0,"size":1000,"indices":[29,36,77,82,83,96,122,164,170,197,223,224,241,261,265,272,281,333,343,350,371,372,373,388,397,420,474,495,535,580,597,605,625,644,668,682,689,695,707,719,756,777,800,831,833,910,944,967],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,8.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Based on discussion offline with   marmbrus   we should remove GenerateProjection","_c1":"Remove GenerateProjection","document":"Based on discussion offline with   marmbrus   we should remove GenerateProjection Remove GenerateProjection","words":["based","on","discussion","offline","with","","","marmbrus","","","we","should","remove","generateprojection","remove","generateprojection"],"filtered":["based","discussion","offline","","","marmbrus","","","remove","generateprojection","remove","generateprojection"],"features":{"type":0,"size":1000,"indices":[19,82,288,372,444,625,650,665,695,924,993],"values":[1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Because CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S  and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file","_c1":"Add   direct  flag option for fs copy so that user can choose not to create    COPYING   file","document":"Because CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S  and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file Add   direct  flag option for fs copy so that user can choose not to create    COPYING   file","words":["because","cli","is","using","commandwithdestination","java","which","add","","","","copying","","","to","the","tail","of","file","name","when","it","does","the","copy","","for","blobstore","like","s","","and","swift","","to","create","","","","copying","","","file","and","rename","it","is","expensive","","","","direct","","flag","can","allow","user","to","avoiding","the","","","","copying","","","file","add","","","direct","","flag","option","for","fs","copy","so","that","user","can","choose","not","to","create","","","","copying","","","file"],"filtered":["cli","using","commandwithdestination","java","add","","","","copying","","","tail","file","name","copy","","blobstore","like","","swift","","create","","","","copying","","","file","rename","expensive","","","","direct","","flag","allow","user","avoiding","","","","copying","","","file","add","","","direct","","flag","option","fs","copy","user","choose","create","","","","copying","","","file"],"features":{"type":0,"size":1000,"indices":[15,18,36,62,69,76,108,135,197,216,222,231,265,281,330,333,343,368,372,388,421,432,466,488,495,518,597,624,694,698,706,710,749,760,833,852,867,882,897,932,967],"values":[1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,30.0,4.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":1}
{"_c0":"Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads","_c1":"Automatically use Kryo serializer when shuffling RDDs with simple types","document":"Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads Automatically use Kryo serializer when shuffling RDDs with simple types","words":["because","classtags","are","available","when","constructing","shuffledrdd","we","can","use","them","to","automatically","use","kryo","for","shuffle","serialization","when","the","rdd","s","types","are","guaranteed","to","be","compatible","with","kryo","","e","g","","rdds","whose","key","","value","","and","or","combiner","types","are","primitives","","arrays","of","primitives","","or","strings","","","this","is","likely","to","result","in","a","large","performance","gain","for","many","rdd","api","workloads","automatically","use","kryo","serializer","when","shuffling","rdds","with","simple","types"],"filtered":["classtags","available","constructing","shuffledrdd","use","automatically","use","kryo","shuffle","serialization","rdd","types","guaranteed","compatible","kryo","","e","g","","rdds","whose","key","","value","","combiner","types","primitives","","arrays","primitives","","strings","","","likely","result","large","performance","gain","many","rdd","api","workloads","automatically","use","kryo","serializer","shuffling","rdds","simple","types"],"features":{"type":0,"size":1000,"indices":[5,15,30,36,76,82,84,130,138,170,187,188,197,212,217,261,281,283,316,333,343,355,371,372,373,388,417,421,445,465,476,477,481,489,505,568,644,650,656,710,759,768,782,791,833,843,865,870,878,924,980,993,997],"values":[1.0,1.0,1.0,2.0,3.0,3.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Before        https   github com apache spark pull        submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now        https   github com apache spark pull       has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now","_c1":"Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result","document":"Before        https   github com apache spark pull        submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now        https   github com apache spark pull       has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result","words":["before","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","submitjob","would","create","a","separate","thread","to","wait","for","the","job","result","","","submitjobthreadpool","","was","a","workaround","in","","receivertracker","","to","run","these","waiting","job","result","threads","","now","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","has","been","merged","to","master","and","resolved","this","blocking","issue","","","submitjobthreadpool","","can","be","removed","now","remove","submitjobthreadpool","since","submitjob","doesn","t","create","a","separate","thread","to","wait","for","the","job","result"],"filtered":["","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","submitjob","create","separate","thread","wait","job","result","","","submitjobthreadpool","","workaround","","receivertracker","","run","waiting","job","result","threads","","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","merged","master","resolved","blocking","issue","","","submitjobthreadpool","","removed","remove","submitjobthreadpool","since","submitjob","doesn","create","separate","thread","wait","job","result"],"features":{"type":0,"size":1000,"indices":[36,98,105,109,133,159,163,170,221,234,245,265,270,288,333,343,364,372,373,388,445,461,470,477,495,500,509,510,512,535,556,580,585,591,597,656,690,710,718,748,759,777,805,833,865,998],"values":[2.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,40.0,1.0,4.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0]},"cluster_label":1}
{"_c0":"Besides the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP       HADOOP        HADOOP      makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it","_c1":"Hadoop Core should support source filesfor multiple schedulers","document":"Besides the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP       HADOOP        HADOOP      makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it Hadoop Core should support source filesfor multiple schedulers","words":["besides","the","default","jt","scheduling","algorithm","","there","is","work","going","on","with","at","least","two","more","schedulers","","hadoop","","","","","","","hadoop","","","","","","","","hadoop","","","","","","makes","it","easier","to","plug","in","new","schedulers","into","the","jt","","where","do","we","place","the","source","files","for","various","schedulers","so","that","it","s","easy","for","users","to","choose","their","scheduler","of","choice","during","deployment","","and","easy","for","developers","to","add","in","more","schedulers","into","the","framework","","without","inundating","it","hadoop","core","should","support","source","filesfor","multiple","schedulers"],"filtered":["besides","default","jt","scheduling","algorithm","","work","going","least","two","schedulers","","hadoop","","","","","","","hadoop","","","","","","","","hadoop","","","","","","makes","easier","plug","new","schedulers","jt","","place","source","files","various","schedulers","easy","users","choose","scheduler","choice","deployment","","easy","developers","add","schedulers","framework","","without","inundating","hadoop","core","support","source","filesfor","multiple","schedulers"],"features":{"type":0,"size":1000,"indices":[25,36,40,70,82,139,181,191,197,215,228,235,268,277,281,315,333,343,368,372,381,388,408,430,432,445,474,493,495,527,534,551,592,599,629,650,665,691,695,706,710,731,748,755,756,757,760,788,818,831,846,884,891,920,978,993,999],"values":[1.0,3.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,23.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Both VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection  Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this     following DataTypes java in SQL  so Java users doesn t need the extra          Define DataTypes in Scala","_c1":"Expose VectorUDT MatrixUDT in a public API","document":"Both VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection  Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this     following DataTypes java in SQL  so Java users doesn t need the extra          Define DataTypes in Scala Expose VectorUDT MatrixUDT in a public API","words":["both","vectorudt","and","matrixudt","are","private","apis","","because","userdefinedtype","itself","is","private","in","spark","","however","","in","order","to","let","developers","implement","their","own","transformers","and","estimators","","we","should","expose","both","types","in","a","public","api","to","simply","the","implementation","of","transformschema","","transform","","etc","","otherwise","","they","need","to","get","the","data","types","using","reflection","","note","that","this","doesn","t","mean","to","expose","vectorudt","matrixudt","classes","","we","can","just","have","a","method","or","a","static","value","that","returns","vectorudt","matrixudt","instance","with","datatype","as","the","return","type","","there","are","two","ways","to","implement","this","","","","","following","datatypes","java","in","sql","","so","java","users","doesn","t","need","the","extra","","","","","","","","","","define","datatypes","in","scala","expose","vectorudt","matrixudt","in","a","public","api"],"filtered":["vectorudt","matrixudt","private","apis","","userdefinedtype","private","spark","","however","","order","let","developers","implement","transformers","estimators","","expose","types","public","api","simply","implementation","transformschema","","transform","","etc","","otherwise","","need","get","data","types","using","reflection","","note","doesn","mean","expose","vectorudt","matrixudt","classes","","method","static","value","returns","vectorudt","matrixudt","instance","datatype","return","type","","two","ways","implement","","","","","following","datatypes","java","sql","","java","users","doesn","need","extra","","","","","","","","","","define","datatypes","scala","expose","vectorudt","matrixudt","public","api"],"features":{"type":0,"size":1000,"indices":[1,8,48,53,78,79,91,101,105,109,118,122,138,144,164,170,174,187,208,228,235,281,299,307,315,319,333,343,368,372,373,377,388,408,421,428,445,465,472,490,498,500,526,537,572,609,624,644,650,651,654,665,673,686,695,698,704,710,718,755,760,768,777,782,809,831,833,842,863,875,909,950,959,967,993,996],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,25.0,2.0,1.0,5.0,1.0,1.0,1.0,10.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,4.0,1.0,2.0,2.0,1.0]},"cluster_label":11}
{"_c0":"Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that","_c1":"Remove LeftSemiJoinBNL","document":"Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that Remove LeftSemiJoinBNL","words":["broadcast","left","semi","join","without","joining","keys","is","already","supported","in","broadcastnestedloopjoin","","it","has","the","same","implementation","as","leftsemijoinbnl","","we","should","remove","that","remove","leftsemijoinbnl"],"filtered":["broadcast","left","semi","join","without","joining","keys","already","supported","broadcastnestedloopjoin","","implementation","leftsemijoinbnl","","remove","remove","leftsemijoinbnl"],"features":{"type":0,"size":1000,"indices":[57,171,220,281,288,372,393,445,495,572,580,593,617,656,662,665,698,710,760,791,884,909,952,993],"values":[1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"CSRF prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header  The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin","_c1":"Add CSRF Filter for REST APIs to Hadoop Common","document":"CSRF prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header  The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin Add CSRF Filter for REST APIs to Hadoop Common","words":["csrf","prevention","for","rest","apis","can","be","provided","through","a","common","servlet","filter","","this","filter","would","check","for","the","existence","of","an","expected","","configurable","","http","header","","","such","as","x","xsrf","header","","the","fact","that","csrf","attacks","are","entirely","browser","based","means","that","the","above","approach","can","ensure","that","requests","are","coming","from","either","","applications","served","by","the","same","origin","as","the","rest","api","or","that","there","is","explicit","policy","configuration","that","allows","the","setting","of","a","header","on","xmlhttprequest","from","another","origin","add","csrf","filter","for","rest","apis","to","hadoop","common"],"filtered":["csrf","prevention","rest","apis","provided","common","servlet","filter","","filter","check","existence","expected","","configurable","","http","header","","","x","xsrf","header","","fact","csrf","attacks","entirely","browser","based","means","approach","ensure","requests","coming","either","","applications","served","origin","rest","api","explicit","policy","configuration","allows","setting","header","xmlhttprequest","another","origin","add","csrf","filter","rest","apis","hadoop","common"],"features":{"type":0,"size":1000,"indices":[36,45,71,82,95,138,163,170,181,187,208,215,216,223,272,281,318,343,372,373,386,388,394,413,432,480,543,572,623,625,640,644,645,656,659,665,691,702,704,708,710,731,742,752,760,766,778,779,783,792,810,823,831,833,842,866,882,921,945,954,964,998],"values":[3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0,6.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":15}
{"_c0":"CSV is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users  We should consider inlining https   github com databricks spark csv","_c1":"Have a built in CSV data source implementation","document":"CSV is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users  We should consider inlining https   github com databricks spark csv Have a built in CSV data source implementation","words":["csv","is","the","most","common","data","format","in","the","","small","data","","world","","it","is","often","the","first","format","people","want","to","try","when","they","see","spark","on","a","single","node","","making","this","built","in","for","the","most","common","source","can","provide","a","better","experience","for","first","time","users","","we","should","consider","inlining","https","","","github","com","databricks","spark","csv","have","a","built","in","csv","data","source","implementation"],"filtered":["csv","common","data","format","","small","data","","world","","often","first","format","people","want","try","see","spark","single","node","","making","built","common","source","provide","better","experience","first","time","users","","consider","inlining","https","","","github","com","databricks","spark","csv","built","csv","data","source","implementation"],"features":{"type":0,"size":1000,"indices":[36,48,70,76,82,105,150,157,159,170,183,221,222,281,288,299,312,323,362,372,373,388,401,445,485,486,495,510,515,531,665,695,698,710,712,742,745,755,770,775,833,906,941,954,993,996,998],"values":[2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,7.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Change cumeDist    cume dist  denseRank    dense rank  percentRank    percent rank  rowNumber    row number  There are two reasons that we should make this change    We should follow the naming convention rule of R  http   www inside r org node           Spark DataFrame has deprecated the old convention  such as cumeDist  and will remove it in Spark      It s better to fix this issue before     release  otherwise we will make breaking API change","_c1":"Rename some window rank function names for SparkR","document":"Change cumeDist    cume dist  denseRank    dense rank  percentRank    percent rank  rowNumber    row number  There are two reasons that we should make this change    We should follow the naming convention rule of R  http   www inside r org node           Spark DataFrame has deprecated the old convention  such as cumeDist  and will remove it in Spark      It s better to fix this issue before     release  otherwise we will make breaking API change Rename some window rank function names for SparkR","words":["change","cumedist","","","","cume","dist","","denserank","","","","dense","rank","","percentrank","","","","percent","rank","","rownumber","","","","row","number","","there","are","two","reasons","that","we","should","make","this","change","","","","we","should","follow","the","naming","convention","rule","of","r","","http","","","www","inside","r","org","node","","","","","","","","","","","spark","dataframe","has","deprecated","the","old","convention","","such","as","cumedist","","and","will","remove","it","in","spark","","","","","","it","s","better","to","fix","this","issue","before","","","","","release","","otherwise","we","will","make","breaking","api","change","rename","some","window","rank","function","names","for","sparkr"],"filtered":["change","cumedist","","","","cume","dist","","denserank","","","","dense","rank","","percentrank","","","","percent","rank","","rownumber","","","","row","number","","two","reasons","make","change","","","","follow","naming","convention","rule","r","","http","","","www","inside","r","org","node","","","","","","","","","","","spark","dataframe","deprecated","old","convention","","cumedist","","remove","spark","","","","","","better","fix","issue","","","","","release","","otherwise","make","breaking","api","change","rename","window","rank","function","names","sparkr"],"features":{"type":0,"size":1000,"indices":[36,105,138,140,158,159,160,161,167,197,264,272,276,288,313,333,343,344,362,371,372,373,388,400,408,420,445,463,495,511,525,535,570,572,580,583,620,634,644,665,672,710,711,719,728,748,760,767,779,788,793,795,831,867,875,917,941,971,993],"values":[1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,44.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":5}
{"_c0":"Classifier getNumClasses  can not support Non Double types  and classification algos relying on it do not support non double labelCol  like  NavieBayes   As suggested by   sethah   it is not a reasonable way to do datatype cast everywhere  And we can make cast only happen in  Predictor     yanboliang   josephkb   srowen","_c1":"Move LabelCol datatype cast into Predictor fit","document":"Classifier getNumClasses  can not support Non Double types  and classification algos relying on it do not support non double labelCol  like  NavieBayes   As suggested by   sethah   it is not a reasonable way to do datatype cast everywhere  And we can make cast only happen in  Predictor     yanboliang   josephkb   srowen Move LabelCol datatype cast into Predictor fit","words":["classifier","getnumclasses","","can","not","support","non","double","types","","and","classification","algos","relying","on","it","do","not","support","non","double","labelcol","","like","","naviebayes","","","as","suggested","by","","","sethah","","","it","is","not","a","reasonable","way","to","do","datatype","cast","everywhere","","and","we","can","make","cast","only","happen","in","","predictor","","","","","yanboliang","","","josephkb","","","srowen","move","labelcol","datatype","cast","into","predictor","fit"],"filtered":["classifier","getnumclasses","","support","non","double","types","","classification","algos","relying","support","non","double","labelcol","","like","","naviebayes","","","suggested","","","sethah","","","reasonable","way","datatype","cast","everywhere","","make","cast","happen","","predictor","","","","","yanboliang","","","josephkb","","","srowen","move","labelcol","datatype","cast","predictor","fit"],"features":{"type":0,"size":1000,"indices":[6,18,66,79,82,85,96,100,112,147,159,170,210,223,224,258,281,282,305,310,320,330,333,335,372,375,388,445,465,495,525,534,549,563,572,616,695,832,833,891,899,918,993],"values":[1.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,20.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail","_c1":"Improve the type check of CollectSet in CheckAnalysis","document":"CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail Improve the type check of CollectSet in CheckAnalysis","words":["collectset","","cannot","have","map","typed","data","because","maptypedata","does","not","implement","","equals","","","so","","if","we","find","map","type","in","","collectset","","","queries","fail","improve","the","type","check","of","collectset","in","checkanalysis"],"filtered":["collectset","","map","typed","data","maptypedata","implement","","equals","","","","find","map","type","","collectset","","","queries","fail","improve","type","check","collectset","checkanalysis"],"features":{"type":0,"size":1000,"indices":[18,73,130,170,172,202,254,299,343,368,372,421,445,472,510,522,526,532,695,698,703,710,882,931,993],"values":[1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,8.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Collection functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented","_c1":"Implement collection functions in SparkR","document":"Collection functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented Implement collection functions in SparkR","words":["collection","functions","documented","at","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","","are","size","","","","explode","","","","array","contains","","","and","sort","array","","","","size","","","","explode","","","are","already","implemented","","array","contains","","","and","sort","array","","","are","to","be","implemented","implement","collection","functions","in","sparkr"],"filtered":["collection","functions","documented","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","","size","","","","explode","","","","array","contains","","","sort","array","","","","size","","","","explode","","","already","implemented","","array","contains","","","sort","array","","","implemented","implement","collection","functions","sparkr"],"features":{"type":0,"size":1000,"indices":[48,57,105,138,177,192,230,307,333,372,376,388,445,472,490,495,498,535,545,587,644,652,656,665,686,706,720,756,767,899],"values":[2.0,1.0,2.0,3.0,2.0,2.0,2.0,1.0,2.0,24.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Common tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development","_c1":"Add unit tests framework  Mockito","document":"Common tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development Add unit tests framework  Mockito","words":["common","tests","are","functional","tests","or","end","to","end","","it","makes","sense","to","have","mockito","framework","for","the","convenience","of","true","unit","tests","development","add","unit","tests","framework","","mockito"],"filtered":["common","tests","functional","tests","end","end","","makes","sense","mockito","framework","convenience","true","unit","tests","development","add","unit","tests","framework","","mockito"],"features":{"type":0,"size":1000,"indices":[5,36,138,187,188,207,284,299,305,335,343,356,372,388,432,495,599,615,619,691,710,954],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read","_c1":"Configuration sends too much data to log j","document":"Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read Configuration sends too much data to log j","words":["configuration","objects","send","a","debug","level","log","message","every","time","they","re","instantiated","","which","include","a","full","stack","trace","","this","is","more","appropriate","for","trace","level","logging","","as","it","renders","other","debug","logs","very","hard","to","read","configuration","sends","too","much","data","to","log","j"],"filtered":["configuration","objects","send","debug","level","log","message","every","time","re","instantiated","","include","full","stack","trace","","appropriate","trace","level","logging","","renders","debug","logs","hard","read","configuration","sends","much","data","log","j"],"features":{"type":0,"size":1000,"indices":[36,48,101,125,149,157,170,236,281,336,372,373,388,395,401,425,495,523,524,572,597,601,629,631,644,645,646,650,674,691,695,722,778,811,876,897,936,944,981],"values":[1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Consider SortMergeJoin  which requires a sorted  clustered distribution of its input rows  Say that both of SMJ s children produce unsorted output but are both single partition  In this case  we will need to inject sort operators but should not need to inject exchanges  Unfortunately  it looks like the Exchange unnecessarily repartitions using a hash partitioning  We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied  I d like to fix this for Spark     since it makes certain types of unit tests easier to write","_c1":"EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied","document":"Consider SortMergeJoin  which requires a sorted  clustered distribution of its input rows  Say that both of SMJ s children produce unsorted output but are both single partition  In this case  we will need to inject sort operators but should not need to inject exchanges  Unfortunately  it looks like the Exchange unnecessarily repartitions using a hash partitioning  We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied  I d like to fix this for Spark     since it makes certain types of unit tests easier to write EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied","words":["consider","sortmergejoin","","which","requires","a","sorted","","clustered","distribution","of","its","input","rows","","say","that","both","of","smj","s","children","produce","unsorted","output","but","are","both","single","partition","","in","this","case","","we","will","need","to","inject","sort","operators","but","should","not","need","to","inject","exchanges","","unfortunately","","it","looks","like","the","exchange","unnecessarily","repartitions","using","a","hash","partitioning","","we","should","update","exchange","so","that","it","does","not","unnecessarily","repartition","children","when","only","the","ordering","requirements","are","unsatisfied","","i","d","like","to","fix","this","for","spark","","","","","since","it","makes","certain","types","of","unit","tests","easier","to","write","ensurerequirements","should","not","add","unnecessary","shuffles","when","only","ordering","requirements","are","unsatisfied"],"filtered":["consider","sortmergejoin","","requires","sorted","","clustered","distribution","input","rows","","say","smj","children","produce","unsorted","output","single","partition","","case","","need","inject","sort","operators","need","inject","exchanges","","unfortunately","","looks","like","exchange","unnecessarily","repartitions","using","hash","partitioning","","update","exchange","unnecessarily","repartition","children","ordering","requirements","unsatisfied","","d","like","fix","spark","","","","","since","makes","certain","types","unit","tests","easier","write","ensurerequirements","add","unnecessary","shuffles","ordering","requirements","unsatisfied"],"features":{"type":0,"size":1000,"indices":[0,18,19,36,37,50,62,76,83,94,105,113,122,138,140,170,173,197,242,296,312,329,330,335,342,343,361,368,372,373,374,388,402,420,432,445,453,465,474,492,495,531,537,554,575,585,597,612,619,624,641,665,670,691,693,698,710,720,760,798,819,831,848,852,863,899,916,938,993],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,4.0,2.0,1.0,13.0,2.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,3.0,1.0,2.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,3.0]},"cluster_label":17}
{"_c0":"Continue the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive","_c1":"Remove setCheckpointDir from LDA and tree Strategy","document":"Continue the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive Remove setCheckpointDir from LDA and tree Strategy","words":["continue","the","discussion","from","the","lda","pr","","checkpoingdir","is","a","global","spark","configuration","","which","should","not","be","altered","by","an","ml","algorithm","","we","could","check","whether","checkpointdir","is","set","if","checkpointinterval","is","positive","remove","setcheckpointdir","from","lda","and","tree","strategy"],"filtered":["continue","discussion","lda","pr","","checkpoingdir","global","spark","configuration","","altered","ml","algorithm","","check","whether","checkpointdir","set","checkpointinterval","positive","remove","setcheckpointdir","lda","tree","strategy"],"features":{"type":0,"size":1000,"indices":[1,18,25,82,105,170,213,215,223,281,288,291,324,333,356,372,494,597,601,607,624,656,665,691,695,710,732,743,752,807,813,871,882,921,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"Create some utility classes to dump both Archive and ChukwaRecords files","_c1":"Utility classes for Archive and ChukwaRecord files","document":"Create some utility classes to dump both Archive and ChukwaRecords files Utility classes for Archive and ChukwaRecord files","words":["create","some","utility","classes","to","dump","both","archive","and","chukwarecords","files","utility","classes","for","archive","and","chukwarecord","files"],"filtered":["create","utility","classes","dump","archive","chukwarecords","files","utility","classes","archive","chukwarecord","files"],"features":{"type":0,"size":1000,"indices":[36,60,265,333,339,388,400,434,551,649,763,809,863],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Cross Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting  There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate","_c1":"Add XFS Filter for UIs to Hadoop Common","document":"Cross Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting  There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate Add XFS Filter for UIs to Hadoop Common","words":["cross","frame","scripting","","xfs","","prevention","for","uis","can","be","provided","through","a","common","servlet","filter","","this","filter","will","set","the","x","frame","options","http","header","to","deny","unless","configured","to","another","valid","setting","","there","are","a","number","of","uis","that","could","just","add","this","to","their","filters","as","well","as","the","yarn","webapp","proxy","which","could","add","it","for","all","it","s","proxied","uis","","","if","appropriate","add","xfs","filter","for","uis","to","hadoop","common"],"filtered":["cross","frame","scripting","","xfs","","prevention","uis","provided","common","servlet","filter","","filter","set","x","frame","options","http","header","deny","unless","configured","another","valid","setting","","number","uis","add","filters","well","yarn","webapp","proxy","add","proxied","uis","","","appropriate","add","xfs","filter","uis","hadoop","common"],"features":{"type":0,"size":1000,"indices":[36,39,100,138,157,170,181,197,213,229,234,235,307,343,372,373,388,420,432,495,543,564,570,572,583,597,640,646,651,656,659,665,677,704,710,731,733,740,760,775,778,779,810,813,831,833,852,877,945,953,954,963,968],"values":[3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,6.0,2.0,4.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Current   ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException","_c1":"ViewFileSystem should support storage policy related API","document":"Current   ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException ViewFileSystem should support storage policy related API","words":["current","","","viewfilesystem","","","does","not","support","storage","policy","related","api","","it","will","throw","","","unsupportedoperationexception","viewfilesystem","should","support","storage","policy","related","api"],"filtered":["current","","","viewfilesystem","","","support","storage","policy","related","api","","throw","","","unsupportedoperationexception","viewfilesystem","support","storage","policy","related","api"],"features":{"type":0,"size":1000,"indices":[18,139,199,267,372,394,420,495,612,644,665,695,698,710,998],"values":[1.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Currently  KMS audit log is using log j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible","_c1":"Allow pluggable audit loggers in KMS","document":"Currently  KMS audit log is using log j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible Allow pluggable audit loggers in KMS","words":["currently","","kms","audit","log","is","using","log","j","","to","write","a","text","format","log","","we","should","refactor","this","","so","that","people","can","easily","add","new","format","audit","logs","","the","current","text","format","log","should","be","the","default","","and","all","of","its","behavior","should","remain","compatible","allow","pluggable","audit","loggers","in","kms"],"filtered":["currently","","kms","audit","log","using","log","j","","write","text","format","log","","refactor","","people","easily","add","new","format","audit","logs","","current","text","format","log","default","","behavior","remain","compatible","allow","pluggable","audit","loggers","kms"],"features":{"type":0,"size":1000,"indices":[25,113,149,169,170,222,231,281,296,333,343,368,372,373,381,388,424,432,445,476,486,533,615,623,624,631,646,656,665,710,735,760,763,766,832,833,876,968,993],"values":[1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently  There will be ConvertToSafe for PythonUDF  that s not needed actually","_c1":"PythonUDF could process UnsafeRow","document":"Currently  There will be ConvertToSafe for PythonUDF  that s not needed actually PythonUDF could process UnsafeRow","words":["currently","","there","will","be","converttosafe","for","pythonudf","","that","s","not","needed","actually","pythonudf","could","process","unsaferow"],"filtered":["currently","","converttosafe","pythonudf","","needed","actually","pythonudf","process","unsaferow"],"features":{"type":0,"size":1000,"indices":[18,22,36,175,197,213,244,372,420,447,656,760,763,822,831,973],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Currently  Union only takes intersect of the constraints from it s children  all others are dropped  we should try to merge them together","_c1":"Improve constraints propagation in Union","document":"Currently  Union only takes intersect of the constraints from it s children  all others are dropped  we should try to merge them together Improve constraints propagation in Union","words":["currently","","union","only","takes","intersect","of","the","constraints","from","it","s","children","","all","others","are","dropped","","we","should","try","to","merge","them","together","improve","constraints","propagation","in","union"],"filtered":["currently","","union","takes","intersect","constraints","children","","others","dropped","","try","merge","together","improve","constraints","propagation","union"],"features":{"type":0,"size":1000,"indices":[87,138,159,179,194,197,254,312,343,361,372,388,445,495,522,665,710,763,773,806,857,874,899,921,924,968,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Currently  many functions do now show usages like the followings     The only exceptions are  cube    grouping    grouping id    rollup    window","_c1":"All functions should show usages by command  DESC FUNCTION","document":"Currently  many functions do now show usages like the followings     The only exceptions are  cube    grouping    grouping id    rollup    window All functions should show usages by command  DESC FUNCTION","words":["currently","","many","functions","do","now","show","usages","like","the","followings","","","","","the","only","exceptions","are","","cube","","","","grouping","","","","grouping","id","","","","rollup","","","","window","all","functions","should","show","usages","by","command","","desc","function"],"filtered":["currently","","many","functions","show","usages","like","followings","","","","","exceptions","","cube","","","","grouping","","","","grouping","id","","","","rollup","","","","window","functions","show","usages","command","","desc","function"],"features":{"type":0,"size":1000,"indices":[2,17,19,98,127,135,138,188,223,313,330,372,396,428,492,511,534,587,665,710,763,781,825,899,968],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,19.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Currently  we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog   MSCK REPAIR TABLE  or  ALTER TABLE table RECOVER PARTITIONS    Actually  very hard for me to remember  MSCK  and have no clue what it means  After the new  Scalable Partition Handling   the table repair becomes much more important for making visible the data in the created data source partitioned table  It is desriable to add it into the Catalog interface so that users can repair the table by","_c1":"Add recoverPartitions API to Catalog","document":"Currently  we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog   MSCK REPAIR TABLE  or  ALTER TABLE table RECOVER PARTITIONS    Actually  very hard for me to remember  MSCK  and have no clue what it means  After the new  Scalable Partition Handling   the table repair becomes much more important for making visible the data in the created data source partitioned table  It is desriable to add it into the Catalog interface so that users can repair the table by Add recoverPartitions API to Catalog","words":["currently","","we","only","have","a","sql","interface","for","recovering","all","the","partitions","in","the","directory","of","a","table","and","update","the","catalog","","","msck","repair","table","","or","","alter","table","table","recover","partitions","","","","actually","","very","hard","for","me","to","remember","","msck","","and","have","no","clue","what","it","means","","after","the","new","","scalable","partition","handling","","","the","table","repair","becomes","much","more","important","for","making","visible","the","data","in","the","created","data","source","partitioned","table","","it","is","desriable","to","add","it","into","the","catalog","interface","so","that","users","can","repair","the","table","by","add","recoverpartitions","api","to","catalog"],"filtered":["currently","","sql","interface","recovering","partitions","directory","table","update","catalog","","","msck","repair","table","","","alter","table","table","recover","partitions","","","","actually","","hard","remember","","msck","","clue","means","","new","","scalable","partition","handling","","","table","repair","becomes","much","important","making","visible","data","created","data","source","partitioned","table","","desriable","add","catalog","interface","users","repair","table","add","recoverpartitions","api","catalog"],"features":{"type":0,"size":1000,"indices":[25,36,50,70,77,81,125,170,187,207,223,255,262,281,299,330,333,343,346,352,368,372,388,394,432,445,447,451,457,471,495,510,520,524,526,537,556,629,636,644,685,686,695,710,755,760,763,770,825,833,837,876,891,892,899,944,962,968,992,993,996],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,16.0,3.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,9.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"Currently Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section  However  the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites  This   so it can exclude the ciphers supplied through this key","_c1":"Support excluding weak Ciphers in HttpServer  through ssl server xml","document":"Currently Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section  However  the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites  This   so it can exclude the ciphers supplied through this key Support excluding weak Ciphers in HttpServer  through ssl server xml","words":["currently","embeded","jetty","server","used","across","all","hadoop","services","is","configured","through","ssl","server","xml","file","from","their","respective","configuration","section","","however","","the","ssl","tls","protocol","being","used","for","this","jetty","servers","can","be","downgraded","to","weak","cipher","suites","","this","","","so","it","can","exclude","the","ciphers","supplied","through","this","key","support","excluding","weak","ciphers","in","httpserver","","through","ssl","server","xml"],"filtered":["currently","embeded","jetty","server","used","across","hadoop","services","configured","ssl","server","xml","file","respective","configuration","section","","however","","ssl","tls","protocol","used","jetty","servers","downgraded","weak","cipher","suites","","","","exclude","ciphers","supplied","key","support","excluding","weak","ciphers","httpserver","","ssl","server","xml"],"features":{"type":0,"size":1000,"indices":[5,36,92,108,116,181,229,235,261,281,293,310,355,356,368,372,373,374,388,411,435,445,492,495,500,543,605,656,673,675,691,695,696,710,722,763,805,833,877,895,921,928,931,968,997],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,6.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp  Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS  For example  we can use them to identify jobs that end up creating a large number of files  Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary","_c1":"Add a new interface for retrieving FS and FC Statistics","document":"Currently FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp  Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS  For example  we can use them to identify jobs that end up creating a large number of files  Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary Add a new interface for retrieving FS and FC Statistics","words":["currently","filesystem","statistics","exposes","the","following","statistics","","bytesread","byteswritten","readops","largereadops","writeops","these","are","in","turn","exposed","as","job","counters","by","mapreduce","and","other","frameworks","","there","is","logic","within","dfsclient","to","map","operations","to","these","counters","that","can","be","confusing","","for","instance","","mkdirs","counts","as","a","writeop","","proposed","enhancement","","add","a","statistic","for","each","dfsclient","operation","including","create","","append","","createsymlink","","delete","","exists","","mkdirs","","rename","and","expose","them","as","new","properties","on","the","statistics","object","","the","operation","specific","counters","can","be","used","for","analyzing","the","load","imposed","by","a","particular","job","on","hdfs","","for","example","","we","can","use","them","to","identify","jobs","that","end","up","creating","a","large","number","of","files","","once","this","information","is","available","in","the","statistics","object","","the","app","frameworks","like","mapreduce","can","expose","them","as","additional","counters","to","be","aggregated","and","recorded","as","part","of","job","summary","add","a","new","interface","for","retrieving","fs","and","fc","statistics"],"filtered":["currently","filesystem","statistics","exposes","following","statistics","","bytesread","byteswritten","readops","largereadops","writeops","turn","exposed","job","counters","mapreduce","frameworks","","logic","within","dfsclient","map","operations","counters","confusing","","instance","","mkdirs","counts","writeop","","proposed","enhancement","","add","statistic","dfsclient","operation","including","create","","append","","createsymlink","","delete","","exists","","mkdirs","","rename","expose","new","properties","statistics","object","","operation","specific","counters","used","analyzing","load","imposed","particular","job","hdfs","","example","","use","identify","jobs","end","creating","large","number","files","","information","available","statistics","object","","app","frameworks","like","mapreduce","expose","additional","counters","aggregated","recorded","part","job","summary","add","new","interface","retrieving","fs","fc","statistics"],"features":{"type":0,"size":1000,"indices":[25,28,36,53,58,59,82,91,94,109,128,135,138,142,143,147,170,202,223,243,250,258,265,281,284,304,321,330,333,341,343,344,349,351,357,364,371,372,373,385,388,401,412,425,432,445,461,470,489,522,551,556,572,573,583,586,605,609,612,621,650,656,664,674,687,690,697,710,735,737,740,754,755,760,763,767,782,801,831,833,867,885,886,909,916,924,953,954,967,978,993],"values":[2.0,1.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,17.0,2.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,3.0,3.0,1.0,2.0,1.0,1.0]},"cluster_label":15}
{"_c0":"Currently FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach","_c1":"Change untar to use Java API on Windows instead of spawning tar process","document":"Currently FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach Change untar to use Java API on Windows instead of spawning tar process","words":["currently","fileutil","untar","","","spawns","tar","utility","to","do","the","work","","tar","may","not","be","present","on","all","platforms","by","default","eg","","windows","","so","changing","this","to","use","java","api","s","would","help","make","it","more","cross","platform","","fileutil","unzip","","","uses","the","same","approach","change","untar","to","use","java","api","on","windows","instead","of","spawning","tar","process"],"filtered":["currently","fileutil","untar","","","spawns","tar","utility","work","","tar","may","present","platforms","default","eg","","windows","","changing","use","java","api","help","make","cross","platform","","fileutil","unzip","","","uses","approach","change","untar","use","java","api","windows","instead","spawning","tar","process"],"features":{"type":0,"size":1000,"indices":[9,18,22,82,95,109,111,158,163,167,197,223,280,285,335,343,347,368,370,372,373,381,388,433,489,495,525,527,534,570,629,644,649,656,666,710,731,763,766,798,809,863,967,968],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Currently Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again  So  this JIRA requests two functionalities     Support for external cluster managers    Allow a cluster manager to clean up the tasks without taking the parent process down","_c1":"Add support for pluggable cluster manager","document":"Currently Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again  So  this JIRA requests two functionalities     Support for external cluster managers    Allow a cluster manager to clean up the tasks without taking the parent process down Add support for pluggable cluster manager","words":["currently","spark","allows","only","a","few","cluster","managers","viz","yarn","","mesos","and","standalone","","but","","as","spark","is","now","being","used","in","newer","and","different","use","cases","","there","is","a","need","for","allowing","other","cluster","managers","to","manage","spark","components","","one","such","use","case","is","","","embedding","spark","components","like","executor","and","driver","inside","another","process","which","may","be","a","datastore","","this","allows","colocation","of","data","and","processing","","another","requirement","that","stems","from","such","a","use","case","is","that","the","executors","driver","should","not","take","the","parent","process","down","when","they","go","down","and","the","components","can","be","relaunched","inside","the","same","process","again","","so","","this","jira","requests","two","functionalities","","","","","support","for","external","cluster","managers","","","","allow","a","cluster","manager","to","clean","up","the","tasks","without","taking","the","parent","process","down","add","support","for","pluggable","cluster","manager"],"filtered":["currently","spark","allows","cluster","managers","viz","yarn","","mesos","standalone","","","spark","used","newer","different","use","cases","","need","allowing","cluster","managers","manage","spark","components","","one","use","case","","","embedding","spark","components","like","executor","driver","inside","another","process","may","datastore","","allows","colocation","data","processing","","another","requirement","stems","use","case","executors","driver","take","parent","process","go","components","relaunched","inside","process","","","jira","requests","two","functionalities","","","","","support","external","cluster","managers","","","","allow","cluster","manager","clean","tasks","without","taking","parent","process","add","support","pluggable","cluster","manager"],"features":{"type":0,"size":1000,"indices":[11,18,22,36,44,48,66,71,76,77,83,89,98,105,128,135,139,141,170,185,217,231,235,272,281,330,333,340,342,343,359,368,372,373,374,388,393,395,400,408,424,432,439,441,445,452,460,463,480,487,489,522,537,547,564,572,576,585,586,597,605,656,665,666,668,669,674,695,710,760,763,779,800,801,821,831,833,855,866,884,899,921,965],"values":[1.0,1.0,4.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,1.0,5.0,1.0,3.0,1.0,1.0,2.0,4.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,18.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,3.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,3.0,6.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Currently different IPC servers in Hadoop use the same config variables names starting with  ipc server   This makes it difficult and confusing to maintain configuration for different IPC servers","_c1":"Support per server IPC configuration","document":"Currently different IPC servers in Hadoop use the same config variables names starting with  ipc server   This makes it difficult and confusing to maintain configuration for different IPC servers Support per server IPC configuration","words":["currently","different","ipc","servers","in","hadoop","use","the","same","config","variables","names","starting","with","","ipc","server","","","this","makes","it","difficult","and","confusing","to","maintain","configuration","for","different","ipc","servers","support","per","server","ipc","configuration"],"filtered":["currently","different","ipc","servers","hadoop","use","config","variables","names","starting","","ipc","server","","","makes","difficult","confusing","maintain","configuration","different","ipc","servers","support","per","server","ipc","configuration"],"features":{"type":0,"size":1000,"indices":[36,40,89,116,181,247,250,272,276,333,352,372,373,388,411,440,445,483,489,495,616,650,656,691,695,710,763],"values":[1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Currently in SQL we implement overwrites by calling fs delete   directly on the original data  This is not ideal since we the original files end up deleted even if the job aborts  We should extend the commit protocol to allow file overwrites to be managed as well","_c1":"Add deleteWithJob hook to internal commit protocol API","document":"Currently in SQL we implement overwrites by calling fs delete   directly on the original data  This is not ideal since we the original files end up deleted even if the job aborts  We should extend the commit protocol to allow file overwrites to be managed as well Add deleteWithJob hook to internal commit protocol API","words":["currently","in","sql","we","implement","overwrites","by","calling","fs","delete","","","directly","on","the","original","data","","this","is","not","ideal","since","we","the","original","files","end","up","deleted","even","if","the","job","aborts","","we","should","extend","the","commit","protocol","to","allow","file","overwrites","to","be","managed","as","well","add","deletewithjob","hook","to","internal","commit","protocol","api"],"filtered":["currently","sql","implement","overwrites","calling","fs","delete","","","directly","original","data","","ideal","since","original","files","end","deleted","even","job","aborts","","extend","commit","protocol","allow","file","overwrites","managed","well","add","deletewithjob","hook","internal","commit","protocol","api"],"features":{"type":0,"size":1000,"indices":[18,78,82,86,108,128,135,157,170,188,211,223,231,247,280,281,284,295,372,373,388,406,432,439,445,470,472,551,572,583,585,644,656,665,684,686,695,710,722,724,759,763,834,953,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0]},"cluster_label":0}
{"_c0":"Currently max queue size for IPC server is set to        handlers   Usually when RPC failures are observed  e g  HADOOP        we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than       There are other improvements also  HADOOP        Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic  Say handler count is     default  and Server can handle      RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for   sec before it is dropped  If there      clients and all of them send RPCs around the same time  not very rare  with heartbeats etc        will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management  For this jira I propose to make queue size per handler configurable  with a larger default  may be","_c1":"IPC server max queue size should be configurable","document":"Currently max queue size for IPC server is set to        handlers   Usually when RPC failures are observed  e g  HADOOP        we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than       There are other improvements also  HADOOP        Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic  Say handler count is     default  and Server can handle      RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for   sec before it is dropped  If there      clients and all of them send RPCs around the same time  not very rare  with heartbeats etc        will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management  For this jira I propose to make queue size per handler configurable  with a larger default  may be IPC server max queue size should be configurable","words":["currently","max","queue","size","for","ipc","server","is","set","to","","","","","","","","handlers","","","usually","when","rpc","failures","are","observed","","e","g","","hadoop","","","","","","","","we","increase","number","of","handlers","and","the","problem","goes","away","","i","think","a","big","part","of","such","a","fix","is","increase","in","max","queue","size","","i","think","we","should","make","maxqsize","per","handler","configurable","","with","a","bigger","default","than","","","","","","","there","are","other","improvements","also","","hadoop","","","","","","","","server","keeps","reading","rpc","requests","from","clients","","when","the","number","in","flight","rpcs","is","larger","than","maxqsize","","the","earliest","rpcs","are","deleted","","this","is","the","main","feedback","server","has","for","the","client","","i","have","often","heard","from","users","that","hadoop","doesn","t","handle","bursty","traffic","","say","handler","count","is","","","","","default","","and","server","can","handle","","","","","","rpcs","a","sec","","quite","conservative","low","for","a","typical","server","","","it","implies","that","an","rpc","can","wait","for","only","for","","","sec","before","it","is","dropped","","if","there","","","","","","clients","and","all","of","them","send","rpcs","around","the","same","time","","not","very","rare","","with","heartbeats","etc","","","","","","","","will","be","dropped","","in","stead","of","dropping","the","earliest","rpcs","","if","the","server","delays","reading","new","rpcs","","the","feedback","to","clients","would","be","much","smoother","","i","will","file","another","jira","regd","queue","management","","for","this","jira","i","propose","to","make","queue","size","per","handler","configurable","","with","a","larger","default","","may","be","ipc","server","max","queue","size","should","be","configurable"],"filtered":["currently","max","queue","size","ipc","server","set","","","","","","","","handlers","","","usually","rpc","failures","observed","","e","g","","hadoop","","","","","","","","increase","number","handlers","problem","goes","away","","think","big","part","fix","increase","max","queue","size","","think","make","maxqsize","per","handler","configurable","","bigger","default","","","","","","","improvements","also","","hadoop","","","","","","","","server","keeps","reading","rpc","requests","clients","","number","flight","rpcs","larger","maxqsize","","earliest","rpcs","deleted","","main","feedback","server","client","","often","heard","users","hadoop","doesn","handle","bursty","traffic","","say","handler","count","","","","","default","","server","handle","","","","","","rpcs","sec","","quite","conservative","low","typical","server","","","implies","rpc","wait","","","sec","dropped","","","","","","","clients","send","rpcs","around","time","","rare","","heartbeats","etc","","","","","","","","dropped","","stead","dropping","earliest","rpcs","","server","delays","reading","new","rpcs","","feedback","clients","much","smoother","","file","another","jira","regd","queue","management","","jira","propose","make","queue","size","per","handler","configurable","","larger","default","","may","ipc","server","max","queue","size","configurable"],"features":{"type":0,"size":1000,"indices":[18,25,28,36,56,58,71,76,88,92,94,101,108,110,135,138,157,159,163,170,181,192,220,229,255,261,272,280,281,283,299,313,329,330,333,340,343,357,372,373,381,383,388,393,401,405,411,415,417,420,424,440,445,452,460,477,483,495,500,510,521,523,524,525,552,564,580,583,598,604,618,622,625,637,650,655,656,665,666,674,693,710,724,737,740,752,755,760,763,771,777,779,780,792,806,809,813,821,831,833,852,871,878,899,921,924,934,944,963,964,968,993],"values":[1.0,1.0,2.0,6.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,8.0,6.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,6.0,3.0,1.0,3.0,5.0,1.0,3.0,1.0,4.0,1.0,77.0,2.0,3.0,2.0,3.0,2.0,1.0,1.0,7.0,2.0,1.0,2.0,5.0,2.0,4.0,1.0,2.0,1.0,2.0,2.0,1.0,7.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,1.0,9.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,2.0]},"cluster_label":3}
{"_c0":"Currently our Optimizer may reorder the predicates to run them more efficient  but in non deterministic condition  change the order between deterministic parts and non deterministic parts may change the number of input rows  For example     may call rand   for different times and therefore the output rows differ","_c1":"Improve the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition","document":"Currently our Optimizer may reorder the predicates to run them more efficient  but in non deterministic condition  change the order between deterministic parts and non deterministic parts may change the number of input rows  For example     may call rand   for different times and therefore the output rows differ Improve the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition","words":["currently","our","optimizer","may","reorder","the","predicates","to","run","them","more","efficient","","but","in","non","deterministic","condition","","change","the","order","between","deterministic","parts","and","non","deterministic","parts","may","change","the","number","of","input","rows","","for","example","","","","","may","call","rand","","","for","different","times","and","therefore","the","output","rows","differ","improve","the","pushdownpredicate","rule","to","pushdown","predicates","currectly","in","non","deterministic","condition"],"filtered":["currently","optimizer","may","reorder","predicates","run","efficient","","non","deterministic","condition","","change","order","deterministic","parts","non","deterministic","parts","may","change","number","input","rows","","example","","","","","may","call","rand","","","different","times","therefore","output","rows","differ","improve","pushdownpredicate","rule","pushdown","predicates","currectly","non","deterministic","condition"],"features":{"type":0,"size":1000,"indices":[0,1,36,83,89,122,146,150,158,224,243,295,333,343,345,364,372,388,389,445,460,481,522,583,612,629,666,670,674,704,710,718,763,798,833,848,884,924,940,972],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,9.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Currently the   MutableRates   metrics class serializes all writes to metrics it contains because of its use of   MetricsRegistry add      i e   even two increments of unrelated metrics contained within the same   MutableRates   object will serialize w r t  each other   This class is used by   RpcDetailedMetrics    which may have many hundreds of threads contending to modify these metrics  Instead we should allow updates to unrelated metrics objects to happen concurrently  To do so we can let each thread locally collect metrics  and on a   snapshot    aggregate the metrics from all of the threads  I have collected some benchmark performance numbers in HADOOP        https   issues apache org jira secure attachment          benchmark results  which indicate that this can bring significantly higher performance in high contention situations","_c1":"Make MutableRates metrics thread local write  aggregate on read","document":"Currently the   MutableRates   metrics class serializes all writes to metrics it contains because of its use of   MetricsRegistry add      i e   even two increments of unrelated metrics contained within the same   MutableRates   object will serialize w r t  each other   This class is used by   RpcDetailedMetrics    which may have many hundreds of threads contending to modify these metrics  Instead we should allow updates to unrelated metrics objects to happen concurrently  To do so we can let each thread locally collect metrics  and on a   snapshot    aggregate the metrics from all of the threads  I have collected some benchmark performance numbers in HADOOP        https   issues apache org jira secure attachment          benchmark results  which indicate that this can bring significantly higher performance in high contention situations Make MutableRates metrics thread local write  aggregate on read","words":["currently","the","","","mutablerates","","","metrics","class","serializes","all","writes","to","metrics","it","contains","because","of","its","use","of","","","metricsregistry","add","","","","","","i","e","","","even","two","increments","of","unrelated","metrics","contained","within","the","same","","","mutablerates","","","object","will","serialize","w","r","t","","each","other","","","this","class","is","used","by","","","rpcdetailedmetrics","","","","which","may","have","many","hundreds","of","threads","contending","to","modify","these","metrics","","instead","we","should","allow","updates","to","unrelated","metrics","objects","to","happen","concurrently","","to","do","so","we","can","let","each","thread","locally","collect","metrics","","and","on","a","","","snapshot","","","","aggregate","the","metrics","from","all","of","the","threads","","i","have","collected","some","benchmark","performance","numbers","in","hadoop","","","","","","","","https","","","issues","apache","org","jira","secure","attachment","","","","","","","","","","benchmark","results","","which","indicate","that","this","can","bring","significantly","higher","performance","in","high","contention","situations","make","mutablerates","metrics","thread","local","write","","aggregate","on","read"],"filtered":["currently","","","mutablerates","","","metrics","class","serializes","writes","metrics","contains","use","","","metricsregistry","add","","","","","","e","","","even","two","increments","unrelated","metrics","contained","within","","","mutablerates","","","object","serialize","w","r","","","","class","used","","","rpcdetailedmetrics","","","","may","many","hundreds","threads","contending","modify","metrics","","instead","allow","updates","unrelated","metrics","objects","happen","concurrently","","let","thread","locally","collect","metrics","","","","snapshot","","","","aggregate","metrics","threads","","collected","benchmark","performance","numbers","hadoop","","","","","","","","https","","","issues","apache","org","jira","secure","attachment","","","","","","","","","","benchmark","results","","indicate","bring","significantly","higher","performance","high","contention","situations","make","mutablerates","metrics","thread","local","write","","aggregate","read"],"features":{"type":0,"size":1000,"indices":[6,23,32,37,41,48,82,106,109,113,116,139,148,164,170,181,188,222,223,231,254,261,281,296,299,329,332,333,336,343,363,368,372,373,388,400,406,408,412,420,421,432,438,445,451,461,462,475,485,489,494,495,520,525,526,534,535,564,570,594,597,605,608,650,656,665,666,667,674,710,739,747,759,760,763,777,805,821,822,833,863,878,885,904,915,921,924,925,968,977,993,998],"values":[1.0,1.0,1.0,1.0,3.0,1.0,2.0,8.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,5.0,1.0,2.0,54.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":5}
{"_c0":"Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on  The versions of these libraries are also present in ivy libraries properties  so that  when a library is updated  it must be updated in two places  which is error prone  We should instead only specify library versions in a single place","_c1":"versions of dependencies should be specified in a single place","document":"Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on  The versions of these libraries are also present in ivy libraries properties  so that  when a library is updated  it must be updated in two places  which is error prone  We should instead only specify library versions in a single place versions of dependencies should be specified in a single place","words":["currently","the","maven","pom","file","is","generated","from","a","template","file","that","includes","the","versions","of","all","the","libraries","we","depend","on","","the","versions","of","these","libraries","are","also","present","in","ivy","libraries","properties","","so","that","","when","a","library","is","updated","","it","must","be","updated","in","two","places","","which","is","error","prone","","we","should","instead","only","specify","library","versions","in","a","single","place","versions","of","dependencies","should","be","specified","in","a","single","place"],"filtered":["currently","maven","pom","file","generated","template","file","includes","versions","libraries","depend","","versions","libraries","also","present","ivy","libraries","properties","","","library","updated","","must","updated","two","places","","error","prone","","instead","specify","library","versions","single","place","versions","dependencies","specified","single","place"],"features":{"type":0,"size":1000,"indices":[23,76,82,108,127,138,170,175,177,191,196,281,315,333,343,349,360,368,372,408,445,446,461,482,490,495,531,562,566,597,656,665,671,710,760,763,792,809,833,852,863,899,921,959,968,991,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,6.0,1.0,4.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,2.0,4.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made  S  does not support flushing of data  data is written once the stream is actually closed  In case of failure  the data for the last minute  default rolling interval  will not be properly written  Therefore we need a flag to close the stream after the write  so that we achieve read after write consistency","_c1":"Flag to close Write Ahead Log after writing","document":"Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made  S  does not support flushing of data  data is written once the stream is actually closed  In case of failure  the data for the last minute  default rolling interval  will not be properly written  Therefore we need a flag to close the stream after the write  so that we achieve read after write consistency Flag to close Write Ahead Log after writing","words":["currently","the","write","ahead","log","in","spark","streaming","flushes","data","as","writes","need","to","be","made","","s","","does","not","support","flushing","of","data","","data","is","written","once","the","stream","is","actually","closed","","in","case","of","failure","","the","data","for","the","last","minute","","default","rolling","interval","","will","not","be","properly","written","","therefore","we","need","a","flag","to","close","the","stream","after","the","write","","so","that","we","achieve","read","after","write","consistency","flag","to","close","write","ahead","log","after","writing"],"filtered":["currently","write","ahead","log","spark","streaming","flushes","data","writes","need","made","","","support","flushing","data","","data","written","stream","actually","closed","","case","failure","","data","last","minute","","default","rolling","interval","","properly","written","","therefore","need","flag","close","stream","write","","achieve","read","write","consistency","flag","close","write","ahead","log","writing"],"features":{"type":0,"size":1000,"indices":[18,36,77,103,105,108,113,147,150,161,170,177,197,218,234,235,263,281,342,343,344,368,372,381,388,420,445,447,470,476,517,537,572,602,607,631,644,650,656,685,695,698,710,760,763,924,932,989,993],"values":[2.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,9.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,5.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":15}
{"_c0":"Currently the check done in the  hasSufficientTimeElapsed    method is hardcoded to    mins wait  The wait time should be driven by configuration and its default value  for clients should be   min","_c1":"Kerberos relogin interval in UserGroupInformation should be configurable","document":"Currently the check done in the  hasSufficientTimeElapsed    method is hardcoded to    mins wait  The wait time should be driven by configuration and its default value  for clients should be   min Kerberos relogin interval in UserGroupInformation should be configurable","words":["currently","the","check","done","in","the","","hassufficienttimeelapsed","","","","method","is","hardcoded","to","","","","mins","wait","","the","wait","time","should","be","driven","by","configuration","and","its","default","value","","for","clients","should","be","","","min","kerberos","relogin","interval","in","usergroupinformation","should","be","configurable"],"filtered":["currently","check","done","","hassufficienttimeelapsed","","","","method","hardcoded","","","","mins","wait","","wait","time","driven","configuration","default","value","","clients","","","min","kerberos","relogin","interval","usergroupinformation","configurable"],"features":{"type":0,"size":1000,"indices":[36,73,157,169,223,235,237,281,296,333,358,372,381,385,388,399,445,477,654,656,665,674,691,707,710,724,763,768,882,886,964],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Currently the dfs  test command only supports  d   e   f   s   z options  It would be helpful if we add  w   r to verify permission of r w before actual read or write  This will help script programming","_c1":"Add  w  r options in dfs  test command","document":"Currently the dfs  test command only supports  d   e   f   s   z options  It would be helpful if we add  w   r to verify permission of r w before actual read or write  This will help script programming Add  w  r options in dfs  test command","words":["currently","the","dfs","","test","command","only","supports","","d","","","e","","","f","","","s","","","z","options","","it","would","be","helpful","if","we","add","","w","","","r","to","verify","permission","of","r","w","before","actual","read","or","write","","this","will","help","script","programming","add","","w","","r","options","in","dfs","","test","command"],"filtered":["currently","dfs","","test","command","supports","","d","","","e","","","f","","","","","z","options","","helpful","add","","w","","","r","verify","permission","r","w","actual","read","write","","help","script","programming","add","","w","","r","options","dfs","","test","command"],"features":{"type":0,"size":1000,"indices":[50,94,113,135,159,163,170,187,197,248,269,275,286,343,347,350,372,373,388,420,432,443,445,495,570,586,650,651,656,710,716,763,878,899,909,915,993,994],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,18.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Currently the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types","_c1":"Support to specify join type when calling join with usingColumns","document":"Currently the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types Support to specify join type when calling join with usingColumns","words":["currently","the","method","join","right","","dataframe","","usingcolumns","","seq","string","","","only","supports","inner","join","","it","is","more","convenient","to","have","it","support","other","join","types","support","to","specify","join","type","when","calling","join","with","usingcolumns"],"filtered":["currently","method","join","right","","dataframe","","usingcolumns","","seq","string","","","supports","inner","join","","convenient","support","join","types","support","specify","join","type","calling","join","usingcolumns"],"features":{"type":0,"size":1000,"indices":[76,161,193,281,299,372,388,410,465,495,526,574,583,629,650,654,674,695,710,730,763,800,888,899,952,991,994],"values":[1.0,1.0,1.0,1.0,1.0,6.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Currently the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones","_c1":"Add support for launching multiple Mesos dispatchers","document":"Currently the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones Add support for launching multiple Mesos dispatchers","words":["currently","the","sbin","","start","stop","","mesos","dispatcher","scripts","only","assume","there","is","one","mesos","dispatcher","launched","","but","potentially","users","that","like","to","run","multi","tenant","dispatcher","might","want","to","launch","multiples","","it","also","helps","local","development","to","have","the","ability","to","launch","multiple","ones","add","support","for","launching","multiple","mesos","dispatchers"],"filtered":["currently","sbin","","start","stop","","mesos","dispatcher","scripts","assume","one","mesos","dispatcher","launched","","potentially","users","like","run","multi","tenant","dispatcher","might","want","launch","multiples","","also","helps","local","development","ability","launch","multiple","ones","add","support","launching","multiple","mesos","dispatchers"],"features":{"type":0,"size":1000,"indices":[1,36,44,83,281,299,311,312,327,330,331,356,364,365,372,388,432,433,495,498,514,567,573,592,594,608,631,695,710,712,753,755,760,763,792,796,831,899,965,985,996],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Currently when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes","_c1":"Improve ActiveStandbyElector s behavior when session expires","document":"Currently when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes Improve ActiveStandbyElector s behavior when session expires","words":["currently","when","the","zk","session","expires","","it","results","in","a","fatal","error","being","sent","to","the","application","callback","","this","is","not","the","best","behavior","","","","for","example","","in","the","case","of","ha","","if","zk","goes","down","","we","would","like","the","current","state","to","be","maintained","","rather","than","causing","either","nn","to","abort","","when","the","zk","clients","are","able","to","reconnect","","they","should","sort","out","the","correct","leader","based","on","the","normal","locking","schemes","improve","activestandbyelector","s","behavior","when","session","expires"],"filtered":["currently","zk","session","expires","","results","fatal","error","sent","application","callback","","best","behavior","","","","example","","case","ha","","zk","goes","","like","current","state","maintained","","rather","causing","either","nn","abort","","zk","clients","able","reconnect","","sort","correct","leader","based","normal","locking","schemes","improve","activestandbyelector","behavior","session","expires"],"features":{"type":0,"size":1000,"indices":[18,20,36,48,76,82,86,89,116,117,138,163,168,169,170,197,217,243,261,281,286,330,333,340,342,343,349,363,372,373,374,388,389,405,412,437,445,495,496,507,522,572,625,649,654,656,658,665,710,714,720,724,735,750,763,883,993,999],"values":[1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,11.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications","_c1":"Add a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY","document":"DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications Add a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY","words":["distribute","by","allows","the","user","to","control","the","partitioning","and","ordering","of","a","data","set","which","can","be","very","useful","for","some","applications","add","a","dataframe","api","that","provides","functionality","similar","to","hiveql","s","distribute","by"],"filtered":["distribute","allows","user","control","partitioning","ordering","data","set","useful","applications","add","dataframe","api","provides","functionality","similar","hiveql","distribute"],"features":{"type":0,"size":1000,"indices":[36,161,170,197,223,239,272,333,343,365,388,400,432,480,492,597,644,656,695,710,742,760,813,833,839,874,882,910,916,944,953],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion","_c1":"Remove the internal implicit conversion from LogicalPlan to DataFrame","document":"DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion Remove the internal implicit conversion from LogicalPlan to DataFrame","words":["dataframe","has","an","internal","implicit","conversion","that","turns","a","logicalplan","into","a","dataframe","","this","has","been","fairly","confusing","to","a","few","new","contributors","","since","it","doesn","t","buy","us","much","","we","should","just","remove","that","implicit","conversion","remove","the","internal","implicit","conversion","from","logicalplan","to","dataframe"],"filtered":["dataframe","internal","implicit","conversion","turns","logicalplan","dataframe","","fairly","confusing","new","contributors","","since","doesn","buy","us","much","","remove","implicit","conversion","remove","internal","implicit","conversion","logicalplan","dataframe"],"features":{"type":0,"size":1000,"indices":[25,110,161,170,208,213,250,288,295,307,372,373,388,400,420,471,495,500,515,524,535,577,580,585,665,710,752,760,777,795,891,921,993],"values":[1.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Dataframe drop supported multi columns in spark api and should make python api also support it","_c1":"Dataframe drop supported multi columns in spark api and should make python api also support it","document":"Dataframe drop supported multi columns in spark api and should make python api also support it Dataframe drop supported multi columns in spark api and should make python api also support it","words":["dataframe","drop","supported","multi","columns","in","spark","api","and","should","make","python","api","also","support","it","dataframe","drop","supported","multi","columns","in","spark","api","and","should","make","python","api","also","support","it"],"filtered":["dataframe","drop","supported","multi","columns","spark","api","make","python","api","also","support","dataframe","drop","supported","multi","columns","spark","api","make","python","api","also","support"],"features":{"type":0,"size":1000,"indices":[105,161,312,333,445,495,525,577,589,593,644,665,695,792,969],"values":[2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,4.0,2.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Dataset always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it","_c1":"Remove spark sql eagerAnalysis","document":"Dataset always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it Remove spark sql eagerAnalysis","words":["dataset","always","does","eager","analysis","now","","thus","","spark","sql","eageranalysis","is","not","used","any","more","","thus","","we","need","to","remove","it","remove","spark","sql","eageranalysis"],"filtered":["dataset","always","eager","analysis","","thus","","spark","sql","eageranalysis","used","","thus","","need","remove","remove","spark","sql","eageranalysis"],"features":{"type":0,"size":1000,"indices":[13,18,91,98,105,182,281,288,296,372,388,450,493,495,537,605,629,684,686,698,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Deprecated configs are currently all strewn across the code base  It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere","_c1":"Centralize deprecated configs in SparkConf","document":"Deprecated configs are currently all strewn across the code base  It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere Centralize deprecated configs in SparkConf","words":["deprecated","configs","are","currently","all","strewn","across","the","code","base","","it","would","be","good","to","simplify","the","handling","of","the","deprecated","configs","in","a","central","location","to","avoid","duplicating","the","deprecation","logic","everywhere","centralize","deprecated","configs","in","sparkconf"],"filtered":["deprecated","configs","currently","strewn","across","code","base","","good","simplify","handling","deprecated","configs","central","location","avoid","duplicating","deprecation","logic","everywhere","centralize","deprecated","configs","sparkconf"],"features":{"type":0,"size":1000,"indices":[19,62,79,109,138,163,168,170,173,219,304,310,323,343,372,388,420,445,493,495,616,620,656,710,741,763,825,843,888,968],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"DistCp uses ThrottleInputStream  which provides a bandwidth throttling on a specified stream  Currently  Distcp allows the max bandwidth value in Mega Bytes  which does not accept fractional values  It would be better if it accepts the Max Bandwitdh in fractional MegaBytes  Due to this we are not able to throttle the bandwidth in KBs in our prod setup","_c1":"Allow ditscp to accept bandwitdh in fraction MegaBytes","document":"DistCp uses ThrottleInputStream  which provides a bandwidth throttling on a specified stream  Currently  Distcp allows the max bandwidth value in Mega Bytes  which does not accept fractional values  It would be better if it accepts the Max Bandwitdh in fractional MegaBytes  Due to this we are not able to throttle the bandwidth in KBs in our prod setup Allow ditscp to accept bandwitdh in fraction MegaBytes","words":["distcp","uses","throttleinputstream","","which","provides","a","bandwidth","throttling","on","a","specified","stream","","currently","","distcp","allows","the","max","bandwidth","value","in","mega","bytes","","which","does","not","accept","fractional","values","","it","would","be","better","if","it","accepts","the","max","bandwitdh","in","fractional","megabytes","","due","to","this","we","are","not","able","to","throttle","the","bandwidth","in","kbs","in","our","prod","setup","allow","ditscp","to","accept","bandwitdh","in","fraction","megabytes"],"filtered":["distcp","uses","throttleinputstream","","provides","bandwidth","throttling","specified","stream","","currently","","distcp","allows","max","bandwidth","value","mega","bytes","","accept","fractional","values","","better","accepts","max","bandwitdh","fractional","megabytes","","due","able","throttle","bandwidth","kbs","prod","setup","allow","ditscp","accept","bandwitdh","fraction","megabytes"],"features":{"type":0,"size":1000,"indices":[18,82,86,111,138,163,170,175,218,231,239,243,274,283,337,372,373,385,388,445,459,480,494,495,496,597,620,656,698,704,710,728,747,761,763,768,825,833,846,911,925,931,941,993],"values":[2.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,6.0,1.0,2.0,3.0,5.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"During heavy shuffle  packet loss for IPC packets was observed from a machine  Avoid packet loss and speed up transfer by using  x   QOS bits for the packets","_c1":"Add a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY","document":"During heavy shuffle  packet loss for IPC packets was observed from a machine  Avoid packet loss and speed up transfer by using  x   QOS bits for the packets Add a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY","words":["during","heavy","shuffle","","packet","loss","for","ipc","packets","was","observed","from","a","machine","","avoid","packet","loss","and","speed","up","transfer","by","using","","x","","","qos","bits","for","the","packets","add","a","configuration","to","set","ipc","client","s","traffic","class","with","iptos","lowdelay","iptos","reliability"],"filtered":["heavy","shuffle","","packet","loss","ipc","packets","observed","machine","","avoid","packet","loss","speed","transfer","using","","x","","","qos","bits","packets","add","configuration","set","ipc","client","traffic","class","iptos","lowdelay","iptos","reliability"],"features":{"type":0,"size":1000,"indices":[36,109,128,135,170,197,198,223,229,234,247,277,329,333,353,363,372,388,426,432,451,483,533,534,568,604,624,650,691,693,710,786,810,813,852,920,921],"values":[2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"During http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is    hours  For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated  The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be    minutes","_c1":"Enable MaxInactiveInterval for hadoop http auth token","document":"During http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is    hours  For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated  The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be    minutes Enable MaxInactiveInterval for hadoop http auth token","words":["during","http","authentication","","a","cookie","which","contains","the","authentication","token","is","dropped","","the","expiry","time","of","the","authentication","token","can","be","configured","via","hadoop","http","authentication","token","validity","","the","default","value","is","","","","hours","","for","clusters","which","require","enhanced","security","","it","is","desirable","to","have","a","configurable","maxinactiveinterval","for","the","authentication","token","","if","there","is","no","activity","during","maxinactiveinterval","","the","authentication","token","will","be","invalidated","","the","maxinactiveinterval","will","be","less","than","hadoop","http","authentication","token","validity","","the","default","value","will","be","","","","minutes","enable","maxinactiveinterval","for","hadoop","http","auth","token"],"filtered":["http","authentication","","cookie","contains","authentication","token","dropped","","expiry","time","authentication","token","configured","via","hadoop","http","authentication","token","validity","","default","value","","","","hours","","clusters","require","enhanced","security","","desirable","configurable","maxinactiveinterval","authentication","token","","activity","maxinactiveinterval","","authentication","token","invalidated","","maxinactiveinterval","less","hadoop","http","authentication","token","validity","","default","value","","","","minutes","enable","maxinactiveinterval","hadoop","http","auth","token"],"features":{"type":0,"size":1000,"indices":[36,48,110,132,146,157,170,181,195,229,261,266,277,280,281,299,332,343,346,372,381,388,420,495,528,580,586,595,597,610,634,656,664,665,688,696,710,721,768,806,831,833,964,992],"values":[3.0,1.0,1.0,1.0,7.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,15.0,2.0,1.0,3.0,1.0,7.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,4.0,4.0,1.0,8.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":15}
{"_c0":"E g  currently we can do up to   sorts within a task      During the aggregation     During a sort on the same key     During the shuffle In environments with tight memory restrictions  the first operator may acquire so much memory such that the subsequent ones in the same task are starved  A simple fix is to reserve at least a page in advance in each of these places  The reserved page size need not be the same as the normal page size  This is a sister problem to SPARK      in Spark Core","_c1":"Reserve a page in all unsafe operators to avoid starving an operator","document":"E g  currently we can do up to   sorts within a task      During the aggregation     During a sort on the same key     During the shuffle In environments with tight memory restrictions  the first operator may acquire so much memory such that the subsequent ones in the same task are starved  A simple fix is to reserve at least a page in advance in each of these places  The reserved page size need not be the same as the normal page size  This is a sister problem to SPARK      in Spark Core Reserve a page in all unsafe operators to avoid starving an operator","words":["e","g","","currently","we","can","do","up","to","","","sorts","within","a","task","","","","","","during","the","aggregation","","","","","during","a","sort","on","the","same","key","","","","","during","the","shuffle","in","environments","with","tight","memory","restrictions","","the","first","operator","may","acquire","so","much","memory","such","that","the","subsequent","ones","in","the","same","task","are","starved","","a","simple","fix","is","to","reserve","at","least","a","page","in","advance","in","each","of","these","places","","the","reserved","page","size","need","not","be","the","same","as","the","normal","page","size","","this","is","a","sister","problem","to","spark","","","","","","in","spark","core","reserve","a","page","in","all","unsafe","operators","to","avoid","starving","an","operator"],"filtered":["e","g","","currently","","","sorts","within","task","","","","","","aggregation","","","","","sort","key","","","","","shuffle","environments","tight","memory","restrictions","","first","operator","may","acquire","much","memory","subsequent","ones","task","starved","","simple","fix","reserve","least","page","advance","places","","reserved","page","size","need","normal","page","size","","sister","problem","spark","","","","","","spark","core","reserve","page","unsafe","operators","avoid","starving","operator"],"features":{"type":0,"size":1000,"indices":[18,82,101,105,109,127,128,138,140,170,183,192,199,213,228,242,257,272,277,281,288,336,343,355,368,372,373,388,410,412,417,445,451,461,524,534,537,567,568,572,649,650,656,666,690,710,720,752,756,757,760,763,775,779,788,799,827,833,838,878,885,928,968,980,989,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,26.0,1.0,4.0,1.0,1.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,9.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"Elt function doesn t support codegen execution  It is better to provide the support","_c1":"Add codegen for Elt function","document":"Elt function doesn t support codegen execution  It is better to provide the support Add codegen for Elt function","words":["elt","function","doesn","t","support","codegen","execution","","it","is","better","to","provide","the","support","add","codegen","for","elt","function"],"filtered":["elt","function","doesn","support","codegen","execution","","better","provide","support","add","codegen","elt","function"],"features":{"type":0,"size":1000,"indices":[36,263,281,284,288,313,372,388,432,495,500,695,710,767,777,941],"values":[1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs  The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request  Using JWT provides a number of benefits    It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens","_c1":"Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth","document":"Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs  The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request  Using JWT provides a number of benefits    It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth","words":["extend","altkerberosauthenticationhandler","to","provide","websso","flow","for","uis","","the","actual","authentication","is","done","by","some","external","service","that","the","handler","will","redirect","to","when","there","is","no","hadoop","auth","cookie","and","no","jwt","token","found","in","the","incoming","request","","using","jwt","provides","a","number","of","benefits","","","","it","is","not","tied","to","any","specific","authentication","mechanism","","","so","buys","us","many","sso","integrations","","","it","is","cryptographically","verifiable","for","determining","whether","it","can","be","trusted","","","checking","for","expiration","allows","for","a","limited","lifetime","and","window","for","compromised","use","this","will","introduce","the","use","of","nimbus","jose","jwt","library","for","processing","","validating","and","parsing","jwt","tokens","add","redirecting","websso","behavior","with","jwt","token","in","hadoop","auth"],"filtered":["extend","altkerberosauthenticationhandler","provide","websso","flow","uis","","actual","authentication","done","external","service","handler","redirect","hadoop","auth","cookie","jwt","token","found","incoming","request","","using","jwt","provides","number","benefits","","","","tied","specific","authentication","mechanism","","","buys","us","many","sso","integrations","","","cryptographically","verifiable","determining","whether","trusted","","","checking","expiration","allows","limited","lifetime","window","compromised","use","introduce","use","nimbus","jose","jwt","library","processing","","validating","parsing","jwt","tokens","add","redirecting","websso","behavior","jwt","token","hadoop","auth"],"features":{"type":0,"size":1000,"indices":[18,33,36,59,76,91,92,99,146,160,170,181,188,208,223,239,255,269,281,288,313,333,339,343,346,362,368,372,373,388,400,420,426,432,445,446,480,489,493,495,511,524,528,533,577,581,583,585,586,605,610,624,628,644,645,650,656,664,677,707,710,735,753,760,771,775,807,830,831,833,834,837,909,945,955],"values":[1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,2.0,13.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"ExternalShuffleService is essential for spark  In order to better monitor shuffle service  we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system","_c1":"Add metrics and source for external shuffle service","document":"ExternalShuffleService is essential for spark  In order to better monitor shuffle service  we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system Add metrics and source for external shuffle service","words":["externalshuffleservice","is","essential","for","spark","","in","order","to","better","monitor","shuffle","service","","we","added","various","metrics","in","shuffle","service","and","externalshuffleservicesource","for","metric","system","add","metrics","and","source","for","external","shuffle","service"],"filtered":["externalshuffleservice","essential","spark","","order","better","monitor","shuffle","service","","added","various","metrics","shuffle","service","externalshuffleservicesource","metric","system","add","metrics","source","external","shuffle","service"],"features":{"type":0,"size":1000,"indices":[15,36,70,105,106,281,286,327,333,372,384,388,432,445,527,568,586,639,663,718,753,941,993,999],"values":[1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"FileSystem createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method  For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist","_c1":"Undeprecate createNonRecursive","document":"FileSystem createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method  For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist Undeprecate createNonRecursive","words":["filesystem","createnonrecursive","","","is","deprecated","","however","","there","is","no","distributedfilesystem","create","","","implementation","which","throws","exception","if","parent","directory","doesn","t","exist","","this","limits","clients","","migration","away","from","the","deprecated","method","","for","hbase","","io","fencing","relies","on","the","behavior","of","filesystem","createnonrecursive","","","","variant","of","create","","","method","should","be","added","which","throws","exception","if","parent","directory","doesn","t","exist","undeprecate","createnonrecursive"],"filtered":["filesystem","createnonrecursive","","","deprecated","","however","","distributedfilesystem","create","","","implementation","throws","exception","parent","directory","doesn","exist","","limits","clients","","migration","away","deprecated","method","","hbase","","io","fencing","relies","behavior","filesystem","createnonrecursive","","","","variant","create","","","method","added","throws","exception","parent","directory","doesn","exist","undeprecate","createnonrecursive"],"features":{"type":0,"size":1000,"indices":[29,36,66,82,144,156,170,209,265,281,340,343,346,364,372,373,384,500,512,593,597,599,620,654,656,660,665,673,689,698,710,724,735,737,764,777,831,838,921,962,992],"values":[1.0,1.0,2.0,1.0,2.0,3.0,2.0,2.0,2.0,2.0,1.0,2.0,1.0,2.0,15.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"FileUtil copyMerge is currently unused in the Hadoop source tree  In branch    it had been part of the implementation of the hadoop fs  getmerge shell command  In branch    the code for that shell command was rewritten in a way that no longer requires this method  Please check more details here   https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment","_c1":"Deprecate FileUtil copyMerge","document":"FileUtil copyMerge is currently unused in the Hadoop source tree  In branch    it had been part of the implementation of the hadoop fs  getmerge shell command  In branch    the code for that shell command was rewritten in a way that no longer requires this method  Please check more details here   https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment Deprecate FileUtil copyMerge","words":["fileutil","copymerge","is","currently","unused","in","the","hadoop","source","tree","","in","branch","","","","it","had","been","part","of","the","implementation","of","the","hadoop","fs","","getmerge","shell","command","","in","branch","","","","the","code","for","that","shell","command","was","rewritten","in","a","way","that","no","longer","requires","this","method","","please","check","more","details","here","","","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","deprecate","fileutil","copymerge"],"filtered":["fileutil","copymerge","currently","unused","hadoop","source","tree","","branch","","","","part","implementation","hadoop","fs","","getmerge","shell","command","","branch","","","","code","shell","command","rewritten","way","longer","requires","method","","please","check","details","","","https","","","issues","apache","org","jira","browse","hadoop","","","","","","","focusedcommentid","","","","","","","","","","page","com","atlassian","jira","plugin","system","issuetabpanels","comment","tabpanel","comment","deprecate","fileutil","copymerge"],"features":{"type":0,"size":1000,"indices":[36,70,76,123,128,135,154,159,170,181,221,234,275,280,281,294,343,346,370,372,373,385,420,445,453,455,475,495,535,575,629,639,654,662,698,710,740,760,763,821,827,841,862,868,871,882,928,938,980,998],"values":[1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,29.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"For HDFS     we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache   license","_c1":"Add support for unix domain sockets to JNI libs","document":"For HDFS     we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache   license Add support for unix domain sockets to JNI libs","words":["for","hdfs","","","","","we","need","to","use","unix","domain","sockets","","this","jira","is","to","include","a","library","in","common","which","adds","a","o","a","h","net","unix","package","based","on","the","code","from","android","","apache","","","license","add","support","for","unix","domain","sockets","to","jni","libs"],"filtered":["hdfs","","","","","need","use","unix","domain","sockets","","jira","include","library","common","adds","o","h","net","unix","package","based","code","android","","apache","","","license","add","support","unix","domain","sockets","jni","libs"],"features":{"type":0,"size":1000,"indices":[7,36,82,170,182,186,260,266,281,282,372,373,388,401,411,420,428,432,445,446,489,490,495,537,597,625,695,710,821,880,915,921,954,967,985,993],"values":[3.0,2.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,8.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"For a variety of reasons  we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi","_c1":"Remove DeveloperApi annotation from private classes","document":"For a variety of reasons  we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi Remove DeveloperApi annotation from private classes","words":["for","a","variety","of","reasons","","we","tagged","a","bunch","of","internal","classes","in","the","execution","package","in","sql","as","developerapi","remove","developerapi","annotation","from","private","classes"],"filtered":["variety","reasons","","tagged","bunch","internal","classes","execution","package","sql","developerapi","remove","developerapi","annotation","private","classes"],"features":{"type":0,"size":1000,"indices":[26,31,36,140,170,233,266,284,288,295,343,372,399,445,572,686,704,710,798,809,921,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"For building SparkR vignettes on Jenkins machines  we need the rmarkdown R  The package is available at https   cran r project org web packages rmarkdown index html   I think running something like Rscript  e  install packages  rmarkdown   repos  http   cran stat ucla edu     should work","_c1":"Install rmarkdown R package on Jenkins machines","document":"For building SparkR vignettes on Jenkins machines  we need the rmarkdown R  The package is available at https   cran r project org web packages rmarkdown index html   I think running something like Rscript  e  install packages  rmarkdown   repos  http   cran stat ucla edu     should work Install rmarkdown R package on Jenkins machines","words":["for","building","sparkr","vignettes","on","jenkins","machines","","we","need","the","rmarkdown","r","","the","package","is","available","at","https","","","cran","r","project","org","web","packages","rmarkdown","index","html","","","i","think","running","something","like","rscript","","e","","install","packages","","rmarkdown","","","repos","","http","","","cran","stat","ucla","edu","","","","","should","work","install","rmarkdown","r","package","on","jenkins","machines"],"filtered":["building","sparkr","vignettes","jenkins","machines","","need","rmarkdown","r","","package","available","https","","","cran","r","project","org","web","packages","rmarkdown","index","html","","","think","running","something","like","rscript","","e","","install","packages","","rmarkdown","","","repos","","http","","","cran","stat","ucla","edu","","","","","work","install","rmarkdown","r","package","jenkins","machines"],"features":{"type":0,"size":1000,"indices":[36,82,266,276,281,306,307,329,330,371,372,526,527,528,535,537,546,548,553,564,570,631,652,665,671,710,756,757,764,767,797,852,860,878,896,963,993,998],"values":[1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,18.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"For lots of SQL operators  we have metrics for both of input and output  the number of input rows should be exactly the number of output rows of child  we could only have metrics for output rows  After we improve the performance using whole stage codegen  the overhead of SQL metrics are not trivial anymore  we should avoid that if it s not necessary  Some of the operator does not have SQL metrics  we should add that for them  For those operators that have the same number of rows from input and output  for example  Projection  we may don t need that","_c1":"Remove duplicated SQL metrics","document":"For lots of SQL operators  we have metrics for both of input and output  the number of input rows should be exactly the number of output rows of child  we could only have metrics for output rows  After we improve the performance using whole stage codegen  the overhead of SQL metrics are not trivial anymore  we should avoid that if it s not necessary  Some of the operator does not have SQL metrics  we should add that for them  For those operators that have the same number of rows from input and output  for example  Projection  we may don t need that Remove duplicated SQL metrics","words":["for","lots","of","sql","operators","","we","have","metrics","for","both","of","input","and","output","","the","number","of","input","rows","should","be","exactly","the","number","of","output","rows","of","child","","we","could","only","have","metrics","for","output","rows","","after","we","improve","the","performance","using","whole","stage","codegen","","the","overhead","of","sql","metrics","are","not","trivial","anymore","","we","should","avoid","that","if","it","s","not","necessary","","some","of","the","operator","does","not","have","sql","metrics","","we","should","add","that","for","them","","for","those","operators","that","have","the","same","number","of","rows","from","input","and","output","","for","example","","projection","","we","may","don","t","need","that","remove","duplicated","sql","metrics"],"filtered":["lots","sql","operators","","metrics","input","output","","number","input","rows","exactly","number","output","rows","child","","metrics","output","rows","","improve","performance","using","whole","stage","codegen","","overhead","sql","metrics","trivial","anymore","","avoid","necessary","","operator","sql","metrics","","add","","operators","number","rows","input","output","","example","","projection","","may","need","remove","duplicated","sql","metrics"],"features":{"type":0,"size":1000,"indices":[0,18,36,77,106,109,114,122,138,140,170,197,199,204,213,243,247,263,288,299,333,343,372,400,427,432,434,495,522,537,583,600,624,630,656,665,666,670,686,697,698,710,735,759,760,777,780,850,863,899,903,921,924,993],"values":[3.0,3.0,6.0,1.0,5.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,3.0,8.0,12.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,3.0,1.0,4.0,4.0,1.0,1.0,6.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0]},"cluster_label":15}
{"_c0":"For very large source trees on s  distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used     M files    K dirs  it was taking    minutes before my fix in HADOOP       and    minutes after the fix","_c1":"Speed up distcp buildListing   using threadpool","document":"For very large source trees on s  distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used     M files    K dirs  it was taking    minutes before my fix in HADOOP       and    minutes after the fix Speed up distcp buildListing   using threadpool","words":["for","very","large","source","trees","on","s","","distcp","is","taking","long","time","to","build","file","listing","","client","code","","before","starting","mappers","","","for","a","dataset","i","used","","","","","m","files","","","","k","dirs","","it","was","taking","","","","minutes","before","my","fix","in","hadoop","","","","","","","and","","","","minutes","after","the","fix","speed","up","distcp","buildlisting","","","using","threadpool"],"filtered":["large","source","trees","","distcp","taking","long","time","build","file","listing","","client","code","","starting","mappers","","","dataset","used","","","","","m","files","","","","k","dirs","","taking","","","","minutes","fix","hadoop","","","","","","","","","","minutes","fix","speed","distcp","buildlisting","","","using","threadpool"],"features":{"type":0,"size":1000,"indices":[36,70,77,82,108,128,135,157,159,170,181,197,198,234,281,317,329,333,372,388,420,437,445,449,456,493,494,495,536,551,605,616,624,638,696,710,782,801,855,881,904,916,944],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,27.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"GenerateUnsafeProjection can be used directly as a code generated serializer  We no longer need SparkSqlSerializer","_c1":"Remove SparkSqlSerializer  in favor of Unsafe exchange","document":"GenerateUnsafeProjection can be used directly as a code generated serializer  We no longer need SparkSqlSerializer Remove SparkSqlSerializer  in favor of Unsafe exchange","words":["generateunsafeprojection","can","be","used","directly","as","a","code","generated","serializer","","we","no","longer","need","sparksqlserializer","remove","sparksqlserializer","","in","favor","of","unsafe","exchange"],"filtered":["generateunsafeprojection","used","directly","code","generated","serializer","","longer","need","sparksqlserializer","remove","sparksqlserializer","","favor","unsafe","exchange"],"features":{"type":0,"size":1000,"indices":[5,170,188,236,242,275,288,315,329,343,346,372,420,445,537,572,605,656,697,833,868,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Given test patch s tendency to get forked into a variety of different projects  it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base","_c1":"Umbrella  Split test patch off into its own TLP","document":"Given test patch s tendency to get forked into a variety of different projects  it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base Umbrella  Split test patch off into its own TLP","words":["given","test","patch","s","tendency","to","get","forked","into","a","variety","of","different","projects","","it","makes","a","lot","of","sense","to","make","an","apache","tlp","so","that","everyone","can","benefit","from","a","common","code","base","umbrella","","split","test","patch","off","into","its","own","tlp"],"filtered":["given","test","patch","tendency","get","forked","variety","different","projects","","makes","lot","sense","make","apache","tlp","everyone","benefit","common","code","base","umbrella","","split","test","patch","tlp"],"features":{"type":0,"size":1000,"indices":[5,26,89,137,157,170,173,197,228,252,255,265,296,343,368,372,388,420,437,495,497,525,541,586,691,703,735,752,760,796,833,891,893,921,954,959],"values":[1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"HADOOP       mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival  The way this was achieved was by applying an intensive exponential retry when throttling occurred  As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries  i e  we will do a client side copy of the blob and then copy it back to destination  This operation will not be subject to throttling and hence should provide a stronger mitigation  However it is more expensive  hence we do it only in the case we fail after all retries","_c1":"Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries","document":"HADOOP       mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival  The way this was achieved was by applying an intensive exponential retry when throttling occurred  As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries  i e  we will do a client side copy of the blob and then copy it back to destination  This operation will not be subject to throttling and hence should provide a stronger mitigation  However it is more expensive  hence we do it only in the case we fail after all retries Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries","words":["hadoop","","","","","","","mitigated","the","problem","of","hmaster","aborting","regionserver","due","to","azure","storage","throttling","event","during","hbase","wal","archival","","the","way","this","was","achieved","was","by","applying","an","intensive","exponential","retry","when","throttling","occurred","","as","a","second","level","of","mitigation","we","will","change","the","mode","of","copy","operation","if","the","operation","fails","even","after","all","retries","","i","e","","we","will","do","a","client","side","copy","of","the","blob","and","then","copy","it","back","to","destination","","this","operation","will","not","be","subject","to","throttling","and","hence","should","provide","a","stronger","mitigation","","however","it","is","more","expensive","","hence","we","do","it","only","in","the","case","we","fail","after","all","retries","change","mode","of","copy","operation","of","hbase","wal","archiving","to","bypass","azure","storage","throttling","after","retries"],"filtered":["hadoop","","","","","","","mitigated","problem","hmaster","aborting","regionserver","due","azure","storage","throttling","event","hbase","wal","archival","","way","achieved","applying","intensive","exponential","retry","throttling","occurred","","second","level","mitigation","change","mode","copy","operation","operation","fails","even","retries","","e","","client","side","copy","blob","copy","back","destination","","operation","subject","throttling","hence","provide","stronger","mitigation","","however","expensive","","hence","case","fail","retries","change","mode","copy","operation","hbase","wal","archiving","bypass","azure","storage","throttling","retries"],"features":{"type":0,"size":1000,"indices":[18,29,62,76,77,91,101,122,135,158,159,170,181,216,221,223,234,236,277,279,281,288,293,329,333,342,343,365,371,372,373,381,388,394,406,413,420,430,445,481,495,531,532,534,564,572,576,611,629,646,656,665,673,710,717,722,726,747,752,775,781,801,805,810,872,878,899,905,911,949,968,993],"values":[1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,2.0,4.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,6.0,1.0,1.0,13.0,2.0,2.0,4.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,4.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,4.0]},"cluster_label":15}
{"_c0":"HADOOP      introduces a configure flag to prevent potential status inconsistency between zkfc and namenode  by making auto and manual failover mutually exclusive  However  as described in       section of design doc at HDFS       we should allow manual and auto failover co exist  by    adding some rpc interfaces at zkfc   manual failover shall be triggered by haadmin  and handled by zkfc if auto failover is enabled","_c1":"Auto HA  Allow manual failover to be invoked from zkfc","document":"HADOOP      introduces a configure flag to prevent potential status inconsistency between zkfc and namenode  by making auto and manual failover mutually exclusive  However  as described in       section of design doc at HDFS       we should allow manual and auto failover co exist  by    adding some rpc interfaces at zkfc   manual failover shall be triggered by haadmin  and handled by zkfc if auto failover is enabled Auto HA  Allow manual failover to be invoked from zkfc","words":["hadoop","","","","","","introduces","a","configure","flag","to","prevent","potential","status","inconsistency","between","zkfc","and","namenode","","by","making","auto","and","manual","failover","mutually","exclusive","","however","","as","described","in","","","","","","","section","of","design","doc","at","hdfs","","","","","","","we","should","allow","manual","and","auto","failover","co","exist","","by","","","","adding","some","rpc","interfaces","at","zkfc","","","manual","failover","shall","be","triggered","by","haadmin","","and","handled","by","zkfc","if","auto","failover","is","enabled","auto","ha","","allow","manual","failover","to","be","invoked","from","zkfc"],"filtered":["hadoop","","","","","","introduces","configure","flag","prevent","potential","status","inconsistency","zkfc","namenode","","making","auto","manual","failover","mutually","exclusive","","however","","described","","","","","","","section","design","doc","hdfs","","","","","","","allow","manual","auto","failover","co","exist","","","","","adding","rpc","interfaces","zkfc","","","manual","failover","shall","triggered","haadmin","","handled","zkfc","auto","failover","enabled","auto","ha","","allow","manual","failover","invoked","zkfc"],"features":{"type":0,"size":1000,"indices":[80,108,144,158,170,181,189,198,215,223,231,235,279,281,309,321,332,333,343,367,372,388,400,403,407,434,445,446,453,455,481,497,508,572,656,658,665,673,681,696,720,748,756,921,932,967,971,993,996],"values":[1.0,4.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,5.0,4.0,4.0,1.0,1.0,28.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"HDFS     added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo","_c1":"Forward port SequenceFile syncFs and friends from Hadoop   x","document":"HDFS     added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo Forward port SequenceFile syncFs and friends from Hadoop   x","words":["hdfs","","","","","added","a","new","public","api","sequencefile","syncfs","","we","need","to","forward","port","this","for","compatibility","","looks","like","it","might","have","introduced","other","apis","that","need","forward","porting","as","well","","eg","localtedblocks","setfilelength","","and","datanode","getblockinfo","forward","port","sequencefile","syncfs","and","friends","from","hadoop","","","x"],"filtered":["hdfs","","","","","added","new","public","api","sequencefile","syncfs","","need","forward","port","compatibility","","looks","like","might","introduced","apis","need","forward","porting","well","","eg","localtedblocks","setfilelength","","datanode","getblockinfo","forward","port","sequencefile","syncfs","friends","hadoop","","","x"],"features":{"type":0,"size":1000,"indices":[9,25,36,157,170,173,179,181,240,299,330,331,333,372,373,384,388,404,442,495,498,504,537,572,583,644,667,674,713,760,772,810,826,834,842,921,967,985,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"HadoopKerberosName has been around as a  secret hack  for quite a while  We should clean up the output and make it official by exposing it via the hadoop command","_c1":"Expose HadoopKerberosName as a hadoop subcommand","document":"HadoopKerberosName has been around as a  secret hack  for quite a while  We should clean up the output and make it official by exposing it via the hadoop command Expose HadoopKerberosName as a hadoop subcommand","words":["hadoopkerberosname","has","been","around","as","a","","secret","hack","","for","quite","a","while","","we","should","clean","up","the","output","and","make","it","official","by","exposing","it","via","the","hadoop","command","expose","hadoopkerberosname","as","a","hadoop","subcommand"],"filtered":["hadoopkerberosname","around","","secret","hack","","quite","","clean","output","make","official","exposing","via","hadoop","command","expose","hadoopkerberosname","hadoop","subcommand"],"features":{"type":0,"size":1000,"indices":[36,73,109,122,128,135,170,181,223,255,315,333,340,372,395,485,495,525,535,540,572,580,595,665,707,710,737,760,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Hive already supports this according to https   issues apache org jira browse HIVE        Currently Spark sql still supports only primitive types","_c1":"collect list   and collect set   should accept struct types as argument","document":"Hive already supports this according to https   issues apache org jira browse HIVE        Currently Spark sql still supports only primitive types collect list   and collect set   should accept struct types as argument","words":["hive","already","supports","this","according","to","https","","","issues","apache","org","jira","browse","hive","","","","","","","","currently","spark","sql","still","supports","only","primitive","types","collect","list","","","and","collect","set","","","should","accept","struct","types","as","argument"],"filtered":["hive","already","supports","according","https","","","issues","apache","org","jira","browse","hive","","","","","","","","currently","spark","sql","still","supports","primitive","types","collect","list","","","collect","set","","","accept","struct","types","argument"],"features":{"type":0,"size":1000,"indices":[57,105,116,154,333,372,373,388,465,475,495,500,535,572,594,599,613,665,686,728,763,800,813,821,846,849,899,994,998],"values":[1.0,1.0,1.0,1.0,1.0,13.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"Hudson should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up   It would be nice if  when the timer goes off  Hudson did a     See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer","_c1":"Hudson should kill long running tests","document":"Hudson should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up   It would be nice if  when the timer goes off  Hudson did a     See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer Hudson should kill long running tests","words":["hudson","should","kill","long","running","tests","","","i","believe","it","is","supposed","to","but","doesn","t","quite","seem","to","do","the","job","if","the","test","is","really","hung","up","","","it","would","be","nice","if","","when","the","timer","goes","off","","hudson","did","a","","","","","see","the","section","","killing","a","hung","test","","at","http","","","wiki","apache","org","lucene","hadoop","hudsonbuildserver","hudson","should","kill","long","running","tests"],"filtered":["hudson","kill","long","running","tests","","","believe","supposed","doesn","quite","seem","job","test","really","hung","","","nice","","timer","goes","","hudson","","","","","see","section","","killing","hung","test","","http","","","wiki","apache","org","lucene","hadoop","hudsonbuildserver","hudson","kill","long","running","tests"],"features":{"type":0,"size":1000,"indices":[23,42,76,83,84,128,163,170,181,255,281,310,329,340,361,370,371,372,388,392,470,481,495,497,500,515,534,535,586,588,607,619,656,665,696,710,756,777,873,904,963,986],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,14.0,2.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":17}
{"_c0":"I was investingating progress in SPARK       and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK       use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK        Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute","_c1":"TrainValidationSplit is missing in pyspark ml tuning","document":"I was investingating progress in SPARK       and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK       use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK        Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute TrainValidationSplit is missing in pyspark ml tuning","words":["i","was","investingating","progress","in","spark","","","","","","","and","i","noticed","that","there","is","no","trainvalidationsplit","class","in","pyspark","ml","tuning","module","","java","scala","s","examples","spark","","","","","","","use","org","apache","spark","ml","tuning","trainvalidationsplit","that","is","not","available","from","python","and","this","blocks","spark","","","","","","","","does","the","class","have","different","name","in","pyspark","","maybe","","also","","i","couldn","t","find","any","jira","task","to","saying","it","need","to","be","implemented","","is","it","by","design","that","the","trainvalidationsplit","estimator","is","not","ported","to","pyspark","","if","not","","that","is","if","the","estimator","needs","porting","then","i","would","like","to","contribute","trainvalidationsplit","is","missing","in","pyspark","ml","tuning"],"filtered":["investingating","progress","spark","","","","","","","noticed","trainvalidationsplit","class","pyspark","ml","tuning","module","","java","scala","examples","spark","","","","","","","use","org","apache","spark","ml","tuning","trainvalidationsplit","available","python","blocks","spark","","","","","","","","class","different","name","pyspark","","maybe","","also","","couldn","find","jira","task","saying","need","implemented","","design","trainvalidationsplit","estimator","ported","pyspark","","","estimator","needs","porting","like","contribute","trainvalidationsplit","missing","pyspark","ml","tuning"],"features":{"type":0,"size":1000,"indices":[15,18,20,89,91,105,163,170,177,189,197,213,223,234,281,299,324,329,330,333,346,350,371,372,373,381,388,399,445,451,454,489,490,495,509,510,525,534,535,537,556,589,596,626,649,656,666,667,698,710,756,760,777,792,821,831,868,921,932,942,967],"values":[1.0,3.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,3.0,4.0,1.0,2.0,1.0,1.0,1.0,26.0,1.0,1.0,4.0,1.0,4.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"If the Window does not have any window expression  it is useless  It might happen after column pruning","_c1":"Eliminate Unnecessary Window","document":"If the Window does not have any window expression  it is useless  It might happen after column pruning Eliminate Unnecessary Window","words":["if","the","window","does","not","have","any","window","expression","","it","is","useless","","it","might","happen","after","column","pruning","eliminate","unnecessary","window"],"filtered":["window","window","expression","","useless","","might","happen","column","pruning","eliminate","unnecessary","window"],"features":{"type":0,"size":1000,"indices":[6,18,77,91,170,242,245,281,299,372,456,495,511,601,698,710,804,906,985],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"If there are known failures  test patch will bail out as soon as it sees them  This causes the precommit builds to potentially not find real issues with a patch  because the tests that would fail might come after a known failure  We should add  fn to just the mvn test command in test patch to get the full list of failures","_c1":"test patch should run tests with  fn to avoid masking test failures","document":"If there are known failures  test patch will bail out as soon as it sees them  This causes the precommit builds to potentially not find real issues with a patch  because the tests that would fail might come after a known failure  We should add  fn to just the mvn test command in test patch to get the full list of failures test patch should run tests with  fn to avoid masking test failures","words":["if","there","are","known","failures","","test","patch","will","bail","out","as","soon","as","it","sees","them","","this","causes","the","precommit","builds","to","potentially","not","find","real","issues","with","a","patch","","because","the","tests","that","would","fail","might","come","after","a","known","failure","","we","should","add","","fn","to","just","the","mvn","test","command","in","test","patch","to","get","the","full","list","of","failures","test","patch","should","run","tests","with","","fn","to","avoid","masking","test","failures"],"filtered":["known","failures","","test","patch","bail","soon","sees","","causes","precommit","builds","potentially","find","real","issues","patch","","tests","fail","might","come","known","failure","","add","","fn","mvn","test","command","test","patch","get","full","list","failures","test","patch","run","tests","","fn","avoid","masking","test","failures"],"features":{"type":0,"size":1000,"indices":[18,41,69,77,103,109,135,137,138,163,170,240,307,330,334,343,363,364,372,373,388,415,420,421,432,445,475,495,498,510,532,572,586,619,650,654,665,684,691,710,728,760,772,831,881,897,924,959,964,985,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,6.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,2.0,2.0,1.0,2.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Implement  struct    encode   decode  in SparkR as documented at http   spark apache org docs latest api scala index html org apache spark sql functions","_c1":"Implement  struct    encode   decode  in SparkR","document":"Implement  struct    encode   decode  in SparkR as documented at http   spark apache org docs latest api scala index html org apache spark sql functions Implement  struct    encode   decode  in SparkR","words":["implement","","struct","","","","encode","","","decode","","in","sparkr","as","documented","at","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","implement","","struct","","","","encode","","","decode","","in","sparkr"],"filtered":["implement","","struct","","","","encode","","","decode","","sparkr","documented","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","functions","implement","","struct","","","","encode","","","decode","","sparkr"],"features":{"type":0,"size":1000,"indices":[105,268,289,307,372,445,472,490,495,498,535,545,572,587,613,644,652,665,686,756,767,899],"values":[2.0,2.0,2.0,1.0,16.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"Implement a wrapper in SparkR to support bisecting k means","_c1":"Bisecting k means wrapper in SparkR","document":"Implement a wrapper in SparkR to support bisecting k means Bisecting k means wrapper in SparkR","words":["implement","a","wrapper","in","sparkr","to","support","bisecting","k","means","bisecting","k","means","wrapper","in","sparkr"],"filtered":["implement","wrapper","sparkr","support","bisecting","k","means","bisecting","k","means","wrapper","sparkr"],"features":{"type":0,"size":1000,"indices":[170,388,394,401,445,456,472,695,762,767],"values":[1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Implement repartitionByColumn on DataFrame  This will allow us to run R functions on each partition identified by column groups with dapply   method","_c1":"SparkR   Implement repartitionByColumn on DataFrame","document":"Implement repartitionByColumn on DataFrame  This will allow us to run R functions on each partition identified by column groups with dapply   method SparkR   Implement repartitionByColumn on DataFrame","words":["implement","repartitionbycolumn","on","dataframe","","this","will","allow","us","to","run","r","functions","on","each","partition","identified","by","column","groups","with","dapply","","","method","sparkr","","","implement","repartitionbycolumn","on","dataframe"],"filtered":["implement","repartitionbycolumn","dataframe","","allow","us","run","r","functions","partition","identified","column","groups","dapply","","","method","sparkr","","","implement","repartitionbycolumn","dataframe"],"features":{"type":0,"size":1000,"indices":[50,82,161,167,208,223,231,364,372,373,388,420,472,506,545,570,587,601,605,650,654,767,885],"values":[1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In     and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs       api java index html  However  this disappeared in        http   spark apache org docs       api java index html  Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since","_c1":"Remove package grouping in genjavadoc","document":"In     and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs       api java index html  However  this disappeared in        http   spark apache org docs       api java index html  Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since Remove package grouping in genjavadoc","words":["in","","","","","and","earlier","releases","","we","have","package","grouping","in","the","generated","java","api","docs","","see","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","however","","this","disappeared","in","","","","","","","","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","rather","than","fixing","it","","i","d","suggest","removing","grouping","","because","it","might","take","some","time","to","fix","and","it","is","a","manual","process","to","update","the","grouping","in","sparkbuild","scala","","no","one","complained","about","missing","groups","since","remove","package","grouping","in","genjavadoc"],"filtered":["","","","","earlier","releases","","package","grouping","generated","java","api","docs","","see","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","however","","disappeared","","","","","","","","http","","","spark","apache","org","docs","","","","","","","api","java","index","html","","rather","fixing","","d","suggest","removing","grouping","","might","take","time","fix","manual","process","update","grouping","sparkbuild","scala","","one","complained","missing","groups","since","remove","package","grouping","genjavadoc"],"features":{"type":0,"size":1000,"indices":[22,44,94,105,157,170,194,207,261,266,281,288,299,307,315,329,332,333,343,346,372,373,388,400,421,437,445,453,490,495,515,525,535,545,585,605,632,644,652,656,665,673,710,775,777,825,855,858,911,967,968,985,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,35.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"In HADOOP       the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over    seconds causing the Failover Controller to kill it  The way the current Guava cache implementation works is approximately     On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache     When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value  I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed  Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters     hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys    hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is    which is likely to be enough","_c1":"Reload cached groups in background after expiry","document":"In HADOOP       the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over    seconds causing the Failover Controller to kill it  The way the current Guava cache implementation works is approximately     On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache     When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value  I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed  Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters     hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys    hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is    which is likely to be enough Reload cached groups in background after expiry","words":["in","hadoop","","","","","","","the","guava","cache","was","introduced","to","allow","refreshes","on","the","namenode","group","cache","to","run","in","the","background","","avoiding","many","slow","group","lookups","","even","with","this","change","","i","have","seen","quite","a","few","clusters","with","issues","due","to","slow","group","lookups","","the","problem","is","most","prevalent","in","ha","clusters","","where","a","slow","group","lookup","on","the","hdfs","user","can","fail","to","return","for","over","","","","seconds","causing","the","failover","controller","to","kill","it","","the","way","the","current","guava","cache","implementation","works","is","approximately","","","","","on","initial","load","","the","first","thread","to","request","groups","for","a","given","user","blocks","until","it","returns","","any","subsequent","threads","requesting","that","user","block","until","that","first","thread","populates","the","cache","","","","","when","the","key","expires","","the","first","thread","to","hit","the","cache","after","expiry","blocks","","while","it","is","blocked","","other","threads","will","return","the","old","value","","i","feel","it","is","this","blocking","thread","that","still","gives","the","namenode","issues","on","slow","group","lookups","","if","the","call","from","the","fc","is","the","one","that","blocks","and","lookups","are","slow","","if","can","cause","the","nn","to","be","killed","","guava","has","the","ability","to","refresh","expired","keys","completely","in","the","background","","where","the","first","thread","that","hits","an","expired","key","schedules","a","background","cache","reload","","but","still","returns","the","old","value","","then","the","cache","is","eventually","updated","","this","patch","introduces","this","background","reload","feature","","there","are","two","new","parameters","","","","","hadoop","security","groups","cache","background","reload","","","default","false","to","keep","the","current","behaviour","","set","to","true","to","enable","a","small","thread","pool","and","background","refresh","for","expired","keys","","","","hadoop","security","groups","cache","background","reload","threads","","","only","relevant","if","the","above","is","set","to","true","","controls","how","many","threads","are","in","the","background","refresh","pool","","default","is","","","","which","is","likely","to","be","enough","reload","cached","groups","in","background","after","expiry"],"filtered":["hadoop","","","","","","","guava","cache","introduced","allow","refreshes","namenode","group","cache","run","background","","avoiding","many","slow","group","lookups","","even","change","","seen","quite","clusters","issues","due","slow","group","lookups","","problem","prevalent","ha","clusters","","slow","group","lookup","hdfs","user","fail","return","","","","seconds","causing","failover","controller","kill","","way","current","guava","cache","implementation","works","approximately","","","","","initial","load","","first","thread","request","groups","given","user","blocks","returns","","subsequent","threads","requesting","user","block","first","thread","populates","cache","","","","","key","expires","","first","thread","hit","cache","expiry","blocks","","blocked","","threads","return","old","value","","feel","blocking","thread","still","gives","namenode","issues","slow","group","lookups","","call","fc","one","blocks","lookups","slow","","cause","nn","killed","","guava","ability","refresh","expired","keys","completely","background","","first","thread","hits","expired","key","schedules","background","cache","reload","","still","returns","old","value","","cache","eventually","updated","","patch","introduces","background","reload","feature","","two","new","parameters","","","","","hadoop","security","groups","cache","background","reload","","","default","false","keep","current","behaviour","","set","true","enable","small","thread","pool","background","refresh","expired","keys","","","","hadoop","security","groups","cache","background","reload","threads","","","relevant","set","true","","controls","many","threads","background","refresh","pool","","default","","","","likely","enough","reload","cached","groups","background","expiry"],"features":{"type":0,"size":1000,"indices":[20,25,36,44,52,76,77,82,83,89,91,92,100,101,109,113,115,118,132,137,138,139,140,146,158,159,163,170,181,183,188,203,208,213,222,229,231,234,237,252,255,258,280,281,299,321,329,332,333,352,355,364,372,373,381,388,389,396,400,406,408,410,420,445,450,475,490,495,511,518,532,539,541,553,569,580,586,591,592,594,597,598,605,609,623,634,650,656,658,667,672,674,683,694,698,707,710,713,722,729,736,737,742,745,747,750,752,753,756,759,760,768,770,773,800,805,813,817,831,833,856,860,867,880,882,883,899,909,911,920,921,967,971,997],"values":[3.0,1.0,3.0,1.0,5.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,1.0,3.0,2.0,3.0,4.0,2.0,1.0,9.0,8.0,3.0,4.0,4.0,5.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,1.0,54.0,4.0,3.0,14.0,1.0,9.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,4.0,1.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,29.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,2.0,6.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":18}
{"_c0":"In ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used  It would be nice to detect this pattern and re use intermediate values","_c1":"Optimize sequential projections","document":"In ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used  It would be nice to detect this pattern and re use intermediate values Optimize sequential projections","words":["in","ml","pipelines","","each","transformer","estimator","appends","new","columns","to","the","input","dataframe","","for","example","","it","might","produce","dataframes","like","the","following","columns","","a","","b","","c","","d","","where","a","is","from","raw","input","","b","","","udf","b","a","","","c","","","udf","c","b","","","and","d","","","udf","d","c","","","some","udfs","could","be","expensive","","however","","if","we","materialize","c","and","d","","udf","b","","and","udf","c","are","triggered","twice","","i","e","","","value","c","is","not","re","used","","it","would","be","nice","to","detect","this","pattern","and","re","use","intermediate","values","optimize","sequential","projections"],"filtered":["ml","pipelines","","transformer","estimator","appends","new","columns","input","dataframe","","example","","might","produce","dataframes","like","following","columns","","","b","","c","","d","","raw","input","","b","","","udf","b","","","c","","","udf","c","b","","","d","","","udf","d","c","","","udfs","expensive","","however","","materialize","c","d","","udf","b","","udf","c","triggered","twice","","e","","","value","c","re","used","","nice","detect","pattern","re","use","intermediate","values","optimize","sequential","projections"],"features":{"type":0,"size":1000,"indices":[0,18,25,36,62,86,91,94,138,139,161,162,163,170,189,213,230,243,273,281,315,324,327,329,330,333,361,366,370,372,373,388,400,422,425,445,483,489,495,506,508,551,605,617,626,656,660,673,693,710,722,768,843,878,885,921,922,969,985,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,4.0,5.0,1.0,1.0,29.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":1}
{"_c0":"In SPARK       we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container  cc    cloud fan    yhuai","_c1":"VectorUDT MatrixUDT should take primitive arrays without boxing","document":"In SPARK       we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container  cc    cloud fan    yhuai VectorUDT MatrixUDT should take primitive arrays without boxing","words":["in","spark","","","","","","","we","switched","to","use","genericarraydata","to","store","indices","and","values","in","vector","matrix","udts","","however","","genericarraydata","is","not","specialized","for","primitive","types","","this","might","hurt","mllib","performance","badly","","we","should","consider","either","specialize","genericarraydata","or","use","a","different","container","","cc","","","","cloud","fan","","","","yhuai","vectorudt","matrixudt","should","take","primitive","arrays","without","boxing"],"filtered":["spark","","","","","","","switched","use","genericarraydata","store","indices","values","vector","matrix","udts","","however","","genericarraydata","specialized","primitive","types","","might","hurt","mllib","performance","badly","","consider","either","specialize","genericarraydata","use","different","container","","cc","","","","cloud","fan","","","","yhuai","vectorudt","matrixudt","take","primitive","arrays","without","boxing"],"features":{"type":0,"size":1000,"indices":[18,36,58,86,89,91,105,109,165,170,187,227,230,281,283,312,333,352,372,373,388,445,465,472,489,500,514,521,563,572,605,665,673,734,742,743,759,822,855,884,903,950,982,985,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,17.0,1.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0]},"cluster_label":17}
{"_c0":"In Spark       DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset  in Java  This is a source compatible change","_c1":"Accept Dataset    instead of DataFrame in MLlib APIs","document":"In Spark       DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset  in Java  This is a source compatible change Accept Dataset    instead of DataFrame in MLlib APIs","words":["in","spark","","","","","","","dataframe","","is","an","alias","of","","dataset","row","","","","mllib","api","actually","works","for","other","types","of","","dataset","","","so","we","should","accept","","dataset","","","","","instead","","it","maps","to","","dataset","","in","java","","this","is","a","source","compatible","change","accept","dataset","","","","instead","of","dataframe","in","mllib","apis"],"filtered":["spark","","","","","","","dataframe","","alias","","dataset","row","","","","mllib","api","actually","works","types","","dataset","","","accept","","dataset","","","","","instead","","maps","","dataset","","java","","source","compatible","change","accept","dataset","","","","instead","dataframe","mllib","apis"],"features":{"type":0,"size":1000,"indices":[36,70,105,158,161,170,220,281,287,343,368,372,373,388,445,447,465,476,493,495,521,644,665,674,752,788,842,846,863,920,967,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,26.0,1.0,1.0,3.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"In Spark     we shouldn t have two parsers anymore  There should be only a single one","_c1":"Merge HiveSqlAstBuilder and SparkSqlAstBuilder","document":"In Spark     we shouldn t have two parsers anymore  There should be only a single one Merge HiveSqlAstBuilder and SparkSqlAstBuilder","words":["in","spark","","","","","we","shouldn","t","have","two","parsers","anymore","","there","should","be","only","a","single","one","merge","hivesqlastbuilder","and","sparksqlastbuilder"],"filtered":["spark","","","","","shouldn","two","parsers","anymore","","single","one","merge","hivesqlastbuilder","sparksqlastbuilder"],"features":{"type":0,"size":1000,"indices":[20,44,105,170,299,333,372,408,445,472,531,597,656,665,667,773,777,831,850,899,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"In SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well","_c1":"Support formula in spark kmeans in SparkR","document":"In SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well Support formula in spark kmeans in SparkR","words":["in","sparkr","","spark","kmeans","take","a","dataframe","with","double","columns","","this","is","different","from","other","ml","methods","we","implemented","","which","support","r","model","formula","","we","should","add","support","for","that","as","well","support","formula","in","spark","kmeans","in","sparkr"],"filtered":["sparkr","","spark","kmeans","take","dataframe","double","columns","","different","ml","methods","implemented","","support","r","model","formula","","add","support","well","support","formula","spark","kmeans","sparkr"],"features":{"type":0,"size":1000,"indices":[36,89,100,105,129,157,161,170,177,281,305,324,372,373,432,445,570,572,573,597,650,665,674,695,760,767,855,857,921,969,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"In an effort to simplify the  in the capacity scheduler  We would reintroduce this  possibly with some revisions to the original design  after a while  This will be an incompatible change  Any objections","_c1":"Remove pre emption from the capacity scheduler code base","document":"In an effort to simplify the  in the capacity scheduler  We would reintroduce this  possibly with some revisions to the original design  after a while  This will be an incompatible change  Any objections Remove pre emption from the capacity scheduler code base","words":["in","an","effort","to","simplify","the","","in","the","capacity","scheduler","","we","would","reintroduce","this","","possibly","with","some","revisions","to","the","original","design","","after","a","while","","this","will","be","an","incompatible","change","","any","objections","remove","pre","emption","from","the","capacity","scheduler","code","base"],"filtered":["effort","simplify","","capacity","scheduler","","reintroduce","","possibly","revisions","original","design","","","incompatible","change","","objections","remove","pre","emption","capacity","scheduler","code","base"],"features":{"type":0,"size":1000,"indices":[19,77,78,91,126,158,163,170,173,189,288,348,372,373,388,399,400,420,445,468,650,656,706,707,710,752,803,853,921,943,978,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0]},"cluster_label":2}
{"_c0":"In codegen  we didn t consider nullability of expressions  Once considering this  we can avoid lots of null check  reduce the size of generated code  also improve performance   Before that  we should double check the correctness of nullablity of all expressions and schema  or we will hit NPE or wrong results","_c1":"Consider nullability of expression in codegen","document":"In codegen  we didn t consider nullability of expressions  Once considering this  we can avoid lots of null check  reduce the size of generated code  also improve performance   Before that  we should double check the correctness of nullablity of all expressions and schema  or we will hit NPE or wrong results Consider nullability of expression in codegen","words":["in","codegen","","we","didn","t","consider","nullability","of","expressions","","once","considering","this","","we","can","avoid","lots","of","null","check","","reduce","the","size","of","generated","code","","also","improve","performance","","","before","that","","we","should","double","check","the","correctness","of","nullablity","of","all","expressions","and","schema","","or","we","will","hit","npe","or","wrong","results","consider","nullability","of","expression","in","codegen"],"filtered":["codegen","","didn","consider","nullability","expressions","","considering","","avoid","lots","null","check","","reduce","size","generated","code","","also","improve","performance","","","","double","check","correctness","nullablity","expressions","schema","","hit","npe","wrong","results","consider","nullability","expression","codegen"],"features":{"type":0,"size":1000,"indices":[100,109,141,147,159,187,192,193,263,312,315,333,343,363,372,373,383,420,434,441,445,478,479,502,522,560,609,665,710,718,759,760,777,792,804,833,863,882,968,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,6.0,1.0,9.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,4.0]},"cluster_label":2}
{"_c0":"In general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations","_c1":"Remove some internal classes  dependency on SQLContext","document":"In general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations Remove some internal classes  dependency on SQLContext","words":["in","general","it","is","better","for","internal","classes","to","not","depend","on","the","external","class","","in","this","case","sqlcontext","","to","reduce","coupling","between","user","facing","apis","and","the","internal","implementations","remove","some","internal","classes","","dependency","on","sqlcontext"],"filtered":["general","better","internal","classes","depend","external","class","","case","sqlcontext","","reduce","coupling","user","facing","apis","internal","implementations","remove","internal","classes","","dependency","sqlcontext"],"features":{"type":0,"size":1000,"indices":[18,36,82,130,273,281,288,295,333,342,372,373,388,400,416,445,451,481,482,495,502,534,586,588,710,809,842,882,925,941],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"In ipc Client  the underlying mechanism is already supporting asynchronous calls    the calls shares a connection  the call requests are sent using a thread pool and the responses can be out of order  Indeed  synchronous call is implemented by invoking wait   in the caller thread in order to wait for the server response  In this JIRA  we change ipc Client to support asynchronous mode  In asynchronous mode  it return once the request has been sent out but not wait for the response from the server","_c1":"Change ipc Client to support asynchronous calls","document":"In ipc Client  the underlying mechanism is already supporting asynchronous calls    the calls shares a connection  the call requests are sent using a thread pool and the responses can be out of order  Indeed  synchronous call is implemented by invoking wait   in the caller thread in order to wait for the server response  In this JIRA  we change ipc Client to support asynchronous mode  In asynchronous mode  it return once the request has been sent out but not wait for the response from the server Change ipc Client to support asynchronous calls","words":["in","ipc","client","","the","underlying","mechanism","is","already","supporting","asynchronous","calls","","","","the","calls","shares","a","connection","","the","call","requests","are","sent","using","a","thread","pool","and","the","responses","can","be","out","of","order","","indeed","","synchronous","call","is","implemented","by","invoking","wait","","","in","the","caller","thread","in","order","to","wait","for","the","server","response","","in","this","jira","","we","change","ipc","client","to","support","asynchronous","mode","","in","asynchronous","mode","","it","return","once","the","request","has","been","sent","out","but","not","wait","for","the","response","from","the","server","change","ipc","client","to","support","asynchronous","calls"],"filtered":["ipc","client","","underlying","mechanism","already","supporting","asynchronous","calls","","","","calls","shares","connection","","call","requests","sent","using","thread","pool","responses","order","","indeed","","synchronous","call","implemented","invoking","wait","","","caller","thread","order","wait","server","response","","jira","","change","ipc","client","support","asynchronous","mode","","asynchronous","mode","","return","request","sent","wait","response","server","change","ipc","client","support","asynchronous","calls"],"features":{"type":0,"size":1000,"indices":[18,20,36,57,71,83,91,92,117,118,135,138,146,147,158,170,177,223,232,281,317,333,343,372,373,388,411,445,477,483,495,496,503,535,580,590,624,654,656,685,695,710,718,775,805,821,833,860,866,921,945,957,993],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,13.0,1.0,3.0,2.0,7.0,3.0,3.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,9.0,2.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"In logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK        Here is a motivating example","_c1":"Optimize SerializeFromObject for primitive array","document":"In logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK        Here is a motivating example Optimize SerializeFromObject for primitive array","words":["in","logical","plan","","","","serializefromobject","","","for","an","array","always","use","","","genericarraydata","","","as","a","destination","","","","unsafearraydata","","","could","be","used","for","an","primitive","array","","this","is","a","simple","approach","to","solve","issues","that","are","addressed","by","spark","","","","","","","","here","is","a","motivating","example","optimize","serializefromobject","for","primitive","array"],"filtered":["logical","plan","","","","serializefromobject","","","array","always","use","","","genericarraydata","","","destination","","","","unsafearraydata","","","used","primitive","array","","simple","approach","solve","issues","addressed","spark","","","","","","","","motivating","example","optimize","serializefromobject","primitive","array"],"features":{"type":0,"size":1000,"indices":[13,25,36,95,105,114,123,135,138,150,170,213,223,243,247,281,372,373,388,445,475,489,499,500,531,551,572,605,656,706,752,760,824,980,982],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,22.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"In many modeling application  data points are not necessarily sampled with equal probabilities  Linear regression should support weighting which account the over or under sampling","_c1":"LinearRegression should supported weighted data","document":"In many modeling application  data points are not necessarily sampled with equal probabilities  Linear regression should support weighting which account the over or under sampling LinearRegression should supported weighted data","words":["in","many","modeling","application","","data","points","are","not","necessarily","sampled","with","equal","probabilities","","linear","regression","should","support","weighting","which","account","the","over","or","under","sampling","linearregression","should","supported","weighted","data"],"filtered":["many","modeling","application","","data","points","necessarily","sampled","equal","probabilities","","linear","regression","support","weighting","account","sampling","linearregression","supported","weighted","data"],"features":{"type":0,"size":1000,"indices":[18,39,116,138,141,169,187,188,248,255,259,263,313,329,352,372,402,445,593,597,639,650,665,695,710,813,927],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"In naive Bayes  we expect inputs to be individual observations  In practice  people may have the frequency table instead  It is useful for us to support instance weights to handle this case","_c1":"Support weighted instances in naive Bayes","document":"In naive Bayes  we expect inputs to be individual observations  In practice  people may have the frequency table instead  It is useful for us to support instance weights to handle this case Support weighted instances in naive Bayes","words":["in","naive","bayes","","we","expect","inputs","to","be","individual","observations","","in","practice","","people","may","have","the","frequency","table","instead","","it","is","useful","for","us","to","support","instance","weights","to","handle","this","case","support","weighted","instances","in","naive","bayes"],"filtered":["naive","bayes","","expect","inputs","individual","observations","","practice","","people","may","frequency","table","instead","","useful","us","support","instance","weights","handle","case","support","weighted","instances","naive","bayes"],"features":{"type":0,"size":1000,"indices":[36,56,189,208,248,272,280,281,292,299,342,359,372,373,385,388,445,483,486,495,591,609,656,666,695,710,796,837,863,960,977,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days","_c1":"Rolling mechanism for demux output","document":"In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days Rolling mechanism for demux output","words":["in","order","to","reduce","the","number","of","file","on","hdfs","we","need","to","have","a","rolling","mechanism","for","the","demux","output","","","avoid","immediate","merging","if","there","s","already","file","for","the","same","time","range","","create","a","spill","file","instead","","","merge","all","raw","files","every","hours","","","merge","all","hourly","files","every","days","rolling","mechanism","for","demux","output"],"filtered":["order","reduce","number","file","hdfs","need","rolling","mechanism","demux","output","","","avoid","immediate","merging","already","file","time","range","","create","spill","file","instead","","","merge","raw","files","every","hours","","","merge","hourly","files","every","days","rolling","mechanism","demux","output"],"features":{"type":0,"size":1000,"indices":[36,57,82,87,108,109,122,144,157,170,188,197,265,299,312,343,372,388,445,502,537,551,583,590,617,656,710,718,721,773,831,845,858,863,936,945,967,968,993],"values":[3.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"In order to upgrade to Kryo    we need to shade Kryo in our custom Hive       fork","_c1":"Shade Kryo in our custom Hive       fork","document":"In order to upgrade to Kryo    we need to shade Kryo in our custom Hive       fork Shade Kryo in our custom Hive       fork","words":["in","order","to","upgrade","to","kryo","","","","we","need","to","shade","kryo","in","our","custom","hive","","","","","","","fork","shade","kryo","in","our","custom","hive","","","","","","","fork"],"filtered":["order","upgrade","kryo","","","","need","shade","kryo","custom","hive","","","","","","","fork","shade","kryo","custom","hive","","","","","","","fork"],"features":{"type":0,"size":1000,"indices":[82,206,242,346,372,388,445,537,599,704,718,763,993],"values":[3.0,2.0,1.0,2.0,15.0,3.0,3.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"In the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and removed","_c1":"When the FileTailingAdaptor is unable to read a file it should take action instead of trying     times","document":"In the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and removed When the FileTailingAdaptor is unable to read a file it should take action instead of trying     times","words":["in","the","filetailingadaptor","","if","when","trying","to","read","a","file","","a","","file","does","not","exist","","","","","permission","denied","","exception","is","throws","then","that","file","should","be","log","and","removed","when","the","filetailingadaptor","is","unable","to","read","a","file","it","should","take","action","instead","of","trying","","","","","times"],"filtered":["filetailingadaptor","","trying","read","file","","","file","exist","","","","","permission","denied","","exception","throws","file","log","removed","filetailingadaptor","unable","read","file","take","action","instead","trying","","","","","times"],"features":{"type":0,"size":1000,"indices":[18,58,76,108,144,170,209,213,265,269,281,333,343,372,381,388,430,445,495,512,593,631,650,656,665,674,698,710,760,840,855,863],"values":[1.0,2.0,2.0,4.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,12.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"In the existing code  there are three layers of serialization involved in sending a task from the scheduler to an executor    A Task object is serialized   The Task object is copied to a byte buffer that also contains serialized information about any additional JARs  files  and Properties needed for the task to execute  This byte buffer is stored as the member variable serializedTask in the TaskDescription class    The TaskDescription is serialized  in addition to the serialized task   JARs  the TaskDescription class contains the task ID and other metadata  and sent in a LaunchTask message  While it is necessary to have two layers of serialization  so that the JAR  file  and Property info can be deserialized prior to deserializing the Task object  the third layer of deserialization is unnecessary  this is as a result of SPARK        We should eliminate a layer of serialization by moving the JARs  files  and Properties into the TaskDescription class","_c1":"taskScheduler has some unneeded serialization","document":"In the existing code  there are three layers of serialization involved in sending a task from the scheduler to an executor    A Task object is serialized   The Task object is copied to a byte buffer that also contains serialized information about any additional JARs  files  and Properties needed for the task to execute  This byte buffer is stored as the member variable serializedTask in the TaskDescription class    The TaskDescription is serialized  in addition to the serialized task   JARs  the TaskDescription class contains the task ID and other metadata  and sent in a LaunchTask message  While it is necessary to have two layers of serialization  so that the JAR  file  and Property info can be deserialized prior to deserializing the Task object  the third layer of deserialization is unnecessary  this is as a result of SPARK        We should eliminate a layer of serialization by moving the JARs  files  and Properties into the TaskDescription class taskScheduler has some unneeded serialization","words":["in","the","existing","code","","there","are","three","layers","of","serialization","involved","in","sending","a","task","from","the","scheduler","to","an","executor","","","","a","task","object","is","serialized","","","the","task","object","is","copied","to","a","byte","buffer","that","also","contains","serialized","information","about","any","additional","jars","","files","","and","properties","needed","for","the","task","to","execute","","this","byte","buffer","is","stored","as","the","member","variable","serializedtask","in","the","taskdescription","class","","","","the","taskdescription","is","serialized","","in","addition","to","the","serialized","task","","","jars","","the","taskdescription","class","contains","the","task","id","and","other","metadata","","and","sent","in","a","launchtask","message","","while","it","is","necessary","to","have","two","layers","of","serialization","","so","that","the","jar","","file","","and","property","info","can","be","deserialized","prior","to","deserializing","the","task","object","","the","third","layer","of","deserialization","is","unnecessary","","this","is","as","a","result","of","spark","","","","","","","","we","should","eliminate","a","layer","of","serialization","by","moving","the","jars","","files","","and","properties","into","the","taskdescription","class","taskscheduler","has","some","unneeded","serialization"],"filtered":["existing","code","","three","layers","serialization","involved","sending","task","scheduler","executor","","","","task","object","serialized","","","task","object","copied","byte","buffer","also","contains","serialized","information","additional","jars","","files","","properties","needed","task","execute","","byte","buffer","stored","member","variable","serializedtask","taskdescription","class","","","","taskdescription","serialized","","addition","serialized","task","","","jars","","taskdescription","class","contains","task","id","metadata","","sent","launchtask","message","","necessary","two","layers","serialization","","jar","","file","","property","info","deserialized","prior","deserializing","task","object","","third","layer","deserialization","unnecessary","","result","spark","","","","","","","","eliminate","layer","serialization","moving","jars","","files","","properties","taskdescription","class","taskscheduler","unneeded","serialization"],"features":{"type":0,"size":1000,"indices":[21,36,48,91,105,108,110,117,138,141,146,159,163,168,170,175,181,223,227,242,244,245,249,252,281,299,311,333,343,349,368,371,372,373,381,388,400,408,415,420,445,451,495,513,520,533,534,551,572,580,587,603,608,609,620,650,653,656,665,670,674,697,707,710,727,733,736,752,760,777,781,792,831,833,843,865,891,909,921,963,978,980,981,993],"values":[2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,6.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,5.0,5.0,2.0,1.0,1.0,32.0,2.0,1.0,6.0,1.0,1.0,2.0,1.0,5.0,7.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,15.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"In the spirit of SPARK     we should clean up the use of SPARK HOME and  if possible  remove it entirely  We need to look through what this is used for  One use was allowing applications to run different versions of Spark in standalone mode  For instance  someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark  This use case is not widely used and maybe should just be removed  The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose  If there are other legitimate reasons for SPARK HOME  we can keep it around    we need to audit the uses of it","_c1":"Clean up and clarify use of SPARK HOME","document":"In the spirit of SPARK     we should clean up the use of SPARK HOME and  if possible  remove it entirely  We need to look through what this is used for  One use was allowing applications to run different versions of Spark in standalone mode  For instance  someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark  This use case is not widely used and maybe should just be removed  The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose  If there are other legitimate reasons for SPARK HOME  we can keep it around    we need to audit the uses of it Clean up and clarify use of SPARK HOME","words":["in","the","spirit","of","spark","","","","","we","should","clean","up","the","use","of","spark","home","and","","if","possible","","remove","it","entirely","","we","need","to","look","through","what","this","is","used","for","","one","use","was","allowing","applications","to","run","different","versions","of","spark","in","standalone","mode","","for","instance","","someone","could","submit","an","application","with","a","custom","spark","home","and","the","worker","would","launch","an","executor","using","a","different","path","for","spark","","this","use","case","is","not","widely","used","and","maybe","should","just","be","removed","","the","existing","constructors","that","take","spark","home","for","this","purpose","should","be","deprecated","and","we","should","explain","that","spark","home","is","no","longer","used","for","this","purpose","","if","there","are","other","legitimate","reasons","for","spark","home","","we","can","keep","it","around","","","","we","need","to","audit","the","uses","of","it","clean","up","and","clarify","use","of","spark","home"],"filtered":["spirit","spark","","","","","clean","use","spark","home","","possible","","remove","entirely","","need","look","used","","one","use","allowing","applications","run","different","versions","spark","standalone","mode","","instance","","someone","submit","application","custom","spark","home","worker","launch","executor","using","different","path","spark","","use","case","widely","used","maybe","removed","","existing","constructors","take","spark","home","purpose","deprecated","explain","spark","home","longer","used","purpose","","legitimate","reasons","spark","home","","keep","around","","","","need","audit","uses","clean","clarify","use","spark","home"],"features":{"type":0,"size":1000,"indices":[1,2,18,26,36,44,89,100,105,111,128,138,140,141,163,164,169,170,213,232,234,235,281,288,307,333,340,342,343,346,364,365,371,372,373,388,399,413,445,489,495,512,526,533,537,543,590,594,599,605,609,610,620,624,650,656,665,668,671,674,710,737,742,750,752,760,775,831,833,855,866,868,878,939,970,993],"values":[1.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,9.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,5.0,2.0,1.0,5.0,2.0,1.0,6.0,1.0,17.0,4.0,3.0,1.0,1.0,2.0,4.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0]},"cluster_label":11}
{"_c0":"Inline tables currently do not support SQL generation  and as a result a view that depends on inline tables would fail","_c1":"Support SQL generation for inline tables","document":"Inline tables currently do not support SQL generation  and as a result a view that depends on inline tables would fail Support SQL generation for inline tables","words":["inline","tables","currently","do","not","support","sql","generation","","and","as","a","result","a","view","that","depends","on","inline","tables","would","fail","support","sql","generation","for","inline","tables"],"filtered":["inline","tables","currently","support","sql","generation","","result","view","depends","inline","tables","fail","support","sql","generation","inline","tables"],"features":{"type":0,"size":1000,"indices":[18,36,82,117,163,170,293,298,333,372,488,532,534,572,593,686,695,760,763,865],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Instead of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer  This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a     megabyte serialized block may actually consume     megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted  This change is also a prerequisite to being able to cache blocks which are larger than  GB  although full support for that depends on several other changes which have not bee implemented yet","_c1":"Store serialized blocks as multiple chunks in MemoryStore","document":"Instead of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer  This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a     megabyte serialized block may actually consume     megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted  This change is also a prerequisite to being able to cache blocks which are larger than  GB  although full support for that depends on several other changes which have not bee implemented yet Store serialized blocks as multiple chunks in MemoryStore","words":["instead","of","storing","serialized","blocks","in","individual","bytebuffers","","the","blockmanager","should","be","capable","of","storing","a","serialized","block","in","multiple","chunks","","each","occupying","a","separate","bytebuffer","","this","change","will","help","to","improve","the","efficiency","of","memory","allocation","and","the","accuracy","of","memory","accounting","when","serializing","blocks","","our","current","serialization","code","uses","a","","","bytebufferoutputstream","","","","which","doubles","and","re","allocates","its","backing","byte","array","","this","increases","the","peak","memory","requirements","during","serialization","","since","we","need","to","hold","extra","memory","while","expanding","the","array","","","in","addition","","we","currently","don","t","account","for","the","extra","wasted","space","at","the","end","of","the","bytebuffer","s","backing","array","","so","a","","","","","megabyte","serialized","block","may","actually","consume","","","","","megabytes","of","memory","","after","switching","to","storing","blocks","in","multiple","chunks","","we","ll","be","able","to","efficiently","trim","the","backing","buffers","so","that","no","space","is","wasted","","this","change","is","also","a","prerequisite","to","being","able","to","cache","blocks","which","are","larger","than","","gb","","although","full","support","for","that","depends","on","several","other","changes","which","have","not","bee","implemented","yet","store","serialized","blocks","as","multiple","chunks","in","memorystore"],"filtered":["instead","storing","serialized","blocks","individual","bytebuffers","","blockmanager","capable","storing","serialized","block","multiple","chunks","","occupying","separate","bytebuffer","","change","help","improve","efficiency","memory","allocation","accuracy","memory","accounting","serializing","blocks","","current","serialization","code","uses","","","bytebufferoutputstream","","","","doubles","re","allocates","backing","byte","array","","increases","peak","memory","requirements","serialization","","since","need","hold","extra","memory","expanding","array","","","addition","","currently","account","extra","wasted","space","end","bytebuffer","backing","array","","","","","","megabyte","serialized","block","may","actually","consume","","","","","megabytes","memory","","switching","storing","blocks","multiple","chunks","","ll","able","efficiently","trim","backing","buffers","space","wasted","","change","also","prerequisite","able","cache","blocks","larger","","gb","","although","full","support","depends","several","changes","bee","implemented","yet","store","serialized","blocks","multiple","chunks","memorystore"],"features":{"type":0,"size":1000,"indices":[3,8,18,20,36,47,76,77,80,82,92,111,138,147,148,158,163,165,168,170,177,197,204,207,261,266,277,281,284,296,298,299,304,333,343,346,347,348,363,368,372,373,374,379,385,388,389,404,420,425,445,447,455,481,496,498,511,522,537,547,564,567,572,584,585,591,592,597,612,653,656,660,665,666,669,674,695,704,706,707,710,718,743,756,760,763,777,779,787,788,792,813,822,843,860,863,885,897,919,961,963,976,989,993],"values":[1.0,2.0,1.0,5.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,1.0,1.0,1.0,2.0,28.0,3.0,1.0,1.0,1.0,6.0,1.0,3.0,2.0,1.0,5.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,3.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,4.0]},"cluster_label":1}
{"_c0":"Interface method   FileFormat prepareRead     was added in  PR        https   github com apache spark pull        to handle a special case in the LibSVM data source  However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean","_c1":"Remove FileFormat prepareRead","document":"Interface method   FileFormat prepareRead     was added in  PR        https   github com apache spark pull        to handle a special case in the LibSVM data source  However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean Remove FileFormat prepareRead","words":["interface","method","","","fileformat","prepareread","","","","","was","added","in","","pr","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","to","handle","a","special","case","in","the","libsvm","data","source","","however","","the","semantics","of","this","interface","method","isn","t","intuitive","","it","returns","a","modified","version","of","the","data","source","options","map","","considering","that","the","libsvm","case","can","be","easily","handled","using","schema","metadata","inside","","","inferschema","","","","we","can","remove","this","interface","method","to","keep","the","","","fileformat","","","interface","clean","remove","fileformat","prepareread"],"filtered":["interface","method","","","fileformat","prepareread","","","","","added","","pr","","","","","","","","https","","","github","com","apache","spark","pull","","","","","","","","handle","special","case","libsvm","data","source","","however","","semantics","interface","method","isn","intuitive","","returns","modified","version","data","source","options","map","","considering","libsvm","case","easily","handled","using","schema","metadata","inside","","","inferschema","","","","remove","interface","method","keep","","","fileformat","","","interface","clean","remove","fileformat","prepareread"],"features":{"type":0,"size":1000,"indices":[50,56,70,105,170,202,208,221,234,237,288,340,342,343,362,372,373,384,388,441,445,453,463,495,510,553,556,594,597,607,609,616,624,651,654,656,673,695,710,712,760,766,777,817,826,833,993,995,998],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,3.0,3.0,36.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"IsNotNull   filter is not being pushed down for JDBC datasource  It looks it is SQL standard according to SQL     SQL       SQL      and SQL    x and I believe most databases support this","_c1":"isnotnull operator not pushed down for JDBC datasource","document":"IsNotNull   filter is not being pushed down for JDBC datasource  It looks it is SQL standard according to SQL     SQL       SQL      and SQL    x and I believe most databases support this isnotnull operator not pushed down for JDBC datasource","words":["isnotnull","","","filter","is","not","being","pushed","down","for","jdbc","datasource","","it","looks","it","is","sql","standard","according","to","sql","","","","","sql","","","","","","","sql","","","","","","and","sql","","","","x","and","i","believe","most","databases","support","this","isnotnull","operator","not","pushed","down","for","jdbc","datasource"],"filtered":["isnotnull","","","filter","pushed","jdbc","datasource","","looks","sql","standard","according","sql","","","","","sql","","","","","","","sql","","","","","","sql","","","","x","believe","databases","support","isnotnull","operator","pushed","jdbc","datasource"],"features":{"type":0,"size":1000,"indices":[18,36,141,173,199,217,281,326,329,333,372,373,374,388,436,495,512,640,686,695,770,790,810,849,967,986],"values":[2.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,2.0,21.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"It is a big change  but it lets us use the type information to prevent accidentally passing internal types to external types","_c1":"Remove InternalRow s inheritance from Row","document":"It is a big change  but it lets us use the type information to prevent accidentally passing internal types to external types Remove InternalRow s inheritance from Row","words":["it","is","a","big","change","","but","it","lets","us","use","the","type","information","to","prevent","accidentally","passing","internal","types","to","external","types","remove","internalrow","s","inheritance","from","row"],"filtered":["big","change","","lets","us","use","type","information","prevent","accidentally","passing","internal","types","external","types","remove","internalrow","inheritance","row"],"features":{"type":0,"size":1000,"indices":[83,94,97,158,170,197,208,263,281,288,295,372,388,403,438,465,489,495,526,586,598,710,720,788,921,978],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"It is a very common shell pattern in   x to effectively replace sub project specific vars with generics  We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated  Additionally  we should use this shell function to deprecate the shell vars that are holdovers already","_c1":"Deprecate shell vars","document":"It is a very common shell pattern in   x to effectively replace sub project specific vars with generics  We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated  Additionally  we should use this shell function to deprecate the shell vars that are holdovers already Deprecate shell vars","words":["it","is","a","very","common","shell","pattern","in","","","x","to","effectively","replace","sub","project","specific","vars","with","generics","","we","should","have","a","function","that","does","this","replacement","and","provides","a","warning","to","the","end","user","that","the","old","shell","var","is","deprecated","","additionally","","we","should","use","this","shell","function","to","deprecate","the","shell","vars","that","are","holdovers","already","deprecate","shell","vars"],"filtered":["common","shell","pattern","","","x","effectively","replace","sub","project","specific","vars","generics","","function","replacement","provides","warning","end","user","old","shell","var","deprecated","","additionally","","use","shell","function","deprecate","shell","vars","holdovers","already","deprecate","shell","vars"],"features":{"type":0,"size":1000,"indices":[57,59,120,123,138,170,239,275,281,284,299,313,333,340,344,372,373,388,422,445,489,495,505,537,546,619,620,650,665,671,672,698,710,760,787,810,882,944,954,964,993],"values":[1.0,1.0,1.0,5.0,1.0,3.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"It is unnecessary and makes the type hierarchy slightly more complicated than needed","_c1":"Remove ExtractValueWithOrdinal abstract class","document":"It is unnecessary and makes the type hierarchy slightly more complicated than needed Remove ExtractValueWithOrdinal abstract class","words":["it","is","unnecessary","and","makes","the","type","hierarchy","slightly","more","complicated","than","needed","remove","extractvaluewithordinal","abstract","class"],"filtered":["unnecessary","makes","type","hierarchy","slightly","complicated","needed","remove","extractvaluewithordinal","abstract","class"],"features":{"type":0,"size":1000,"indices":[1,242,244,261,281,288,333,495,526,534,629,676,691,710,774,865,983],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"It looks like the head master branch of Spark uses quite an old version of Jetty         v         There have been some announcement of security vulnerabilities  notably in      and there are versions of both   and   that address those  We recently left a web ui port open and had the server compromised within days  Albeit  this upgrade shouldn t be the only security improvement made  the current version is clearly vulnerable  as is","_c1":"Upgrade Jetty to latest version of","document":"It looks like the head master branch of Spark uses quite an old version of Jetty         v         There have been some announcement of security vulnerabilities  notably in      and there are versions of both   and   that address those  We recently left a web ui port open and had the server compromised within days  Albeit  this upgrade shouldn t be the only security improvement made  the current version is clearly vulnerable  as is Upgrade Jetty to latest version of","words":["it","looks","like","the","head","master","branch","of","spark","uses","quite","an","old","version","of","jetty","","","","","","","","","v","","","","","","","","","there","have","been","some","announcement","of","security","vulnerabilities","","notably","in","","","","","","and","there","are","versions","of","both","","","and","","","that","address","those","","we","recently","left","a","web","ui","port","open","and","had","the","server","compromised","within","days","","albeit","","this","upgrade","shouldn","t","be","the","only","security","improvement","made","","the","current","version","is","clearly","vulnerable","","as","is","upgrade","jetty","to","latest","version","of"],"filtered":["looks","like","head","master","branch","spark","uses","quite","old","version","jetty","","","","","","","","","v","","","","","","","","","announcement","security","vulnerabilities","","notably","","","","","","versions","","","","","address","","recently","left","web","ui","port","open","server","compromised","within","days","","albeit","","upgrade","shouldn","security","improvement","made","","current","version","clearly","vulnerable","","upgrade","jetty","latest","version"],"features":{"type":0,"size":1000,"indices":[34,64,105,111,138,170,173,242,255,270,281,299,327,330,333,343,344,372,373,388,400,404,411,412,424,445,453,477,492,495,498,535,572,575,600,608,626,634,656,662,667,671,672,697,710,752,760,764,777,783,830,831,858,863,899,954,987,993,995,996],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,5.0,1.0,31.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0]},"cluster_label":1}
{"_c0":"It seems  EqualNullSafe  filter was missed for batch pruneing partitions in cached tables  Supporting this improve the performance roughly       it will vary   Running the codes below","_c1":"Support partition batch pruning with     EqualNullSafe  predicate in InMemoryTableScanExec","document":"It seems  EqualNullSafe  filter was missed for batch pruneing partitions in cached tables  Supporting this improve the performance roughly       it will vary   Running the codes below Support partition batch pruning with     EqualNullSafe  predicate in InMemoryTableScanExec","words":["it","seems","","equalnullsafe","","filter","was","missed","for","batch","pruneing","partitions","in","cached","tables","","supporting","this","improve","the","performance","roughly","","","","","","","it","will","vary","","","running","the","codes","below","support","partition","batch","pruning","with","","","","","equalnullsafe","","predicate","in","inmemorytablescanexec"],"filtered":["seems","","equalnullsafe","","filter","missed","batch","pruneing","partitions","cached","tables","","supporting","improve","performance","roughly","","","","","","","vary","","","running","codes","support","partition","batch","pruning","","","","","equalnullsafe","","predicate","inmemorytablescanexec"],"features":{"type":0,"size":1000,"indices":[36,50,103,108,234,240,250,372,373,420,445,450,456,457,481,495,496,497,522,593,607,640,650,695,710,759,830,856,872,910,963],"values":[1.0,1.0,2.0,2.0,1.0,1.0,1.0,16.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"It will be great to have these SQL functions  IFNULL  NULLIF  NVL  NVL  The meaning of these functions could be found in oracle docs","_c1":"SQL function  IFNULL  NULLIF  NVL and NVL","document":"It will be great to have these SQL functions  IFNULL  NULLIF  NVL  NVL  The meaning of these functions could be found in oracle docs SQL function  IFNULL  NULLIF  NVL and NVL","words":["it","will","be","great","to","have","these","sql","functions","","ifnull","","nullif","","nvl","","nvl","","the","meaning","of","these","functions","could","be","found","in","oracle","docs","sql","function","","ifnull","","nullif","","nvl","and","nvl"],"filtered":["great","sql","functions","","ifnull","","nullif","","nvl","","nvl","","meaning","functions","found","oracle","docs","sql","function","","ifnull","","nullif","","nvl","nvl"],"features":{"type":0,"size":1000,"indices":[178,213,260,299,313,333,343,372,388,420,445,461,495,545,587,656,686,701,710,715,798,826,955],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,4.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"It would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project","_c1":"Make hadoop dockerfile usable by Yetus","document":"It would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project Make hadoop dockerfile usable by Yetus","words":["it","would","be","better","if","hadoop","s","dockerfile","could","be","used","by","yetus","so","that","external","dependencies","are","owned","by","the","project","make","hadoop","dockerfile","usable","by","yetus"],"filtered":["better","hadoop","dockerfile","used","yetus","external","dependencies","owned","project","make","hadoop","dockerfile","usable","yetus"],"features":{"type":0,"size":1000,"indices":[116,138,163,170,181,196,197,213,223,368,394,495,525,586,605,656,671,710,760,831,941],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"It would be easier to fix bugs and maintain the ec  script separately from Spark releases  For more information  see https   issues apache org jira browse SPARK","_c1":"Move spark ec  scripts to AMPLab","document":"It would be easier to fix bugs and maintain the ec  script separately from Spark releases  For more information  see https   issues apache org jira browse SPARK Move spark ec  scripts to AMPLab","words":["it","would","be","easier","to","fix","bugs","and","maintain","the","ec","","script","separately","from","spark","releases","","for","more","information","","see","https","","","issues","apache","org","jira","browse","spark","move","spark","ec","","scripts","to","amplab"],"filtered":["easier","fix","bugs","maintain","ec","","script","separately","spark","releases","","information","","see","https","","","issues","apache","org","jira","browse","spark","move","spark","ec","","scripts","amplab"],"features":{"type":0,"size":1000,"indices":[36,105,154,163,247,282,333,350,372,388,433,445,453,474,475,494,495,515,535,573,629,656,672,705,710,821,921,978,998],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"It would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value","_c1":"Introduce additonal implementation with a dense format for UnsafeArrayData","document":"It would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value Introduce additonal implementation with a dense format for UnsafeArrayData","words":["it","would","be","good","to","have","an","additional","implementation","","which","uses","dense","format","","for","","","unsafearraydata","","","to","reduce","memory","footprint","","current","","","unsafearraydata","","","implementation","uses","only","a","sparse","format","","it","is","useful","for","an","","","unsafearraydata","","","that","is","created","by","a","method","","","fromprimitivearray","","","","which","have","no","","","null","","","value","introduce","additonal","implementation","with","a","dense","format","for","unsafearraydata"],"filtered":["good","additional","implementation","","uses","dense","format","","","","unsafearraydata","","","reduce","memory","footprint","","current","","","unsafearraydata","","","implementation","uses","sparse","format","","useful","","","unsafearraydata","","","created","method","","","fromprimitivearray","","","","","","null","","","value","introduce","additonal","implementation","dense","format","unsafearraydata"],"features":{"type":0,"size":1000,"indices":[36,109,111,114,160,163,167,168,170,193,222,223,262,272,281,299,346,372,388,396,495,502,597,650,654,656,698,710,752,760,768,788,899,909],"values":[3.0,1.0,2.0,5.0,1.0,1.0,2.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,3.0,1.0,25.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings  It could then be hooked up to  bin hadoop fs cat  and the web ui to textify sequence and compressed files","_c1":"Create a utility to convert binary  sequence and compressed  files to strings","document":"It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings  It could then be hooked up to  bin hadoop fs cat  and the web ui to textify sequence and compressed files Create a utility to convert binary  sequence and compressed  files to strings","words":["it","would","be","nice","to","have","a","utility","that","looked","at","the","first","n","bytes","of","a","file","and","picked","a","decoder","for","it","into","strings","","it","could","then","be","hooked","up","to","","bin","hadoop","fs","cat","","and","the","web","ui","to","textify","sequence","and","compressed","files","create","a","utility","to","convert","binary","","sequence","and","compressed","","files","to","strings"],"filtered":["nice","utility","looked","first","n","bytes","file","picked","decoder","strings","","hooked","","bin","hadoop","fs","cat","","web","ui","textify","sequence","compressed","files","create","utility","convert","binary","","sequence","compressed","","files","strings"],"features":{"type":0,"size":1000,"indices":[19,36,40,84,108,128,135,163,170,181,183,213,237,246,265,274,282,299,333,342,343,356,370,372,381,388,456,495,551,567,626,649,655,656,710,746,753,756,760,764,891],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,2.0,1.0,1.0,1.0,5.0,1.0,5.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"It would be useful if more of JDBCRDD s JDBC    Spark SQL functionality was usable from outside of JDBCRDD  this would make it easier to write test harnesses comparing Spark output against other JDBC databases","_c1":"Refactor JDBCRDD to expose JDBC    SparkSQL conversion functionality","document":"It would be useful if more of JDBCRDD s JDBC    Spark SQL functionality was usable from outside of JDBCRDD  this would make it easier to write test harnesses comparing Spark output against other JDBC databases Refactor JDBCRDD to expose JDBC    SparkSQL conversion functionality","words":["it","would","be","useful","if","more","of","jdbcrdd","s","jdbc","","","","spark","sql","functionality","was","usable","from","outside","of","jdbcrdd","","this","would","make","it","easier","to","write","test","harnesses","comparing","spark","output","against","other","jdbc","databases","refactor","jdbcrdd","to","expose","jdbc","","","","sparksql","conversion","functionality"],"filtered":["useful","jdbcrdd","jdbc","","","","spark","sql","functionality","usable","outside","jdbcrdd","","make","easier","write","test","harnesses","comparing","spark","output","jdbc","databases","refactor","jdbcrdd","expose","jdbc","","","","sparksql","conversion","functionality"],"features":{"type":0,"size":1000,"indices":[105,109,111,113,116,122,141,163,170,197,234,241,272,342,343,365,372,373,388,420,474,495,525,549,586,623,629,656,674,686,790,850,921],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,7.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0]},"cluster_label":2}
{"_c0":"It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell","_c1":"Implement a native OS runtime for Hadoop","document":"It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell Implement a native OS runtime for Hadoop","words":["it","would","be","useful","to","implement","a","jni","based","runtime","for","hadoop","to","get","access","to","the","native","os","runtime","","this","would","allow","us","to","stop","relying","on","exec","ing","bash","to","get","access","to","information","such","as","user","groups","","process","limits","etc","","and","for","features","such","as","chown","chgrp","","org","apache","hadoop","util","shell","implement","a","native","os","runtime","for","hadoop"],"filtered":["useful","implement","jni","based","runtime","hadoop","get","access","native","os","runtime","","allow","us","stop","relying","exec","ing","bash","get","access","information","user","groups","","process","limits","etc","","features","chown","chgrp","","org","apache","hadoop","util","shell","implement","native","os","runtime","hadoop"],"features":{"type":0,"size":1000,"indices":[22,36,82,101,123,163,170,181,187,208,224,231,272,305,328,333,372,373,374,388,404,472,476,490,495,512,535,572,605,625,655,656,699,710,755,796,856,882,883,959,978],"values":[1.0,3.0,1.0,1.0,1.0,2.0,2.0,3.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,4.0,1.0,3.0,6.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"It would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure  This would allow us to pull the HDFS  MapReduce  and YARN shell functions out of hadoop functions sh  Additionally  it should let  rd parties such as HBase influence things like classpaths at runtime","_c1":"Pluggable shell integration","document":"It would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure  This would allow us to pull the HDFS  MapReduce  and YARN shell functions out of hadoop functions sh  Additionally  it should let  rd parties such as HBase influence things like classpaths at runtime Pluggable shell integration","words":["it","would","be","useful","to","provide","a","way","for","core","and","non","core","hadoop","components","to","plug","into","the","shell","infrastructure","","this","would","allow","us","to","pull","the","hdfs","","mapreduce","","and","yarn","shell","functions","out","of","hadoop","functions","sh","","additionally","","it","should","let","","rd","parties","such","as","hbase","influence","things","like","classpaths","at","runtime","pluggable","shell","integration"],"filtered":["useful","provide","way","core","non","core","hadoop","components","plug","shell","infrastructure","","allow","us","pull","hdfs","","mapreduce","","yarn","shell","functions","hadoop","functions","sh","","additionally","","let","","rd","parties","hbase","influence","things","like","classpaths","runtime","pluggable","shell","integration"],"features":{"type":0,"size":1000,"indices":[29,36,104,123,139,159,163,164,170,181,208,224,228,231,272,288,330,333,343,344,372,373,374,388,424,460,479,495,564,572,587,597,646,654,656,658,665,710,756,788,846,891,904,909,953,967],"values":[1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,6.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Java   is coming quickly to various clusters  Making sure Hadoop seamlessly works with Java   is important for the Apache community  This JIRA is to track the issues experiences encountered during Java   migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add a comment here","_c1":"Umbrella  Support Java   in Hadoop","document":"Java   is coming quickly to various clusters  Making sure Hadoop seamlessly works with Java   is important for the Apache community  This JIRA is to track the issues experiences encountered during Java   migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add a comment here Umbrella  Support Java   in Hadoop","words":["java","","","is","coming","quickly","to","various","clusters","","making","sure","hadoop","seamlessly","works","with","java","","","is","important","for","the","apache","community","","this","jira","is","to","track","the","issues","experiences","encountered","during","java","","","migration","","if","you","find","a","potential","bug","","","please","create","a","separate","jira","either","as","a","sub","task","or","linked","into","this","jira","","if","you","find","a","hadoop","or","jvm","configuration","tuning","","you","can","create","a","jira","as","well","","or","you","can","add","a","comment","here","umbrella","","support","java","","","in","hadoop"],"filtered":["java","","","coming","quickly","various","clusters","","making","sure","hadoop","seamlessly","works","java","","","important","apache","community","","jira","track","issues","experiences","encountered","java","","","migration","","find","potential","bug","","","please","create","separate","jira","either","sub","task","linked","jira","","find","hadoop","jvm","configuration","tuning","","create","jira","well","","add","comment","umbrella","","support","java","","","hadoop"],"features":{"type":0,"size":1000,"indices":[36,68,135,157,170,181,187,249,255,265,277,281,294,300,318,332,339,372,373,388,425,432,445,451,475,493,495,505,510,537,572,596,599,650,691,695,710,718,720,781,821,830,833,841,891,920,967,996,999],"values":[1.0,1.0,1.0,1.0,8.0,3.0,3.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,17.0,2.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Jetty  is no longer maintained  Update the dependency to jetty","_c1":"Update jetty dependency to version","document":"Jetty  is no longer maintained  Update the dependency to jetty Update jetty dependency to version","words":["jetty","","is","no","longer","maintained","","update","the","dependency","to","jetty","update","jetty","dependency","to","version"],"filtered":["jetty","","longer","maintained","","update","dependency","jetty","update","jetty","dependency","version"],"features":{"type":0,"size":1000,"indices":[281,286,343,346,372,388,492,588,710,868,995],"values":[1.0,1.0,2.0,1.0,2.0,2.0,3.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"KMS and HttpFS are using Tomcat         we should move it to        to get bug fixes and security fixes  We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS","_c1":"Update Tomcat version used by HttpFS and KMS to latest   x version","document":"KMS and HttpFS are using Tomcat         we should move it to        to get bug fixes and security fixes  We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS Update Tomcat version used by HttpFS and KMS to latest   x version","words":["kms","and","httpfs","are","using","tomcat","","","","","","","","","we","should","move","it","to","","","","","","","","to","get","bug","fixes","and","security","fixes","","we","should","add","a","property","with","the","tomcat","version","in","the","hadoop","project","pom","and","use","that","property","from","kms","and","httpfs","update","tomcat","version","used","by","httpfs","and","kms","to","latest","","","x","version"],"filtered":["kms","httpfs","using","tomcat","","","","","","","","","move","","","","","","","","get","bug","fixes","security","fixes","","add","property","tomcat","version","hadoop","project","pom","use","property","kms","httpfs","update","tomcat","version","used","httpfs","kms","latest","","","x","version"],"features":{"type":0,"size":1000,"indices":[138,170,181,223,240,249,282,333,343,360,372,388,432,445,472,489,495,498,605,615,624,634,650,665,671,710,733,760,810,921,959,993,995],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,18.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0]},"cluster_label":11}
{"_c0":"KMS and HttpFS currently uses Tomcat         propose to upgrade to the latest version is","_c1":"Upgrade Tomcat to","document":"KMS and HttpFS currently uses Tomcat         propose to upgrade to the latest version is Upgrade Tomcat to","words":["kms","and","httpfs","currently","uses","tomcat","","","","","","","","","propose","to","upgrade","to","the","latest","version","is","upgrade","tomcat","to"],"filtered":["kms","httpfs","currently","uses","tomcat","","","","","","","","","propose","upgrade","latest","version","upgrade","tomcat"],"features":{"type":0,"size":1000,"indices":[111,242,281,333,372,388,472,498,615,671,693,710,763,995],"values":[1.0,2.0,1.0,1.0,8.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Kolmogorov Smirnov Test is a popular nonparametric test of equality of distributions  There is implementation in MLlib  It will be nice if we can expose that in SparkR","_c1":"Add Kolmogorov Smirnov Test to SparkR","document":"Kolmogorov Smirnov Test is a popular nonparametric test of equality of distributions  There is implementation in MLlib  It will be nice if we can expose that in SparkR Add Kolmogorov Smirnov Test to SparkR","words":["kolmogorov","smirnov","test","is","a","popular","nonparametric","test","of","equality","of","distributions","","there","is","implementation","in","mllib","","it","will","be","nice","if","we","can","expose","that","in","sparkr","add","kolmogorov","smirnov","test","to","sparkr"],"filtered":["kolmogorov","smirnov","test","popular","nonparametric","test","equality","distributions","","implementation","mllib","","nice","expose","sparkr","add","kolmogorov","smirnov","test","sparkr"],"features":{"type":0,"size":1000,"indices":[109,170,281,343,370,372,388,402,420,432,445,446,495,521,567,586,656,698,760,767,827,831,833,897,966,993],"values":[1.0,2.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK       The change has been pushed back into Netty to support the same things under the DefaultFileRegion  https   github com netty netty issues      https   github com netty netty commit a    b  d c   f b f  e      b    a    be It looks like that went into        Final  I believe at the time we created LazyFileRegion we were on        Final and we are now using        Final so we should be able to use the netty class directly","_c1":"Remove LazyFileRegion","document":"LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK       The change has been pushed back into Netty to support the same things under the DefaultFileRegion  https   github com netty netty issues      https   github com netty netty commit a    b  d c   f b f  e      b    a    be It looks like that went into        Final  I believe at the time we created LazyFileRegion we were on        Final and we are now using        Final so we should be able to use the netty class directly Remove LazyFileRegion","words":["lazyfileregion","was","created","so","we","didn","t","create","a","file","descriptor","before","having","to","send","the","file","","see","https","","","issues","apache","org","jira","browse","spark","","","","","","","the","change","has","been","pushed","back","into","netty","to","support","the","same","things","under","the","defaultfileregion","","https","","","github","com","netty","netty","issues","","","","","","https","","","github","com","netty","netty","commit","a","","","","b","","d","c","","","f","b","f","","e","","","","","","b","","","","a","","","","be","it","looks","like","that","went","into","","","","","","","","final","","i","believe","at","the","time","we","created","lazyfileregion","we","were","on","","","","","","","","final","and","we","are","now","using","","","","","","","","final","so","we","should","be","able","to","use","the","netty","class","directly","remove","lazyfileregion"],"filtered":["lazyfileregion","created","didn","create","file","descriptor","send","file","","see","https","","","issues","apache","org","jira","browse","spark","","","","","","","change","pushed","back","netty","support","things","defaultfileregion","","https","","","github","com","netty","netty","issues","","","","","","https","","","github","com","netty","netty","commit","","","","b","","d","c","","","f","b","f","","e","","","","","","b","","","","","","","looks","like","went","","","","","","","","final","","believe","time","created","lazyfileregion","","","","","","","","final","using","","","","","","","","final","able","use","netty","class","directly","remove","lazyfileregion"],"features":{"type":0,"size":1000,"indices":[39,78,82,94,98,105,108,138,140,141,154,157,158,159,170,173,188,221,234,248,262,265,288,329,330,333,361,368,371,372,388,422,430,436,475,489,495,496,510,515,523,534,535,580,587,624,656,665,675,695,710,722,756,759,760,777,821,878,891,904,915,962,986,993,998],"values":[1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,59.0,3.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,3.0]},"cluster_label":5}
{"_c0":"Let s pull the shell code out of the hadoop dist pom xml","_c1":"pull shell code out of hadoop dist","document":"Let s pull the shell code out of the hadoop dist pom xml pull shell code out of hadoop dist","words":["let","s","pull","the","shell","code","out","of","the","hadoop","dist","pom","xml","pull","shell","code","out","of","hadoop","dist"],"filtered":["let","pull","shell","code","hadoop","dist","pom","xml","pull","shell","code","hadoop","dist"],"features":{"type":0,"size":1000,"indices":[92,123,164,181,197,343,360,420,597,654,710,779],"values":[1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,2.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Like shuffle file encryption in SPARK       spills data should also be encrypted","_c1":"Support shuffle spill encryption in Spark","document":"Like shuffle file encryption in SPARK       spills data should also be encrypted Support shuffle spill encryption in Spark","words":["like","shuffle","file","encryption","in","spark","","","","","","","spills","data","should","also","be","encrypted","support","shuffle","spill","encryption","in","spark"],"filtered":["like","shuffle","file","encryption","spark","","","","","","","spills","data","also","encrypted","support","shuffle","spill","encryption","spark"],"features":{"type":0,"size":1000,"indices":[105,108,177,179,249,312,330,372,445,568,656,665,695,792],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,6.0,2.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base","_c1":"Remove InsertIntoHiveTable From Logical Plan","document":"LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base Remove InsertIntoHiveTable From Logical Plan","words":["logicalplan","","insertintohivetable","","is","useless","","thus","","we","can","remove","it","from","the","code","base","remove","insertintohivetable","from","logical","plan"],"filtered":["logicalplan","","insertintohivetable","","useless","","thus","","remove","code","base","remove","insertintohivetable","logical","plan"],"features":{"type":0,"size":1000,"indices":[123,173,247,281,288,372,420,495,515,684,710,766,833,906,921,993],"values":[1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"MLlib s Transformer uses the deprecated callUDF API","_c1":"Remove the use of the deprecated callUDF in MLlib","document":"MLlib s Transformer uses the deprecated callUDF API Remove the use of the deprecated callUDF in MLlib","words":["mllib","s","transformer","uses","the","deprecated","calludf","api","remove","the","use","of","the","deprecated","calludf","in","mllib"],"filtered":["mllib","transformer","uses","deprecated","calludf","api","remove","use","deprecated","calludf","mllib"],"features":{"type":0,"size":1000,"indices":[111,197,288,335,343,445,489,521,620,644,710,922],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0]},"cluster_label":13}
{"_c0":"Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines  Making them public should be safe  even if we change internal formats","_c1":"Make DefaultParamsReadable Writable public APIs","document":"Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines  Making them public should be safe  even if we change internal formats Make DefaultParamsReadable Writable public APIs","words":["making","defaultparamswritable","and","defaultparamsreadable","public","will","help","users","who","have","their","own","transformers","save","and","load","pipelines","","making","them","public","should","be","safe","","even","if","we","change","internal","formats","make","defaultparamsreadable","writable","public","apis"],"filtered":["making","defaultparamswritable","defaultparamsreadable","public","help","users","transformers","save","load","pipelines","","making","public","safe","","even","change","internal","formats","make","defaultparamsreadable","writable","public","apis"],"features":{"type":0,"size":1000,"indices":[158,170,217,228,235,258,295,299,333,347,372,406,420,498,520,525,620,642,656,660,665,691,692,714,755,782,842,924,993,996],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Many Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime","_c1":"SparkSession time   a simple timer function","document":"Many Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime SparkSession time   a simple timer function","words":["many","spark","developers","often","want","to","test","the","runtime","of","some","function","in","interactive","debugging","and","testing","","it","d","be","really","useful","to","have","a","simple","spark","time","method","that","can","test","the","runtime","sparksession","time","","","a","simple","timer","function"],"filtered":["many","spark","developers","often","want","test","runtime","function","interactive","debugging","testing","","d","really","useful","simple","spark","time","method","test","runtime","sparksession","time","","","simple","timer","function"],"features":{"type":0,"size":1000,"indices":[42,94,105,157,170,188,272,299,310,313,315,333,343,372,374,388,400,401,445,489,495,528,586,654,656,710,712,738,760,833,942,980],"values":[1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull       discussion r","_c1":"Move DT RF GBT Param setter methods to subclasses","document":"Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull       discussion r Move DT RF GBT Param setter methods to subclasses","words":["move","dt","rf","gbt","param","setter","methods","to","subclasses","and","deprecate","these","methods","in","the","model","classes","to","make","them","more","java","friendly","","see","discussion","at","https","","","github","com","apache","spark","pull","","","","","","","discussion","r","move","dt","rf","gbt","param","setter","methods","to","subclasses"],"filtered":["move","dt","rf","gbt","param","setter","methods","subclasses","deprecate","methods","model","classes","make","java","friendly","","see","discussion","https","","","github","com","apache","spark","pull","","","","","","","discussion","r","move","dt","rf","gbt","param","setter","methods","subclasses"],"features":{"type":0,"size":1000,"indices":[2,79,105,129,221,275,282,288,333,372,388,445,461,495,510,515,525,570,597,629,695,709,710,756,778,782,809,857,924,967,998],"values":[2.0,2.0,3.0,3.0,1.0,1.0,2.0,2.0,1.0,9.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search","_c1":"NetworkTopology is not efficient adding getting removing nodes","document":"NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search NetworkTopology is not efficient adding getting removing nodes","words":["networktoplogy","uses","nodes","with","a","list","of","children","","the","access","to","these","children","is","slow","as","it","s","a","linear","search","networktopology","is","not","efficient","adding","getting","removing","nodes"],"filtered":["networktoplogy","uses","nodes","list","children","","access","children","slow","linear","search","networktopology","efficient","adding","getting","removing","nodes"],"features":{"type":0,"size":1000,"indices":[18,71,111,132,146,170,197,203,224,231,281,329,342,343,361,372,388,460,461,495,572,650,669,710,728,968],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"New file system API  HADOOP       should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations  MAPREDUCE","_c1":"security implementation for new FileSystem  FileContext  API","document":"New file system API  HADOOP       should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations  MAPREDUCE security implementation for new FileSystem  FileContext  API","words":["new","file","system","api","","hadoop","","","","","","","should","implement","security","features","currently","provided","by","filesystem","apis","this","is","a","critical","requirement","for","mapreduce","components","to","migrate","and","use","new","apis","for","internal","filesystem","operations","","mapreduce","security","implementation","for","new","filesystem","","filecontext","","api"],"filtered":["new","file","system","api","","hadoop","","","","","","","implement","security","features","currently","provided","filesystem","apis","critical","requirement","mapreduce","components","migrate","use","new","apis","internal","filesystem","operations","","mapreduce","security","implementation","new","filesystem","","filecontext","","api"],"features":{"type":0,"size":1000,"indices":[25,36,88,108,139,170,181,223,281,295,333,364,372,373,388,472,489,609,633,634,639,644,665,698,731,735,755,763,800,842,953],"values":[3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":2}
{"_c0":"Once   JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit","_c1":"JarFinder getJar should delete the jar file upon destruction of the JVM","document":"Once   JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit JarFinder getJar should delete the jar file upon destruction of the JVM","words":["once","","","jarfinder","getjar","","","","","is","invoked","by","a","client","app","","it","would","be","really","useful","to","destroy","the","generated","jar","after","the","jvm","is","destroyed","by","setting","","","tempjar","deleteonexit","","","","","","in","order","to","preserve","backwards","compatibility","a","configuration","setting","could","be","implemented","","e","g","","","","test","build","dir","purge","on","exit","jarfinder","getjar","should","delete","the","jar","file","upon","destruction","of","the","jvm"],"filtered":["","","jarfinder","getjar","","","","","invoked","client","app","","really","useful","destroy","generated","jar","jvm","destroyed","setting","","","tempjar","deleteonexit","","","","","","order","preserve","backwards","compatibility","configuration","setting","implemented","","e","g","","","","test","build","dir","purge","exit","jarfinder","getjar","delete","jar","file","upon","destruction","jvm"],"features":{"type":0,"size":1000,"indices":[12,30,51,77,82,94,108,135,147,163,170,177,213,223,255,272,281,300,309,310,315,343,372,388,417,422,436,445,495,536,568,586,603,656,659,665,691,692,706,709,710,718,735,834,878,906,953],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,18.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Other people aren t seeing this  yet    but unless you explicitly exclude v     of commons lang  from the azure build  which HADOOP       does   then the dependency declaration of commons lang  v       is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests  I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common","_c1":"explicitly declare the commons lang  dependency as","document":"Other people aren t seeing this  yet    but unless you explicitly exclude v     of commons lang  from the azure build  which HADOOP       does   then the dependency declaration of commons lang  v       is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests  I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common explicitly declare the commons lang  dependency as","words":["other","people","aren","t","seeing","this","","yet","","","","but","unless","you","explicitly","exclude","v","","","","","of","commons","lang","","from","the","azure","build","","which","hadoop","","","","","","","does","","","then","the","dependency","declaration","of","commons","lang","","v","","","","","","","is","creating","a","resolution","conflict","","that","s","a","dependency","only","needed","for","the","local","dynamodb","","","tests","","i","propose","to","fix","this","in","s","guard","by","explicitly","declaring","the","version","used","in","the","tests","to","be","that","of","the","azure","storage","one","","excluding","that","you","get","for","free","","it","doesn","t","impact","anything","shipped","in","production","","but","puts","the","hadoop","build","in","control","of","what","versions","of","commons","lang","are","coming","in","everywhere","by","way","of","the","commons","config","version","declared","in","hadoop","common","explicitly","declare","the","commons","lang","","dependency","as"],"filtered":["people","aren","seeing","","yet","","","","unless","explicitly","exclude","v","","","","","commons","lang","","azure","build","","hadoop","","","","","","","","","dependency","declaration","commons","lang","","v","","","","","","","creating","resolution","conflict","","dependency","needed","local","dynamodb","","","tests","","propose","fix","guard","explicitly","declaring","version","used","tests","azure","storage","one","","excluding","get","free","","doesn","impact","anything","shipped","production","","puts","hadoop","build","control","versions","commons","lang","coming","everywhere","way","commons","config","version","declared","hadoop","common","explicitly","declare","commons","lang","","dependency"],"features":{"type":0,"size":1000,"indices":[6,36,44,73,83,114,116,135,138,158,159,170,181,197,223,244,246,281,310,318,329,343,348,352,372,373,381,388,394,425,436,445,477,486,495,500,526,536,572,580,588,597,605,608,616,619,656,671,674,675,693,698,710,745,755,760,767,777,788,805,810,819,874,897,899,921,930,953,954,959,980,984,994,995],"values":[1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,33.0,2.0,1.0,2.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0]},"cluster_label":1}
{"_c0":"Otherwise  other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish","_c1":"MemorySink should not call DataFrame collect when holding a lock","document":"Otherwise  other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish MemorySink should not call DataFrame collect when holding a lock","words":["otherwise","","other","threads","cannot","query","the","content","in","memorysink","when","","dataframe","collect","","takes","long","time","to","finish","memorysink","should","not","call","dataframe","collect","when","holding","a","lock"],"filtered":["otherwise","","threads","query","content","memorysink","","dataframe","collect","","takes","long","time","finish","memorysink","call","dataframe","collect","holding","lock"],"features":{"type":0,"size":1000,"indices":[9,18,30,76,89,109,146,157,161,170,242,254,372,388,445,594,665,674,710,855,875,904,931,941],"values":[1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Our code can go through SessionState catalog  This brings two small benefits     Reduces internal dependency on SQLContext     Removes another public method in Java  Java does not obey package private visibility   More importantly  according to the design in SPARK        we d need to claim this catalog function for the user facing public functions  rather than having an internal field","_c1":"Remove SQLContext catalog  internal method","document":"Our code can go through SessionState catalog  This brings two small benefits     Reduces internal dependency on SQLContext     Removes another public method in Java  Java does not obey package private visibility   More importantly  according to the design in SPARK        we d need to claim this catalog function for the user facing public functions  rather than having an internal field Remove SQLContext catalog  internal method","words":["our","code","can","go","through","sessionstate","catalog","","this","brings","two","small","benefits","","","","","reduces","internal","dependency","on","sqlcontext","","","","","removes","another","public","method","in","java","","java","does","not","obey","package","private","visibility","","","more","importantly","","according","to","the","design","in","spark","","","","","","","","we","d","need","to","claim","this","catalog","function","for","the","user","facing","public","functions","","rather","than","having","an","internal","field","remove","sqlcontext","catalog","","internal","method"],"filtered":["code","go","sessionstate","catalog","","brings","two","small","benefits","","","","","reduces","internal","dependency","sqlcontext","","","","","removes","another","public","method","java","","java","obey","package","private","visibility","","","importantly","","according","design","spark","","","","","","","","d","need","claim","catalog","function","user","facing","public","functions","","rather","internal","field","remove","sqlcontext","catalog","","internal","method"],"features":{"type":0,"size":1000,"indices":[18,36,77,82,94,105,171,189,253,261,266,288,295,313,372,373,388,408,413,416,420,437,445,451,498,510,533,534,537,543,587,588,605,629,641,654,675,698,704,710,742,752,779,833,849,853,871,882,967,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,22.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"Our current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins","_c1":"Rewrite Intersect phyiscal plan using semi join","document":"Our current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins Rewrite Intersect phyiscal plan using semi join","words":["our","current","intersect","physical","operator","simply","delegates","to","rdd","intersect","","we","should","remove","the","intersect","physical","operator","and","simply","transform","a","logical","intersect","into","a","semi","join","","this","way","","we","can","take","advantage","of","all","the","benefits","of","join","implementations","","e","g","","managed","memory","","code","generation","","broadcast","joins","rewrite","intersect","phyiscal","plan","using","semi","join"],"filtered":["current","intersect","physical","operator","simply","delegates","rdd","intersect","","remove","intersect","physical","operator","simply","transform","logical","intersect","semi","join","","way","","take","advantage","benefits","join","implementations","","e","g","","managed","memory","","code","generation","","broadcast","joins","rewrite","intersect","phyiscal","plan","using","semi","join"],"features":{"type":0,"size":1000,"indices":[113,117,123,159,170,179,199,205,220,247,288,319,333,343,372,373,388,417,420,462,533,534,617,624,649,665,684,704,710,738,788,833,855,870,878,891,925,952,968,993,996],"values":[1.0,1.0,1.0,1.0,2.0,5.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Our current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true","_c1":"Deprecate registerTempTable and add dataset createTempView","document":"Our current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true Deprecate registerTempTable and add dataset createTempView","words":["our","current","dataset","registertemptable","does","not","actually","materialize","data","","so","","it","should","be","considered","as","creating","a","temp","view","","we","can","deprecate","it","and","create","a","new","method","called","dataset","createtempview","replaceifexists","","boolean","","","the","default","value","of","replaceifexists","should","be","false","","for","registertemptable","","it","will","call","dataset","createtempview","replaceifexists","","","true","deprecate","registertemptable","and","add","dataset","createtempview"],"filtered":["current","dataset","registertemptable","actually","materialize","data","","","considered","creating","temp","view","","deprecate","create","new","method","called","dataset","createtempview","replaceifexists","","boolean","","","default","value","replaceifexists","false","","registertemptable","","call","dataset","createtempview","replaceifexists","","","true","deprecate","registertemptable","add","dataset","createtempview"],"features":{"type":0,"size":1000,"indices":[18,19,25,36,146,170,171,188,265,275,293,333,343,366,368,369,372,381,398,420,432,447,448,493,495,572,640,654,656,665,678,695,698,704,710,737,767,768,833,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,10.0,1.0,1.0,1.0,1.0,1.0,3.0,4.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Our current dataset.registerTempTable does not actually materialize data. So, it should be considered as creating a temp view. We can deprecate it and create a new method called dataset.createTempView(replaceIfExists: Boolean). The default value of replaceIfExists should be false. For registerTempTable, it will call dataset.createTempView(replaceIfExists = true).","_c1":"Deprecate registerTempTable and add dataset.createTempView","document":"Our current dataset.registerTempTable does not actually materialize data. So, it should be considered as creating a temp view. We can deprecate it and create a new method called dataset.createTempView(replaceIfExists: Boolean). The default value of replaceIfExists should be false. For registerTempTable, it will call dataset.createTempView(replaceIfExists = true). Deprecate registerTempTable and add dataset.createTempView","words":["our","current","dataset.registertemptable","does","not","actually","materialize","data.","so,","it","should","be","considered","as","creating","a","temp","view.","we","can","deprecate","it","and","create","a","new","method","called","dataset.createtempview(replaceifexists:","boolean).","the","default","value","of","replaceifexists","should","be","false.","for","registertemptable,","it","will","call","dataset.createtempview(replaceifexists","=","true).","deprecate","registertemptable","and","add","dataset.createtempview"],"filtered":["current","dataset.registertemptable","actually","materialize","data.","so,","considered","creating","temp","view.","deprecate","create","new","method","called","dataset.createtempview(replaceifexists:","boolean).","default","value","replaceifexists","false.","registertemptable,","call","dataset.createtempview(replaceifexists","=","true).","deprecate","registertemptable","add","dataset.createtempview"],"features":{"type":0,"size":1000,"indices":[18,19,25,36,43,98,141,146,170,171,197,265,275,328,333,343,344,348,366,381,398,420,432,447,448,495,572,612,640,654,656,665,698,704,710,767,768,833,884,968,977,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Our project  Pig  exposes FsShell functionality to our end users through a shell command  We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics  The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign  for instance  removing a non existent directory  We have   asks related to this issue      Meaningful error code returned from FsShell  we use java class  so that we can take different actions on different errors     Unix like ways to tell the command to ignore certain behavior  Here are the commands that we would like to be expanded implemented    rm  f   rmdir    ignore fail on non empty   mkdir  p","_c1":"Extensions to FsShell","document":"Our project  Pig  exposes FsShell functionality to our end users through a shell command  We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics  The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign  for instance  removing a non existent directory  We have   asks related to this issue      Meaningful error code returned from FsShell  we use java class  so that we can take different actions on different errors     Unix like ways to tell the command to ignore certain behavior  Here are the commands that we would like to be expanded implemented    rm  f   rmdir    ignore fail on non empty   mkdir  p Extensions to FsShell","words":["our","project","","pig","","exposes","fsshell","functionality","to","our","end","users","through","a","shell","command","","we","want","to","use","this","command","with","no","modifications","to","make","sure","that","whether","you","work","with","hdfs","through","hadoop","or","pig","you","get","identical","semantics","","the","main","concern","that","has","been","recently","raised","by","our","users","is","that","there","is","no","way","to","ignore","certain","failures","that","they","consider","to","be","benign","","for","instance","","removing","a","non","existent","directory","","we","have","","","asks","related","to","this","issue","","","","","","meaningful","error","code","returned","from","fsshell","","we","use","java","class","","so","that","we","can","take","different","actions","on","different","errors","","","","","unix","like","ways","to","tell","the","command","to","ignore","certain","behavior","","here","are","the","commands","that","we","would","like","to","be","expanded","implemented","","","","rm","","f","","","rmdir","","","","ignore","fail","on","non","empty","","","mkdir","","p","extensions","to","fsshell"],"filtered":["project","","pig","","exposes","fsshell","functionality","end","users","shell","command","","want","use","command","modifications","make","sure","whether","work","hdfs","hadoop","pig","get","identical","semantics","","main","concern","recently","raised","users","way","ignore","certain","failures","consider","benign","","instance","","removing","non","existent","directory","","","","asks","related","issue","","","","","","meaningful","error","code","returned","fsshell","","use","java","class","","take","different","actions","different","errors","","","","","unix","like","ways","tell","command","ignore","certain","behavior","","commands","like","expanded","implemented","","","","rm","","f","","","rmdir","","","","ignore","fail","non","empty","","","mkdir","","p","extensions","fsshell"],"features":{"type":0,"size":1000,"indices":[7,19,34,36,48,50,82,89,92,123,135,138,158,159,163,166,170,177,181,187,199,209,223,224,234,248,281,284,293,299,310,312,330,333,343,346,352,365,368,372,373,388,420,425,428,467,474,489,525,527,532,534,535,538,543,554,560,580,606,609,630,637,650,656,671,704,710,712,735,748,755,760,778,807,821,831,833,850,855,902,921,943,959,967,968,981,992,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,33.0,3.0,10.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0]},"cluster_label":1}
{"_c0":"Parquet files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector","_c1":"Vectorize parquet decoding using ColumnarBatch","document":"Parquet files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector Vectorize parquet decoding using ColumnarBatch","words":["parquet","files","benefit","from","vectorized","decoding","","columnarbatches","have","been","designed","to","support","this","","this","means","that","a","single","encoded","parquet","column","is","decoded","to","a","single","columnvector","vectorize","parquet","decoding","using","columnarbatch"],"filtered":["parquet","files","benefit","vectorized","decoding","","columnarbatches","designed","support","","means","single","encoded","parquet","column","decoded","single","columnvector","vectorize","parquet","decoding","using","columnarbatch"],"features":{"type":0,"size":1000,"indices":[18,170,172,281,299,372,373,377,388,394,402,426,531,535,541,551,601,624,693,695,742,760,835,906,921,975],"values":[1.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Per   nongli  s suggestions  We should do these things     Remove the non vectorized parquet reader code     Support the remaining types  just big decimals      Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader","_c1":"Cleanup Extend the Vectorized Parquet Reader","document":"Per   nongli  s suggestions  We should do these things     Remove the non vectorized parquet reader code     Support the remaining types  just big decimals      Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader Cleanup Extend the Vectorized Parquet Reader","words":["per","","","nongli","","s","suggestions","","we","should","do","these","things","","","","","remove","the","non","vectorized","parquet","reader","code","","","","","support","the","remaining","types","","just","big","decimals","","","","","","move","the","logic","to","determine","if","our","parquet","reader","can","be","used","to","planning","","only","complex","types","should","fall","back","to","the","parquet","mr","reader","cleanup","extend","the","vectorized","parquet","reader"],"filtered":["per","","","nongli","","suggestions","","things","","","","","remove","non","vectorized","parquet","reader","code","","","","","support","remaining","types","","big","decimals","","","","","","move","logic","determine","parquet","reader","used","planning","","complex","types","fall","back","parquet","mr","reader","cleanup","extend","vectorized","parquet","reader"],"features":{"type":0,"size":1000,"indices":[9,170,172,197,224,272,282,288,304,307,372,388,402,420,430,440,446,461,465,499,534,536,598,605,637,656,665,695,704,710,729,747,833,834,843,899,904,918,970,993],"values":[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,19.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Per discussion on HADOOP        I d like to revert HADOOP        It removes a deprecated API  but the   x line does not have a release with the new replacement API  This places a burden on downstream applications","_c1":"Revert HADOOP       Remove unused TrashPolicy getInstance and initialize code","document":"Per discussion on HADOOP        I d like to revert HADOOP        It removes a deprecated API  but the   x line does not have a release with the new replacement API  This places a burden on downstream applications Revert HADOOP       Remove unused TrashPolicy getInstance and initialize code","words":["per","discussion","on","hadoop","","","","","","","","i","d","like","to","revert","hadoop","","","","","","","","it","removes","a","deprecated","api","","but","the","","","x","line","does","not","have","a","release","with","the","new","replacement","api","","this","places","a","burden","on","downstream","applications","revert","hadoop","","","","","","","remove","unused","trashpolicy","getinstance","and","initialize","code"],"filtered":["per","discussion","hadoop","","","","","","","","d","like","revert","hadoop","","","","","","","","removes","deprecated","api","","","","x","line","release","new","replacement","api","","places","burden","downstream","applications","revert","hadoop","","","","","","","remove","unused","trashpolicy","getinstance","initialize","code"],"features":{"type":0,"size":1000,"indices":[14,18,25,82,83,94,127,158,170,171,181,182,288,299,329,330,333,340,371,372,373,388,420,440,452,455,465,495,620,644,650,695,698,710,742,810,879,924],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,24.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"Per discussions with Arun  Chris  Hong and Rajiv  et al  we concluded that the current metrics framework needs an overhaul to    Allow multiple plugins for different monitoring systems simultaneously  see also  HADOOP          Refresh metrics plugin config without server restart     Including filtering of metrics per plugin    Support metrics schema for plugins  The jira will be resolved when core hadoop components  hdfs  mapreduce  are updated to use the new framework   Updates to external components that use the existing metrics framework will be tracked by different issues   The current design wiki http   wiki apache org hadoop HADOOP      MetricsV","_c1":"Overhaul metrics framework","document":"Per discussions with Arun  Chris  Hong and Rajiv  et al  we concluded that the current metrics framework needs an overhaul to    Allow multiple plugins for different monitoring systems simultaneously  see also  HADOOP          Refresh metrics plugin config without server restart     Including filtering of metrics per plugin    Support metrics schema for plugins  The jira will be resolved when core hadoop components  hdfs  mapreduce  are updated to use the new framework   Updates to external components that use the existing metrics framework will be tracked by different issues   The current design wiki http   wiki apache org hadoop HADOOP      MetricsV Overhaul metrics framework","words":["per","discussions","with","arun","","chris","","hong","and","rajiv","","et","al","","we","concluded","that","the","current","metrics","framework","needs","an","overhaul","to","","","","allow","multiple","plugins","for","different","monitoring","systems","simultaneously","","see","also","","hadoop","","","","","","","","","","refresh","metrics","plugin","config","without","server","restart","","","","","including","filtering","of","metrics","per","plugin","","","","support","metrics","schema","for","plugins","","the","jira","will","be","resolved","when","core","hadoop","components","","hdfs","","mapreduce","","are","updated","to","use","the","new","framework","","","updates","to","external","components","that","use","the","existing","metrics","framework","will","be","tracked","by","different","issues","","","the","current","design","wiki","http","","","wiki","apache","org","hadoop","hadoop","","","","","","metricsv","overhaul","metrics","framework"],"filtered":["per","discussions","arun","","chris","","hong","rajiv","","et","al","","concluded","current","metrics","framework","needs","overhaul","","","","allow","multiple","plugins","different","monitoring","systems","simultaneously","","see","also","","hadoop","","","","","","","","","","refresh","metrics","plugin","config","without","server","restart","","","","","including","filtering","metrics","per","plugin","","","","support","metrics","schema","plugins","","jira","resolved","core","hadoop","components","","hdfs","","mapreduce","","updated","use","new","framework","","","updates","external","components","use","existing","metrics","framework","tracked","different","issues","","","current","design","wiki","http","","","wiki","apache","org","hadoop","hadoop","","","","","","metricsv","overhaul","metrics","framework"],"features":{"type":0,"size":1000,"indices":[25,36,76,89,106,138,139,140,181,189,223,228,231,270,292,333,343,346,352,371,372,388,411,420,429,440,441,443,475,489,490,495,513,515,535,566,586,591,592,599,650,655,656,665,666,674,695,710,737,751,752,760,792,820,821,840,858,873,884,900,953,954,960,962,967,980,993],"values":[1.0,2.0,1.0,2.0,6.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,40.0,3.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":1}
{"_c0":"Per our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev        mbox   CCA g  F aVRBH WyyK nvBSLCMPtSdUuL Ge  WW DnmnvY SXg mail gmail com  E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size","_c1":"Add support for custom coalescers","document":"Per our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev        mbox   CCA g  F aVRBH WyyK nvBSLCMPtSdUuL Ge  WW DnmnvY SXg mail gmail com  E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size Add support for custom coalescers","words":["per","our","discussion","on","the","mailing","list","","please","see","","here","http","","","mail","archives","apache","org","mod","mbox","","spark","dev","","","","","","","","mbox","","","cca","g","","f","avrbh","wyyk","nvbslcmptsduul","ge","","ww","dnmnvy","sxg","mail","gmail","com","","e","","","it","would","be","nice","to","specify","a","custom","coalescing","policy","as","the","current","","","coalesce","","","","","method","only","allows","the","user","to","specify","the","number","of","partitions","and","we","cannot","really","control","much","","the","need","for","this","feature","popped","up","when","i","wanted","to","merge","small","files","by","coalescing","them","by","size","add","support","for","custom","coalescers"],"filtered":["per","discussion","mailing","list","","please","see","","http","","","mail","archives","apache","org","mod","mbox","","spark","dev","","","","","","","","mbox","","","cca","g","","f","avrbh","wyyk","nvbslcmptsduul","ge","","ww","dnmnvy","sxg","mail","gmail","com","","e","","","nice","specify","custom","coalescing","policy","current","","","coalesce","","","","","method","allows","user","specify","number","partitions","really","control","much","","need","feature","popped","wanted","merge","small","files","coalescing","size","add","support","custom","coalescers"],"features":{"type":0,"size":1000,"indices":[36,44,63,76,82,105,128,135,163,170,192,221,223,227,248,310,329,333,343,346,354,370,372,373,388,395,411,417,425,432,440,455,457,480,495,515,524,535,537,551,567,572,583,612,630,647,654,656,665,695,699,704,710,728,735,736,742,773,789,841,874,878,882,899,906,924,931,983,991,992,993,998],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,26.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"Post HADOOP       we need to rework how heap is configured for small footprint machines  deprecate some options  introduce new ones for greater flexibility","_c1":"rework heap management vars","document":"Post HADOOP       we need to rework how heap is configured for small footprint machines  deprecate some options  introduce new ones for greater flexibility rework heap management vars","words":["post","hadoop","","","","","","","we","need","to","rework","how","heap","is","configured","for","small","footprint","machines","","deprecate","some","options","","introduce","new","ones","for","greater","flexibility","rework","heap","management","vars"],"filtered":["post","hadoop","","","","","","","need","rework","heap","configured","small","footprint","machines","","deprecate","options","","introduce","new","ones","greater","flexibility","rework","heap","management","vars"],"features":{"type":0,"size":1000,"indices":[25,36,96,160,181,229,237,275,281,318,372,388,396,400,497,537,567,604,651,742,797,809,973,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,8.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Previously  we use   MB as the default page size  which was way too big for a lot of Spark applications  especially for single node   This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available","_c1":"Pick default page size more intelligently","document":"Previously  we use   MB as the default page size  which was way too big for a lot of Spark applications  especially for single node   This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available Pick default page size more intelligently","words":["previously","","we","use","","","mb","as","the","default","page","size","","which","was","way","too","big","for","a","lot","of","spark","applications","","especially","for","single","node","","","this","patch","changes","it","so","that","the","default","page","size","","if","unset","by","the","user","","is","determined","by","the","number","of","cores","available","and","the","total","execution","memory","available","pick","default","page","size","more","intelligently"],"filtered":["previously","","use","","","mb","default","page","size","","way","big","lot","spark","applications","","especially","single","node","","","patch","changes","default","page","size","","unset","user","","determined","number","cores","available","total","execution","memory","available","pick","default","page","size","intelligently"],"features":{"type":0,"size":1000,"indices":[36,105,137,140,159,170,192,223,234,281,284,333,343,362,363,368,371,372,373,381,489,495,531,571,572,583,597,598,624,629,644,710,735,742,760,764,770,775,788,827,882,993],"values":[2.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,10.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Provide API for SVM algorithm for DataFrames  I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation  The API should mimic existing spark ml classification APIs","_c1":"spark ml API for linear SVM","document":"Provide API for SVM algorithm for DataFrames  I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation  The API should mimic existing spark ml classification APIs spark ml API for linear SVM","words":["provide","api","for","svm","algorithm","for","dataframes","","i","would","recommend","using","owl","qn","","rather","than","wrapping","spark","mllib","s","sgd","based","implementation","","the","api","should","mimic","existing","spark","ml","classification","apis","spark","ml","api","for","linear","svm"],"filtered":["provide","api","svm","algorithm","dataframes","","recommend","using","owl","qn","","rather","wrapping","spark","mllib","sgd","based","implementation","","api","mimic","existing","spark","ml","classification","apis","spark","ml","api","linear","svm"],"features":{"type":0,"size":1000,"indices":[5,36,105,162,163,197,215,261,276,288,324,329,371,372,437,521,536,549,624,625,644,665,698,710,723,781,842,883,998],"values":[1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Python s   since  is defined under  pyspark sql   It would be nice to move it under  pyspark  to be shared by all components","_c1":"Move  since annotator to pyspark to be shared by all components","document":"Python s   since  is defined under  pyspark sql   It would be nice to move it under  pyspark  to be shared by all components Move  since annotator to pyspark to be shared by all components","words":["python","s","","","since","","is","defined","under","","pyspark","sql","","","it","would","be","nice","to","move","it","under","","pyspark","","to","be","shared","by","all","components","move","","since","annotator","to","pyspark","to","be","shared","by","all","components"],"filtered":["python","","","since","","defined","","pyspark","sql","","","nice","move","","pyspark","","shared","components","move","","since","annotator","pyspark","shared","components"],"features":{"type":0,"size":1000,"indices":[33,39,126,139,163,197,223,281,282,293,370,372,388,495,509,585,589,656,686,968],"values":[1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,9.0,4.0,2.0,3.0,2.0,1.0,3.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Query     Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct","_c1":"Optimizer should remove unnecessary distincts  in multiple unions","document":"Query     Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct Optimizer should remove unnecessary distincts  in multiple unions","words":["query","","","","","only","one","distinct","should","be","necessary","","this","makes","a","bunch","of","unions","slower","than","a","bunch","of","union","alls","followed","by","a","distinct","optimizer","should","remove","unnecessary","distincts","","in","multiple","unions"],"filtered":["query","","","","","one","distinct","necessary","","makes","bunch","unions","slower","bunch","union","alls","followed","distinct","optimizer","remove","unnecessary","distincts","","multiple","unions"],"features":{"type":0,"size":1000,"indices":[44,170,194,221,223,242,261,288,295,316,343,372,373,399,406,438,445,592,656,665,691,697,845,899],"values":[1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler","_c1":"Add non iterator interface to RandomSampler","document":"RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler Add non iterator interface to RandomSampler","words":["randomsampler","sample","currently","accepts","iterator","as","input","and","output","another","iterator","","this","makes","it","inappropriate","to","use","in","wholestage","codegen","of","sampler","operator","","we","should","add","non","iterator","interface","to","randomsampler","add","non","iterator","interface","to","randomsampler"],"filtered":["randomsampler","sample","currently","accepts","iterator","input","output","another","iterator","","makes","inappropriate","use","wholestage","codegen","sampler","operator","","add","non","iterator","interface","randomsampler","add","non","iterator","interface","randomsampler"],"features":{"type":0,"size":1000,"indices":[0,122,199,224,226,263,297,333,343,372,373,388,432,445,489,495,556,572,583,597,620,663,665,691,763,779,949,993],"values":[1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Read ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html","_c1":"Read ADLS credentials from Credential Provider","document":"Read ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html Read ADLS credentials from Credential Provider","words":["read","adls","credentials","using","hadoop","credentialprovider","api","","see","https","","","hadoop","apache","org","docs","current","hadoop","project","dist","hadoop","common","credentialproviderapi","html","read","adls","credentials","from","credential","provider"],"filtered":["read","adls","credentials","using","hadoop","credentialprovider","api","","see","https","","","hadoop","apache","org","docs","current","hadoop","project","dist","hadoop","common","credentialproviderapi","html","read","adls","credentials","credential","provider"],"features":{"type":0,"size":1000,"indices":[181,256,265,287,372,440,495,515,535,545,624,644,650,652,671,704,710,779,921,954,963,998],"values":[4.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Really simple request  to upgrade fastutil to     x  The current version      x  has some minor API s in the Object xxOpenHashMap structures which is used in many places in Spark  and has been marked deprecated  Plus there is a conflict with another library we are using  saddle    http   saddle github io   which uses a newer version of fastutil  I d be happy to send a PR  I guess a bigger question is do you want to keep using fastutil  SPARK      but Spark uses more than just hashmaps  so that probably requires another discussion","_c1":"Remove Fastutil","document":"Really simple request  to upgrade fastutil to     x  The current version      x  has some minor API s in the Object xxOpenHashMap structures which is used in many places in Spark  and has been marked deprecated  Plus there is a conflict with another library we are using  saddle    http   saddle github io   which uses a newer version of fastutil  I d be happy to send a PR  I guess a bigger question is do you want to keep using fastutil  SPARK      but Spark uses more than just hashmaps  so that probably requires another discussion Remove Fastutil","words":["really","simple","request","","to","upgrade","fastutil","to","","","","","x","","the","current","version","","","","","","x","","has","some","minor","api","s","in","the","object","xxopenhashmap","structures","which","is","used","in","many","places","in","spark","","and","has","been","marked","deprecated","","plus","there","is","a","conflict","with","another","library","we","are","using","","saddle","","","","http","","","saddle","github","io","","","which","uses","a","newer","version","of","fastutil","","i","d","be","happy","to","send","a","pr","","i","guess","a","bigger","question","is","do","you","want","to","keep","using","fastutil","","spark","","","","","","but","spark","uses","more","than","just","hashmaps","","so","that","probably","requires","another","discussion","remove","fastutil"],"filtered":["really","simple","request","","upgrade","fastutil","","","","","x","","current","version","","","","","","x","","minor","api","object","xxopenhashmap","structures","used","many","places","spark","","marked","deprecated","","plus","conflict","another","library","using","","saddle","","","","http","","","saddle","github","io","","","uses","newer","version","fastutil","","d","happy","send","pr","","guess","bigger","question","want","keep","using","fastutil","","spark","","","","","","spark","uses","hashmaps","","probably","requires","another","discussion","remove","fastutil"],"features":{"type":0,"size":1000,"indices":[83,92,94,105,111,125,127,138,141,170,188,197,211,242,261,272,281,288,297,307,310,329,333,343,368,372,388,400,425,441,445,446,487,510,512,523,534,535,580,594,597,605,607,620,624,629,644,650,656,665,666,695,710,712,760,779,810,831,838,860,871,938,942,947,980,993,994,995],"values":[1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,31.0,4.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":1}
{"_c0":"Recently the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too","_c1":"Apply fast serialization on collect limit","document":"Recently the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too Apply fast serialization on collect limit","words":["recently","the","fast","serialization","has","been","introduced","to","collecting","dataframe","dataset","","the","same","technology","can","be","used","on","collect","limit","operator","too","apply","fast","serialization","on","collect","limit"],"filtered":["recently","fast","serialization","introduced","collecting","dataframe","dataset","","technology","used","collect","limit","operator","apply","fast","serialization","collect","limit"],"features":{"type":0,"size":1000,"indices":[34,35,36,82,161,199,222,263,372,388,493,535,580,594,605,644,656,710,713,832,833,843],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Refactoring  Instance  case class out from LOR and LIR  and also cleaning up some code","_c1":"Refactoring  Instance  out from LOR and LIR  and also cleaning up some code","document":"Refactoring  Instance  case class out from LOR and LIR  and also cleaning up some code Refactoring  Instance  out from LOR and LIR  and also cleaning up some code","words":["refactoring","","instance","","case","class","out","from","lor","and","lir","","and","also","cleaning","up","some","code","refactoring","","instance","","out","from","lor","and","lir","","and","also","cleaning","up","some","code"],"filtered":["refactoring","","instance","","case","class","lor","lir","","also","cleaning","code","refactoring","","instance","","lor","lir","","also","cleaning","code"],"features":{"type":0,"size":1000,"indices":[128,225,333,342,372,400,420,534,609,654,727,792,826,921,939],"values":[2.0,2.0,4.0,1.0,6.0,2.0,2.0,1.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0]},"cluster_label":2}
{"_c0":"Remove TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION","_c1":"Removal of TestHiveSharedState","document":"Remove TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION Removal of TestHiveSharedState","words":["remove","testhivesharedstate","","otherwise","","we","are","not","really","testing","the","reflection","logic","based","on","the","setting","of","we","are","not","really","testing","the","reflection","logic","based","on","the","setting","of","catalog","implementation","removal","of","testhivesharedstate"],"filtered":["remove","testhivesharedstate","","otherwise","","really","testing","reflection","logic","based","setting","really","testing","reflection","logic","based","setting","catalog","implementation","removal","testhivesharedstate"],"features":{"type":0,"size":1000,"indices":[18,82,138,288,304,310,343,372,476,489,510,625,651,659,698,710,875,971,993],"values":[2.0,2.0,2.0,1.0,2.0,2.0,3.0,2.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,4.0,1.0,2.0,2.0]},"cluster_label":0}
{"_c0":"Remove useless  databaseName   from  SimpleCatalogRelation","_c1":"Remove databaseName from SimpleCatalogRelation","document":"Remove useless  databaseName   from  SimpleCatalogRelation Remove databaseName from SimpleCatalogRelation","words":["remove","useless","","databasename","","","from","","simplecatalogrelation","remove","databasename","from","simplecatalogrelation"],"filtered":["remove","useless","","databasename","","","","simplecatalogrelation","remove","databasename","simplecatalogrelation"],"features":{"type":0,"size":1000,"indices":[119,288,372,482,906,921],"values":[2.0,2.0,4.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Right now  ContextCleaner referenceBuffer  is ConcurrentLinkedQueue and the time complexity of the  remove  action is O   n    It can be changed to use ConcurrentHashMap whose  remove  is O","_c1":"Change ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster","document":"Right now  ContextCleaner referenceBuffer  is ConcurrentLinkedQueue and the time complexity of the  remove  action is O   n    It can be changed to use ConcurrentHashMap whose  remove  is O Change ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster","words":["right","now","","contextcleaner","referencebuffer","","is","concurrentlinkedqueue","and","the","time","complexity","of","the","","remove","","action","is","o","","","n","","","","it","can","be","changed","to","use","concurrenthashmap","whose","","remove","","is","o","change","contextcleaner","referencebuffer","to","concurrenthashmap","to","make","it","faster"],"filtered":["right","","contextcleaner","referencebuffer","","concurrentlinkedqueue","time","complexity","","remove","","action","o","","","n","","","","changed","use","concurrenthashmap","whose","","remove","","o","change","contextcleaner","referencebuffer","concurrenthashmap","make","faster"],"features":{"type":0,"size":1000,"indices":[39,98,122,157,158,206,281,284,288,333,343,372,388,392,477,489,495,525,530,574,655,656,710,770,833,840,880],"values":[2.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,11.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"Right now  Spark     does not load hive site xml  Based on users  feedback  it seems make sense to still load this conf file  Originally  this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances  Let s avoid of using this way to load hive site xml  Instead  since hive site xml is a normal hadoop conf file  we can first find its url using the classloader and then use Hadoop Configuration s addResource  or add hive site xml as a default resource through Configuration addDefaultResource  to load confs  Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive","_c1":"Bring back the hive site xml support for Spark","document":"Right now  Spark     does not load hive site xml  Based on users  feedback  it seems make sense to still load this conf file  Originally  this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances  Let s avoid of using this way to load hive site xml  Instead  since hive site xml is a normal hadoop conf file  we can first find its url using the classloader and then use Hadoop Configuration s addResource  or add hive site xml as a default resource through Configuration addDefaultResource  to load confs  Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive Bring back the hive site xml support for Spark","words":["right","now","","spark","","","","","does","not","load","hive","site","xml","","based","on","users","","feedback","","it","seems","make","sense","to","still","load","this","conf","file","","originally","","this","file","was","loaded","when","we","load","hiveconf","class","and","all","settings","can","be","retrieved","after","we","create","a","hiveconf","instances","","let","s","avoid","of","using","this","way","to","load","hive","site","xml","","instead","","since","hive","site","xml","is","a","normal","hadoop","conf","file","","we","can","first","find","its","url","using","the","classloader","and","then","use","hadoop","configuration","s","addresource","","or","add","hive","site","xml","as","a","default","resource","through","configuration","adddefaultresource","","to","load","confs","","please","note","that","hive","site","xml","needs","to","be","loaded","into","the","hadoop","conf","used","to","create","metadatahive","bring","back","the","hive","site","xml","support","for","spark"],"filtered":["right","","spark","","","","","load","hive","site","xml","","based","users","","feedback","","seems","make","sense","still","load","conf","file","","originally","","file","loaded","load","hiveconf","class","settings","retrieved","create","hiveconf","instances","","let","avoid","using","way","load","hive","site","xml","","instead","","since","hive","site","xml","normal","hadoop","conf","file","","first","find","url","using","classloader","use","hadoop","configuration","addresource","","add","hive","site","xml","default","resource","configuration","adddefaultresource","","load","confs","","please","note","hive","site","xml","needs","loaded","hadoop","conf","used","create","metadatahive","bring","back","hive","site","xml","support","spark"],"features":{"type":0,"size":1000,"indices":[5,18,36,37,73,76,77,82,92,98,104,105,108,109,159,164,170,180,181,183,187,197,224,234,258,265,281,296,333,343,372,373,381,388,396,398,430,431,432,460,489,495,497,510,525,534,540,543,572,574,585,599,605,624,625,649,656,666,670,674,691,695,698,709,710,738,755,760,796,800,833,841,863,891,909,968,993],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,5.0,2.0,1.0,1.0,2.0,1.0,17.0,3.0,2.0,5.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,7.0,1.0,1.0,3.0]},"cluster_label":11}
{"_c0":"Right now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition","_c1":"SortMergeJoin and BroadcastHashJoin should support condition","document":"Right now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition SortMergeJoin and BroadcastHashJoin should support condition","words":["right","now","","we","use","a","filter","follow","sortmergejoin","or","broadcasthashjoin","for","conditions","","the","result","projection","of","join","could","be","very","expensive","if","they","generate","lots","of","rows","","could","be","reduce","mostly","by","condition","sortmergejoin","and","broadcasthashjoin","should","support","condition"],"filtered":["right","","use","filter","follow","sortmergejoin","broadcasthashjoin","conditions","","result","projection","join","expensive","generate","lots","rows","","reduce","mostly","condition","sortmergejoin","broadcasthashjoin","support","condition"],"features":{"type":0,"size":1000,"indices":[36,48,62,98,170,187,213,223,333,343,372,389,434,489,497,502,574,612,630,640,656,665,670,695,710,728,822,830,865,944,952,956,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Right now  filter push down only works with Project  Aggregate  Generate and Join  they can t be pushed through many other plans","_c1":"Improve filter push down","document":"Right now  filter push down only works with Project  Aggregate  Generate and Join  they can t be pushed through many other plans Improve filter push down","words":["right","now","","filter","push","down","only","works","with","project","","aggregate","","generate","and","join","","they","can","t","be","pushed","through","many","other","plans","improve","filter","push","down"],"filtered":["right","","filter","push","works","project","","aggregate","","generate","join","","pushed","many","plans","improve","filter","push"],"features":{"type":0,"size":1000,"indices":[48,98,188,217,333,372,436,438,522,543,574,640,650,656,671,674,777,830,833,897,899,920,952,978],"values":[1.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"Right now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy  It should be part of constructor of UnsafeRow","_c1":"The numFields of UnsafeRow should not changed by pointTo","document":"Right now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy  It should be part of constructor of UnsafeRow The numFields of UnsafeRow should not changed by pointTo","words":["right","now","","numfields","will","be","passed","in","by","pointto","","","","then","bitsetwidthinbytes","is","calculated","","making","pointto","","","a","little","bit","heavy","","it","should","be","part","of","constructor","of","unsaferow","the","numfields","of","unsaferow","should","not","changed","by","pointto"],"filtered":["right","","numfields","passed","pointto","","","","bitsetwidthinbytes","calculated","","making","pointto","","","little","bit","heavy","","part","constructor","unsaferow","numfields","unsaferow","changed","pointto"],"features":{"type":0,"size":1000,"indices":[18,73,98,122,170,175,223,281,343,353,372,381,391,392,420,445,446,495,533,574,617,656,665,710,740,819,858,996],"values":[1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Right now  we have QualifiedTableName  TableIdentifier  and Seq String  to represent table identifiers  We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name  database name  return unquoted string  and return quoted string  There will be TODOs having  SPARK        in it  Those places need to be updated","_c1":"Consolidate different forms of table identifiers","document":"Right now  we have QualifiedTableName  TableIdentifier  and Seq String  to represent table identifiers  We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name  database name  return unquoted string  and return quoted string  There will be TODOs having  SPARK        in it  Those places need to be updated Consolidate different forms of table identifiers","words":["right","now","","we","have","qualifiedtablename","","tableidentifier","","and","seq","string","","to","represent","table","identifiers","","we","should","only","have","one","form","and","looks","tableidentifier","is","the","best","one","because","it","provides","methods","to","get","table","name","","database","name","","return","unquoted","string","","and","return","quoted","string","","there","will","be","todos","having","","spark","","","","","","","","in","it","","those","places","need","to","be","updated","consolidate","different","forms","of","table","identifiers"],"filtered":["right","","qualifiedtablename","","tableidentifier","","seq","string","","represent","table","identifiers","","one","form","looks","tableidentifier","best","one","provides","methods","get","table","name","","database","name","","return","unquoted","string","","return","quoted","string","","todos","","spark","","","","","","","","","places","need","updated","consolidate","different","forms","table","identifiers"],"features":{"type":0,"size":1000,"indices":[15,21,44,89,98,105,118,127,129,164,173,174,227,231,239,281,299,333,343,372,388,410,420,421,436,445,461,482,490,495,537,574,600,621,656,665,675,710,763,831,837,858,859,888,899,959,993],"values":[2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,18.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,2.0]},"cluster_label":11}
{"_c0":"Right now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations","_c1":"Remove EmptyRow class","document":"Right now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations Remove EmptyRow class","words":["right","now","internalrow","is","megamorphic","because","it","has","many","different","implementations","","we","should","work","towards","having","only","one","or","at","most","two","internalrow","implementations","remove","emptyrow","class"],"filtered":["right","internalrow","megamorphic","many","different","implementations","","work","towards","one","two","internalrow","implementations","remove","emptyrow","class"],"features":{"type":0,"size":1000,"indices":[44,62,89,98,187,188,263,266,281,288,372,408,421,495,527,534,574,580,618,665,675,756,770,899,925,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Right now S Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option","_c1":"S Credentials should support use of CredentialProvider","document":"Right now S Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option S Credentials should support use of CredentialProvider","words":["right","now","s","credentials","only","works","with","cleartext","passwords","in","configs","","as","a","secret","access","key","or","the","uri","","","the","non","uri","version","should","use","credential","providers","with","a","fallback","to","the","clear","text","option","s","credentials","should","support","use","of","credentialprovider"],"filtered":["right","credentials","works","cleartext","passwords","configs","","secret","access","key","uri","","","non","uri","version","use","credential","providers","fallback","clear","text","option","credentials","support","use","credentialprovider"],"features":{"type":0,"size":1000,"indices":[33,57,79,98,169,170,187,197,222,224,265,287,343,355,363,372,388,395,445,489,572,574,650,665,695,710,716,825,899,920,924,963,995],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Right now the Java users cannot use ActorHelper because it uses special Scala syntax  This patch just refactored the codes to provide Java API and add an example","_c1":"Refactor ActorReceiver to support Java","document":"Right now the Java users cannot use ActorHelper because it uses special Scala syntax  This patch just refactored the codes to provide Java API and add an example Refactor ActorReceiver to support Java","words":["right","now","the","java","users","cannot","use","actorhelper","because","it","uses","special","scala","syntax","","this","patch","just","refactored","the","codes","to","provide","java","api","and","add","an","example","refactor","actorreceiver","to","support","java"],"filtered":["right","java","users","use","actorhelper","uses","special","scala","syntax","","patch","refactored","codes","provide","java","api","add","example","refactor","actorreceiver","support","java"],"features":{"type":0,"size":1000,"indices":[11,98,111,137,243,288,307,333,372,373,388,421,428,432,474,489,490,495,574,623,644,695,710,752,755,817,830,931,942,967],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":13}
{"_c0":"Right now the git repo has a branch named  master  in addition to our  trunk  branch  Since  master  is the common place name of the  most recent  branch in git repositories  this is misleading to new folks  It looks like the branch is from     months ago  We should remove it","_c1":"delete spurious  master  branch","document":"Right now the git repo has a branch named  master  in addition to our  trunk  branch  Since  master  is the common place name of the  most recent  branch in git repositories  this is misleading to new folks  It looks like the branch is from     months ago  We should remove it delete spurious  master  branch","words":["right","now","the","git","repo","has","a","branch","named","","master","","in","addition","to","our","","trunk","","branch","","since","","master","","is","the","common","place","name","of","the","","most","recent","","branch","in","git","repositories","","this","is","misleading","to","new","folks","","it","looks","like","the","branch","is","from","","","","","months","ago","","we","should","remove","it","delete","spurious","","master","","branch"],"filtered":["right","git","repo","branch","named","","master","","addition","","trunk","","branch","","since","","master","","common","place","name","","recent","","branch","git","repositories","","misleading","new","folks","","looks","like","branch","","","","","months","ago","","remove","delete","spurious","","master","","branch"],"features":{"type":0,"size":1000,"indices":[15,25,83,98,170,173,191,270,281,288,330,334,343,348,372,373,388,445,495,548,574,575,580,585,599,653,665,704,710,752,770,807,842,877,921,953,954,968,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,18.0,1.0,2.0,2.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"SPARK      need to generate random data which follow Weibull distribution","_c1":"Add WeibullGenerator for RandomDataGenerator","document":"SPARK      need to generate random data which follow Weibull distribution Add WeibullGenerator for RandomDataGenerator","words":["spark","","","","","","need","to","generate","random","data","which","follow","weibull","distribution","add","weibullgenerator","for","randomdatagenerator"],"filtered":["spark","","","","","","need","generate","random","data","follow","weibull","distribution","add","weibullgenerator","randomdatagenerator"],"features":{"type":0,"size":1000,"indices":[36,105,372,388,432,537,540,597,695,728,738,750,798,823,830],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"SPARK      uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately","_c1":"Separate configs between shuffle and RPC","document":"SPARK      uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately Separate configs between shuffle and RPC","words":["spark","","","","","","uses","network","module","to","implement","rpc","","however","","there","are","some","configurations","named","with","","spark","shuffle","","prefix","in","the","network","module","","we","should","refactor","them","and","make","sure","the","user","can","control","them","in","shuffle","and","rpc","separately","separate","configs","between","shuffle","and","rpc"],"filtered":["spark","","","","","","uses","network","module","implement","rpc","","however","","configurations","named","","spark","shuffle","","prefix","network","module","","refactor","make","sure","user","control","shuffle","rpc","separately","separate","configs","shuffle","rpc"],"features":{"type":0,"size":1000,"indices":[74,79,105,111,138,173,181,299,333,372,388,400,445,472,481,494,525,568,623,650,665,673,710,718,783,821,831,833,874,882,924,993],"values":[2.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,3.0,10.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"See  SPARK        We added varargs  again   Though it is technically correct  it often requires that developers do clean assembly  rather than  not clean  assembly  which is a nuisance during development  This JIRA will remove it for now  pending a fix to the Scala compiler","_c1":"Params setDefault should not keep varargs annotation","document":"See  SPARK        We added varargs  again   Though it is technically correct  it often requires that developers do clean assembly  rather than  not clean  assembly  which is a nuisance during development  This JIRA will remove it for now  pending a fix to the Scala compiler Params setDefault should not keep varargs annotation","words":["see","","spark","","","","","","","","we","added","varargs","","again","","","though","it","is","technically","correct","","it","often","requires","that","developers","do","clean","assembly","","rather","than","","not","clean","","assembly","","which","is","a","nuisance","during","development","","this","jira","will","remove","it","for","now","","pending","a","fix","to","the","scala","compiler","params","setdefault","should","not","keep","varargs","annotation"],"filtered":["see","","spark","","","","","","","","added","varargs","","","","though","technically","correct","","often","requires","developers","clean","assembly","","rather","","clean","","assembly","","nuisance","development","","jira","remove","","pending","fix","scala","compiler","params","setdefault","keep","varargs","annotation"],"features":{"type":0,"size":1000,"indices":[18,36,59,86,98,105,116,170,233,261,277,281,288,315,339,340,356,372,373,377,384,388,401,420,437,445,487,490,495,515,534,579,589,594,597,665,710,760,817,821,853,938,993],"values":[2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,18.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"See parent task SPARK","_c1":"python converage ml classification module","document":"See parent task SPARK python converage ml classification module","words":["see","parent","task","spark","python","converage","ml","classification","module"],"filtered":["see","parent","task","spark","python","converage","ml","classification","module"],"features":{"type":0,"size":1000,"indices":[66,105,125,299,324,451,515,549,589],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"See parent task SPARK","_c1":"python converage ml regression module","document":"See parent task SPARK python converage ml regression module","words":["see","parent","task","spark","python","converage","ml","regression","module"],"filtered":["see","parent","task","spark","python","converage","ml","regression","module"],"features":{"type":0,"size":1000,"indices":[66,105,125,299,324,451,515,589,695],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"See parent task SPARK","_c1":"python converage pyspark ml linalg","document":"See parent task SPARK python converage pyspark ml linalg","words":["see","parent","task","spark","python","converage","pyspark","ml","linalg"],"filtered":["see","parent","task","spark","python","converage","pyspark","ml","linalg"],"features":{"type":0,"size":1000,"indices":[66,105,125,324,451,460,509,515,589],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment  We can call the new module mllib local  which might contain local models in the future  The major issue is to remove dependencies on user defined types  The package name will be changed from mllib to ml  For example  Vector will be changed from  org apache spark mllib linalg Vector  to  org apache spark ml linalg Vector   The return vector type in the new ML pipeline will be the one in ML package  however  the existing mllib code will not be touched  As a result  this will potentially break the API  Also  when the vector is loaded from mllib vector by Spark SQL  the vector will automatically converted into the one in ml package","_c1":"Separate out local linear algebra as a standalone module without Spark dependency","document":"Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment  We can call the new module mllib local  which might contain local models in the future  The major issue is to remove dependencies on user defined types  The package name will be changed from mllib to ml  For example  Vector will be changed from  org apache spark mllib linalg Vector  to  org apache spark ml linalg Vector   The return vector type in the new ML pipeline will be the one in ML package  however  the existing mllib code will not be touched  As a result  this will potentially break the API  Also  when the vector is loaded from mllib vector by Spark SQL  the vector will automatically converted into the one in ml package Separate out local linear algebra as a standalone module without Spark dependency","words":["separate","out","linear","algebra","as","a","standalone","module","without","spark","dependency","to","simplify","production","deployment","","we","can","call","the","new","module","mllib","local","","which","might","contain","local","models","in","the","future","","the","major","issue","is","to","remove","dependencies","on","user","defined","types","","the","package","name","will","be","changed","from","mllib","to","ml","","for","example","","vector","will","be","changed","from","","org","apache","spark","mllib","linalg","vector","","to","","org","apache","spark","ml","linalg","vector","","","the","return","vector","type","in","the","new","ml","pipeline","will","be","the","one","in","ml","package","","however","","the","existing","mllib","code","will","not","be","touched","","as","a","result","","this","will","potentially","break","the","api","","also","","when","the","vector","is","loaded","from","mllib","vector","by","spark","sql","","the","vector","will","automatically","converted","into","the","one","in","ml","package","separate","out","local","linear","algebra","as","a","standalone","module","without","spark","dependency"],"filtered":["separate","linear","algebra","standalone","module","without","spark","dependency","simplify","production","deployment","","call","new","module","mllib","local","","might","contain","local","models","future","","major","issue","remove","dependencies","user","defined","types","","package","name","changed","mllib","ml","","example","","vector","changed","","org","apache","spark","mllib","linalg","vector","","","org","apache","spark","ml","linalg","vector","","","return","vector","type","new","ml","pipeline","one","ml","package","","however","","existing","mllib","code","touched","","result","","potentially","break","api","","also","","vector","loaded","mllib","vector","spark","sql","","vector","automatically","converted","one","ml","package","separate","local","linear","algebra","standalone","module","without","spark","dependency"],"features":{"type":0,"size":1000,"indices":[6,15,18,19,25,36,44,55,76,82,105,118,126,146,170,193,196,223,243,266,281,288,299,301,324,329,371,372,373,388,392,398,420,445,449,460,465,467,495,498,505,521,526,528,535,572,588,597,603,608,634,644,654,656,673,686,710,718,742,748,792,818,833,865,866,882,884,891,921,985,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,5.0,2.0,1.0,18.0,1.0,4.0,2.0,1.0,7.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,2.0,2.0,3.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,4.0,1.0,1.0,12.0,2.0,7.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0]},"cluster_label":15}
{"_c0":"SequenceFile s block compression format is too complex and requires   codecs to compress or decompress  It would be good to have a file format that only needs","_c1":"New binary file format","document":"SequenceFile s block compression format is too complex and requires   codecs to compress or decompress  It would be good to have a file format that only needs New binary file format","words":["sequencefile","s","block","compression","format","is","too","complex","and","requires","","","codecs","to","compress","or","decompress","","it","would","be","good","to","have","a","file","format","that","only","needs","new","binary","file","format"],"filtered":["sequencefile","block","compression","format","complex","requires","","","codecs","compress","decompress","","good","file","format","needs","new","binary","file","format"],"features":{"type":0,"size":1000,"indices":[9,25,108,163,168,170,187,197,222,274,281,299,331,333,372,388,441,480,495,511,567,644,656,666,760,812,899,938],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Set minimum version of trunk to JDK","_c1":"JDK   Set minimum version of Hadoop   to JDK","document":"Set minimum version of trunk to JDK JDK   Set minimum version of Hadoop   to JDK","words":["set","minimum","version","of","trunk","to","jdk","jdk","","","set","minimum","version","of","hadoop","","","to","jdk"],"filtered":["set","minimum","version","trunk","jdk","jdk","","","set","minimum","version","hadoop","","","jdk"],"features":{"type":0,"size":1000,"indices":[83,181,255,343,372,388,486,813,995],"values":[1.0,1.0,2.0,2.0,4.0,2.0,3.0,2.0,2.0]},"cluster_label":0}
{"_c0":"Several classes and methods have been deprecated and are creating lots of build warnings in branch      This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated  Any public use will require a deprecated API  We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc","_c1":"Eliminate MLlib     build warnings from deprecations","document":"Several classes and methods have been deprecated and are creating lots of build warnings in branch      This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated  Any public use will require a deprecated API  We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc Eliminate MLlib     build warnings from deprecations","words":["several","classes","and","methods","have","been","deprecated","and","are","creating","lots","of","build","warnings","in","branch","","","","","","this","issue","is","to","identify","and","fix","those","items","","","","","withsgd","classes","","change","to","make","class","not","deprecated","","object","deprecated","","and","public","class","constructor","deprecated","","any","public","use","will","require","a","deprecated","api","","we","need","to","keep","a","non","deprecated","private","api","since","we","cannot","eliminate","certain","uses","","python","api","","streaming","algs","","and","examples","","","","","use","in","pythonmllibapi","","change","to","using","private","constructors","","","","streaming","algs","","no","warnings","after","we","un","deprecate","the","classes","","","","examples","","deprecate","or","change","ones","which","use","deprecated","apis","","","multiclassmetrics","fields","","precision","","etc","eliminate","mllib","","","","","build","warnings","from","deprecations"],"filtered":["several","classes","methods","deprecated","creating","lots","build","warnings","branch","","","","","","issue","identify","fix","items","","","","","withsgd","classes","","change","make","class","deprecated","","object","deprecated","","public","class","constructor","deprecated","","public","use","require","deprecated","api","","need","keep","non","deprecated","private","api","since","eliminate","certain","uses","","python","api","","streaming","algs","","examples","","","","","use","pythonmllibapi","","change","using","private","constructors","","","","streaming","algs","","warnings","un","deprecate","classes","","","","examples","","deprecate","change","ones","use","deprecated","apis","","","multiclassmetrics","fields","","precision","","etc","eliminate","mllib","","","","","build","warnings","deprecations"],"features":{"type":0,"size":1000,"indices":[4,18,19,73,77,90,91,101,111,129,138,146,157,158,170,187,202,224,245,263,275,281,299,333,343,346,350,357,372,373,388,420,434,445,489,498,502,521,525,534,535,536,537,547,567,575,585,586,589,594,597,599,600,620,624,644,650,667,704,710,748,767,809,811,842,921,931,967,971,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,38.0,1.0,4.0,1.0,1.0,3.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":1}
{"_c0":"Shell java has a hardcoded path to  bin bash which is not correct on all platforms  Pointed out by   aw  while reviewing HADOOP","_c1":"Remove hardcoded absolute path for shell executable","document":"Shell java has a hardcoded path to  bin bash which is not correct on all platforms  Pointed out by   aw  while reviewing HADOOP Remove hardcoded absolute path for shell executable","words":["shell","java","has","a","hardcoded","path","to","","bin","bash","which","is","not","correct","on","all","platforms","","pointed","out","by","","","aw","","while","reviewing","hadoop","remove","hardcoded","absolute","path","for","shell","executable"],"filtered":["shell","java","hardcoded","path","","bin","bash","correct","platforms","","pointed","","","aw","","reviewing","hadoop","remove","hardcoded","absolute","path","shell","executable"],"features":{"type":0,"size":1000,"indices":[18,36,44,82,116,123,169,170,181,223,237,281,288,294,372,388,580,597,654,668,707,731,749,883,920,954,967,968],"values":[1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Should we change the hash function for Text to something that handles non ascii characters better  http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup","_c1":"Replace Text hashcode with a better hash function for non ascii strings","document":"Should we change the hash function for Text to something that handles non ascii characters better  http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup Replace Text hashcode with a better hash function for non ascii strings","words":["should","we","change","the","hash","function","for","text","to","something","that","handles","non","ascii","characters","better","","http","","","bailey","svn","sourceforge","net","viewvc","bailey","trunk","src","java","org","apache","bailey","util","hash","java","view","markup","replace","text","hashcode","with","a","better","hash","function","for","non","ascii","strings"],"filtered":["change","hash","function","text","something","handles","non","ascii","characters","better","","http","","","bailey","svn","sourceforge","net","viewvc","bailey","trunk","src","java","org","apache","bailey","util","hash","java","view","markup","replace","text","hashcode","better","hash","function","non","ascii","strings"],"features":{"type":0,"size":1000,"indices":[12,36,37,83,84,158,169,170,224,293,313,372,374,382,388,495,535,537,553,558,650,665,697,699,710,760,787,830,915,941,967,968,988,993,997],"values":[1.0,2.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Since   spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf    When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf","_c1":"Move spark sql hive thriftServer singleSession to SQLConf","document":"Since   spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf    When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf Move spark sql hive thriftServer singleSession to SQLConf","words":["since","","","spark","sql","hive","thriftserver","singlesession","","","is","a","configuration","of","sql","component","","this","conf","can","be","moved","from","","","sparkconf","","","to","","","staticsqlconf","","","","when","we","introduced","","","spark","sql","hive","thriftserver","singlesession","","","","all","the","sql","configuration","can","be","modified","in","different","sessions","","later","","static","sql","configuration","is","added","","it","is","a","perfect","fit","for","","","spark","sql","hive","thriftserver","singlesession","","","","previously","","we","did","the","same","move","for","","","spark","sql","warehouse","dir","","","from","","","sparkconf","","","to","","","staticsqlconf","move","spark","sql","hive","thriftserver","singlesession","to","sqlconf"],"filtered":["since","","","spark","sql","hive","thriftserver","singlesession","","","configuration","sql","component","","conf","moved","","","sparkconf","","","","","staticsqlconf","","","","introduced","","","spark","sql","hive","thriftserver","singlesession","","","","sql","configuration","modified","different","sessions","","later","","static","sql","configuration","added","","perfect","fit","","","spark","sql","hive","thriftserver","singlesession","","","","previously","","move","","","spark","sql","warehouse","dir","","","","","sparkconf","","","","","staticsqlconf","move","spark","sql","hive","thriftserver","singlesession","sqlconf"],"features":{"type":0,"size":1000,"indices":[36,76,89,105,115,118,138,144,170,281,282,343,372,373,375,384,388,445,466,495,585,596,599,607,609,616,656,686,691,692,709,710,713,764,799,813,833,843,889,921,942,968,993],"values":[2.0,1.0,1.0,5.0,4.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,38.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,4.0,4.0,1.0,1.0,1.0,3.0,8.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,1.0,2.0]},"cluster_label":1}
{"_c0":"Since SPARK       is resolved  MapPartitionWithPrepare is not needed anymore","_c1":"Remove PrepareRDD","document":"Since SPARK       is resolved  MapPartitionWithPrepare is not needed anymore Remove PrepareRDD","words":["since","spark","","","","","","","is","resolved","","mappartitionwithprepare","is","not","needed","anymore","remove","preparerdd"],"filtered":["since","spark","","","","","","","resolved","","mappartitionwithprepare","needed","anymore","remove","preparerdd"],"features":{"type":0,"size":1000,"indices":[18,59,105,244,281,288,372,466,585,591,850],"values":[1.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Since ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy","_c1":"PySpark ml evaluation should support save load","document":"Since ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy PySpark ml evaluation should support save load","words":["since","ml","evaluation","has","supported","save","load","at","scala","side","","supporting","it","at","python","side","is","very","straightforward","and","easy","pyspark","ml","evaluation","should","support","save","load"],"filtered":["since","ml","evaluation","supported","save","load","scala","side","","supporting","python","side","straightforward","easy","pyspark","ml","evaluation","support","save","load"],"features":{"type":0,"size":1000,"indices":[258,281,324,333,372,490,495,496,509,520,580,585,589,593,602,665,673,695,756,760,781,944],"values":[2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Since our min version is now JDK   there s hardlink support via   Files    This means we can deprecate the JNI implementation and discontinue usage","_c1":"Deprecate usage of NativeIO link","document":"Since our min version is now JDK   there s hardlink support via   Files    This means we can deprecate the JNI implementation and discontinue usage Deprecate usage of NativeIO link","words":["since","our","min","version","is","now","jdk","","","there","s","hardlink","support","via","","","files","","","","this","means","we","can","deprecate","the","jni","implementation","and","discontinue","usage","deprecate","usage","of","nativeio","link"],"filtered":["since","min","version","jdk","","","hardlink","support","via","","","files","","","","means","deprecate","jni","implementation","discontinue","usage","deprecate","usage","nativeio","link"],"features":{"type":0,"size":1000,"indices":[53,66,98,154,197,275,281,316,333,343,372,373,385,394,486,490,551,585,595,695,698,704,710,831,833,969,993,995],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"Some codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira","_c1":"Remove unused codes in subexpressionEliminationForWholeStageCodegen","document":"Some codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira Remove unused codes in subexpressionEliminationForWholeStageCodegen","words":["some","codes","in","subexpressioneliminationforwholestagecodegen","are","never","used","actually","","remove","them","using","this","jira","remove","unused","codes","in","subexpressioneliminationforwholestagecodegen"],"filtered":["codes","subexpressioneliminationforwholestagecodegen","never","used","actually","","remove","using","jira","remove","unused","codes","subexpressioneliminationforwholestagecodegen"],"features":{"type":0,"size":1000,"indices":[126,128,138,288,372,373,400,445,447,455,605,624,821,830,924],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Some of the checkstyle checks are not realistic  like the line length   leading to spurious    in precommit  Let s disable","_c1":"Disable spurious checkstyle checks","document":"Some of the checkstyle checks are not realistic  like the line length   leading to spurious    in precommit  Let s disable Disable spurious checkstyle checks","words":["some","of","the","checkstyle","checks","are","not","realistic","","like","the","line","length","","","leading","to","spurious","","","","in","precommit","","let","s","disable","disable","spurious","checkstyle","checks"],"filtered":["checkstyle","checks","realistic","","like","line","length","","","leading","spurious","","","","precommit","","let","disable","disable","spurious","checkstyle","checks"],"features":{"type":0,"size":1000,"indices":[18,138,164,175,182,197,225,287,297,330,343,372,388,400,445,710,745,813,877,881],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,7.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":2}
{"_c0":"Some of the monitoring functions could be moved from YARN to Common for easier sharing","_c1":"Move ResourceCalculatorPlugin from YARN to Common","document":"Some of the monitoring functions could be moved from YARN to Common for easier sharing Move ResourceCalculatorPlugin from YARN to Common","words":["some","of","the","monitoring","functions","could","be","moved","from","yarn","to","common","for","easier","sharing","move","resourcecalculatorplugin","from","yarn","to","common"],"filtered":["monitoring","functions","moved","yarn","common","easier","sharing","move","resourcecalculatorplugin","yarn","common"],"features":{"type":0,"size":1000,"indices":[36,138,213,219,282,343,388,400,474,564,587,656,710,762,820,921,954],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher  The only option right now is to created a property file","_c1":"Add   conf to mesos dispatcher process","document":"Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher  The only option right now is to created a property file Add   conf to mesos dispatcher process","words":["sometimes","we","simply","need","to","add","a","property","in","spark","config","for","the","mesos","dispatcher","","the","only","option","right","now","is","to","created","a","property","file","add","","","conf","to","mesos","dispatcher","process"],"filtered":["sometimes","simply","need","add","property","spark","config","mesos","dispatcher","","option","right","created","property","file","add","","","conf","mesos","dispatcher","process"],"features":{"type":0,"size":1000,"indices":[22,36,98,105,108,170,222,262,281,307,319,352,372,388,432,433,445,537,574,709,710,733,899,965,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"SortPartitions and RedistributeData logical operators are not actually used and can be removed  Note that we do have a Sort operator  with global flag false  that subsumed SortPartitions","_c1":"Remove SortPartitions and RedistributeData","document":"SortPartitions and RedistributeData logical operators are not actually used and can be removed  Note that we do have a Sort operator  with global flag false  that subsumed SortPartitions Remove SortPartitions and RedistributeData","words":["sortpartitions","and","redistributedata","logical","operators","are","not","actually","used","and","can","be","removed","","note","that","we","do","have","a","sort","operator","","with","global","flag","false","","that","subsumed","sortpartitions","remove","sortpartitions","and","redistributedata"],"filtered":["sortpartitions","redistributedata","logical","operators","actually","used","removed","","note","sort","operator","","global","flag","false","","subsumed","sortpartitions","remove","sortpartitions","redistributedata"],"features":{"type":0,"size":1000,"indices":[18,82,138,140,170,199,247,288,299,333,372,447,512,534,605,627,650,656,669,720,737,760,792,833,909,932,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Spark SQL currently falls back to Hive for xpath related functions","_c1":"Implement xpath user defined functions","document":"Spark SQL currently falls back to Hive for xpath related functions Implement xpath user defined functions","words":["spark","sql","currently","falls","back","to","hive","for","xpath","related","functions","implement","xpath","user","defined","functions"],"filtered":["spark","sql","currently","falls","back","hive","xpath","related","functions","implement","xpath","user","defined","functions"],"features":{"type":0,"size":1000,"indices":[36,105,126,199,388,430,472,587,599,604,663,686,763,882],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Spark SQL should collapse adjacent   Repartition   operators and only keep the last one","_c1":"Collapse adjacent Repartition operations","document":"Spark SQL should collapse adjacent   Repartition   operators and only keep the last one Collapse adjacent Repartition operations","words":["spark","sql","should","collapse","adjacent","","","repartition","","","operators","and","only","keep","the","last","one","collapse","adjacent","repartition","operations"],"filtered":["spark","sql","collapse","adjacent","","","repartition","","","operators","keep","last","one","collapse","adjacent","repartition","operations"],"features":{"type":0,"size":1000,"indices":[44,105,140,333,372,594,641,665,686,710,735,809,899,904,989],"values":[1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"Spark has an option called   spark localExecution enabled    according to the docs   quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote  This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method   As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark","_c1":"Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled","document":"Spark has an option called   spark localExecution enabled    according to the docs   quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote  This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method   As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled","words":["spark","has","an","option","called","","","spark","localexecution","enabled","","","","according","to","the","docs","","","quote","","enables","spark","to","run","certain","jobs","","such","as","first","","","or","take","","","on","the","driver","","without","sending","tasks","to","the","cluster","","this","can","make","certain","jobs","execute","very","quickly","","but","may","require","shipping","a","whole","partition","of","data","to","the","driver","","","quote","","this","feature","ends","up","adding","quite","a","bit","of","complexity","to","dagscheduler","","especially","in","the","","","runlocallywithinthread","","","method","","but","as","far","as","i","know","nobody","uses","this","feature","","i","searched","the","mailing","list","and","haven","t","seen","any","recent","mentions","of","the","configuration","nor","stacktraces","including","the","runlocally","method","","","as","a","step","towards","scheduler","complexity","reduction","","i","propose","that","we","remove","this","feature","and","all","code","related","to","it","for","spark","remove","dagscheduler","runlocallywithinthread","and","spark","localexecution","enabled"],"filtered":["spark","option","called","","","spark","localexecution","enabled","","","","according","docs","","","quote","","enables","spark","run","certain","jobs","","first","","","take","","","driver","","without","sending","tasks","cluster","","make","certain","jobs","execute","quickly","","may","require","shipping","whole","partition","data","driver","","","quote","","feature","ends","adding","quite","bit","complexity","dagscheduler","","especially","","","runlocallywithinthread","","","method","","far","know","nobody","uses","feature","","searched","mailing","list","haven","seen","recent","mentions","configuration","stacktraces","including","runlocally","method","","","step","towards","scheduler","complexity","reduction","","propose","remove","feature","code","related","spark","remove","dagscheduler","runlocallywithinthread","spark","localexecution","enabled"],"features":{"type":0,"size":1000,"indices":[19,36,45,50,62,82,83,91,105,111,128,135,145,170,183,187,199,222,231,255,272,288,300,329,333,338,341,343,344,359,361,364,372,373,381,388,398,420,439,445,446,455,495,505,522,525,530,545,548,558,572,580,586,587,628,654,665,666,690,691,693,695,710,728,736,746,752,760,765,777,779,817,833,837,842,849,855,884,903,944,954,964,968,978,983,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,29.0,4.0,2.0,6.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":1}
{"_c0":"Spark has configurable L  regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression","_c1":"SparkR spark glm should have configurable regularization parameter","document":"Spark has configurable L  regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression SparkR spark glm should have configurable regularization parameter","words":["spark","has","configurable","l","","regularization","parameter","for","generalized","linear","regression","","it","is","very","important","to","have","them","in","sparkr","so","that","users","can","run","ridge","regression","sparkr","spark","glm","should","have","configurable","regularization","parameter"],"filtered":["spark","configurable","l","","regularization","parameter","generalized","linear","regression","","important","sparkr","users","run","ridge","regression","sparkr","spark","glm","configurable","regularization","parameter"],"features":{"type":0,"size":1000,"indices":[6,24,36,73,105,281,299,329,364,368,372,388,435,445,495,537,580,665,695,755,760,767,833,924,944,964,984,999],"values":[1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added","_c1":"Open up SparkILoop getAddedJars","document":"SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added Open up SparkILoop getAddedJars","words":["sparkiloop","getaddedjars","is","a","useful","method","to","use","so","we","can","programmatically","get","the","list","of","jars","added","open","up","sparkiloop","getaddedjars"],"filtered":["sparkiloop","getaddedjars","useful","method","use","programmatically","get","list","jars","added","open","sparkiloop","getaddedjars"],"features":{"type":0,"size":1000,"indices":[110,128,141,170,272,275,281,343,354,368,384,388,489,654,710,728,783,833,959,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The   FileContext   class currently is annotated as   Evolving    However  at this point we really need to treat it as a   Stable   interface","_c1":"FileContext and AbstractFileSystem should be annotated as a Stable interface","document":"The   FileContext   class currently is annotated as   Evolving    However  at this point we really need to treat it as a   Stable   interface FileContext and AbstractFileSystem should be annotated as a Stable interface","words":["the","","","filecontext","","","class","currently","is","annotated","as","","","evolving","","","","however","","at","this","point","we","really","need","to","treat","it","as","a","","","stable","","","interface","filecontext","and","abstractfilesystem","should","be","annotated","as","a","stable","interface"],"filtered":["","","filecontext","","","class","currently","annotated","","","evolving","","","","however","","point","really","need","treat","","","stable","","","interface","filecontext","abstractfilesystem","annotated","stable","interface"],"features":{"type":0,"size":1000,"indices":[57,170,281,310,333,372,373,382,388,489,495,510,520,534,537,556,572,633,656,665,673,680,710,756,763,993],"values":[1.0,2.0,1.0,1.0,1.0,14.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"The   TaskMetricsUIData updatedBlockStatuses   field is assigned to but never read  increasing the memory consumption of the web UI  We should remove this field","_c1":"Remove unused TaskMetricsUIData updatedBlockStatuses field","document":"The   TaskMetricsUIData updatedBlockStatuses   field is assigned to but never read  increasing the memory consumption of the web UI  We should remove this field Remove unused TaskMetricsUIData updatedBlockStatuses field","words":["the","","","taskmetricsuidata","updatedblockstatuses","","","field","is","assigned","to","but","never","read","","increasing","the","memory","consumption","of","the","web","ui","","we","should","remove","this","field","remove","unused","taskmetricsuidata","updatedblockstatuses","field"],"filtered":["","","taskmetricsuidata","updatedblockstatuses","","","field","assigned","never","read","","increasing","memory","consumption","web","ui","","remove","field","remove","unused","taskmetricsuidata","updatedblockstatuses","field"],"features":{"type":0,"size":1000,"indices":[19,83,92,126,241,281,288,343,372,373,388,413,455,513,626,650,665,710,721,764,788,993],"values":[2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,6.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The   o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed","_c1":"Removed deprecated o a h fs permission AccessControlException","document":"The   o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed Removed deprecated o a h fs permission AccessControlException","words":["the","","","o","a","h","fs","permission","accesscontrolexception","","","has","been","deprecated","for","last","major","releases","and","it","should","be","removed","removed","deprecated","o","a","h","fs","permission","accesscontrolexception"],"filtered":["","","o","h","fs","permission","accesscontrolexception","","","deprecated","last","major","releases","removed","removed","deprecated","o","h","fs","permission","accesscontrolexception"],"features":{"type":0,"size":1000,"indices":[36,135,170,261,269,282,333,372,449,453,495,512,535,580,620,656,665,710,880,989],"values":[1.0,2.0,2.0,2.0,2.0,2.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"The DiskChecker class has a few unused public methods  We can remove them","_c1":"Cleanup DiskChecker interface","document":"The DiskChecker class has a few unused public methods  We can remove them Cleanup DiskChecker interface","words":["the","diskchecker","class","has","a","few","unused","public","methods","","we","can","remove","them","cleanup","diskchecker","interface"],"filtered":["diskchecker","class","unused","public","methods","","remove","cleanup","diskchecker","interface"],"features":{"type":0,"size":1000,"indices":[129,170,288,372,400,455,498,534,536,556,580,710,833,850,924,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The EMC ViPR ECS object storage platform uses proprietary headers starting by x emc    like Amazon does with x amz     Headers starting by x emc   should be included in the signature computation  but it s not done by the Amazon S  Java SDK  it s done by the EMC S  SDK   When s a copy an object it copies all the headers  but when the object includes x emc   headers  it generates a signature mismatch  Removing the x emc   headers from the copy would allow s a to be compatible with the EMC ViPR ECS object storage platform  Removing the x   which aren t x amz   headers from the copy would allow s a to be compatible with any object storage platform which is using proprietary headers","_c1":"Ignore x   and response headers when copying an Amazon S  object","document":"The EMC ViPR ECS object storage platform uses proprietary headers starting by x emc    like Amazon does with x amz     Headers starting by x emc   should be included in the signature computation  but it s not done by the Amazon S  Java SDK  it s done by the EMC S  SDK   When s a copy an object it copies all the headers  but when the object includes x emc   headers  it generates a signature mismatch  Removing the x emc   headers from the copy would allow s a to be compatible with the EMC ViPR ECS object storage platform  Removing the x   which aren t x amz   headers from the copy would allow s a to be compatible with any object storage platform which is using proprietary headers Ignore x   and response headers when copying an Amazon S  object","words":["the","emc","vipr","ecs","object","storage","platform","uses","proprietary","headers","starting","by","x","emc","","","","like","amazon","does","with","x","amz","","","","","headers","starting","by","x","emc","","","should","be","included","in","the","signature","computation","","but","it","s","not","done","by","the","amazon","s","","java","sdk","","it","s","done","by","the","emc","s","","sdk","","","when","s","a","copy","an","object","it","copies","all","the","headers","","but","when","the","object","includes","x","emc","","","headers","","it","generates","a","signature","mismatch","","removing","the","x","emc","","","headers","from","the","copy","would","allow","s","a","to","be","compatible","with","the","emc","vipr","ecs","object","storage","platform","","removing","the","x","","","which","aren","t","x","amz","","","headers","from","the","copy","would","allow","s","a","to","be","compatible","with","any","object","storage","platform","which","is","using","proprietary","headers","ignore","x","","","and","response","headers","when","copying","an","amazon","s","","object"],"filtered":["emc","vipr","ecs","object","storage","platform","uses","proprietary","headers","starting","x","emc","","","","like","amazon","x","amz","","","","","headers","starting","x","emc","","","included","signature","computation","","done","amazon","","java","sdk","","done","emc","","sdk","","","copy","object","copies","headers","","object","includes","x","emc","","","headers","","generates","signature","mismatch","","removing","x","emc","","","headers","copy","allow","compatible","emc","vipr","ecs","object","storage","platform","","removing","x","","","aren","x","amz","","","headers","copy","allow","compatible","object","storage","platform","using","proprietary","headers","ignore","x","","","response","headers","copying","amazon","","object"],"features":{"type":0,"size":1000,"indices":[16,18,64,73,76,83,91,109,111,135,163,167,170,197,216,223,231,234,262,281,330,333,372,388,394,422,445,476,491,495,518,597,616,617,624,650,656,665,698,703,707,710,717,752,777,810,828,852,921,967,968,994,996],"values":[1.0,1.0,3.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,4.0,8.0,3.0,4.0,2.0,1.0,8.0,1.0,1.0,1.0,30.0,2.0,3.0,2.0,2.0,2.0,2.0,4.0,1.0,2.0,2.0,1.0,1.0,9.0,3.0,1.0,1.0,7.0,2.0,11.0,1.0,2.0,1.0,8.0,3.0,1.0,2.0,1.0,3.0,2.0,2.0]},"cluster_label":1}
{"_c0":"The HBase s HMaster port number conflicts with Hadoop kms port number  Both uses        There might be use cases user need kms and HBase present on the same cluster  The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories  Users would have to manually override the default port of either application on their cluster  It would be nice to have different default ports so kms and HBase could naturally coexist","_c1":"Change kms server port number which conflicts with HMaster port number","document":"The HBase s HMaster port number conflicts with Hadoop kms port number  Both uses        There might be use cases user need kms and HBase present on the same cluster  The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories  Users would have to manually override the default port of either application on their cluster  It would be nice to have different default ports so kms and HBase could naturally coexist Change kms server port number which conflicts with HMaster port number","words":["the","hbase","s","hmaster","port","number","conflicts","with","hadoop","kms","port","number","","both","uses","","","","","","","","there","might","be","use","cases","user","need","kms","and","hbase","present","on","the","same","cluster","","the","hbase","is","able","to","encrypt","its","hfiles","but","user","might","need","kms","to","encrypt","other","hdfs","directories","","users","would","have","to","manually","override","the","default","port","of","either","application","on","their","cluster","","it","would","be","nice","to","have","different","default","ports","so","kms","and","hbase","could","naturally","coexist","change","kms","server","port","number","which","conflicts","with","hmaster","port","number"],"filtered":["hbase","hmaster","port","number","conflicts","hadoop","kms","port","number","","uses","","","","","","","","might","use","cases","user","need","kms","hbase","present","cluster","","hbase","able","encrypt","hfiles","user","might","need","kms","encrypt","hdfs","directories","","users","manually","override","default","port","either","application","cluster","","nice","different","default","ports","kms","hbase","naturally","coexist","change","kms","server","port","number","conflicts","hmaster","port","number"],"features":{"type":0,"size":1000,"indices":[29,82,83,89,91,111,117,158,163,169,181,197,213,225,235,239,281,296,299,315,333,343,346,368,370,372,381,388,404,411,489,495,496,522,537,572,576,583,597,615,650,656,674,710,755,809,831,843,863,882,903,916,938,967,985],"values":[4.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,11.0,2.0,4.0,5.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,4.0,1.0,5.0,2.0,3.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"The RPC layer supports QoS but other protocols  ex  webhdfs  are completely unconstrained  Generalizing   Server Call   to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols","_c1":"Design Server Call to be extensible for unified call queue","document":"The RPC layer supports QoS but other protocols  ex  webhdfs  are completely unconstrained  Generalizing   Server Call   to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols Design Server Call to be extensible for unified call queue","words":["the","rpc","layer","supports","qos","but","other","protocols","","ex","","webhdfs","","are","completely","unconstrained","","generalizing","","","server","call","","","to","be","extensible","with","simple","changes","to","the","handlers","will","enable","unifying","the","call","queue","for","multiple","protocols","design","server","call","to","be","extensible","for","unified","call","queue"],"filtered":["rpc","layer","supports","qos","protocols","","ex","","webhdfs","","completely","unconstrained","","generalizing","","","server","call","","","extensible","simple","changes","handlers","enable","unifying","call","queue","multiple","protocols","design","server","call","extensible","unified","call","queue"],"features":{"type":0,"size":1000,"indices":[21,28,36,83,89,138,146,181,189,225,280,363,372,388,389,411,420,424,430,451,565,592,605,622,650,656,674,710,828,980,981,994],"values":[1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,8.0,3.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in  The issue is that HDFS does not update the file size until it s closed  HDFS        and if no new metrics record comes in  then the file size will never be updated  This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour","_c1":"RollingFileSystemSink should eagerly rotate directories","document":"The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in  The issue is that HDFS does not update the file size until it s closed  HDFS        and if no new metrics record comes in  then the file size will never be updated  This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour RollingFileSystemSink should eagerly rotate directories","words":["the","rollingfilesystemsink","only","rolls","over","to","a","new","directory","if","a","new","metrics","record","comes","in","","the","issue","is","that","hdfs","does","not","update","the","file","size","until","it","s","closed","","hdfs","","","","","","","","and","if","no","new","metrics","record","comes","in","","then","the","file","size","will","never","be","updated","","this","jira","is","to","add","a","background","thread","to","the","sink","that","will","eagerly","close","the","file","at","the","top","of","the","hour","rollingfilesystemsink","should","eagerly","rotate","directories"],"filtered":["rollingfilesystemsink","rolls","new","directory","new","metrics","record","comes","","issue","hdfs","update","file","size","closed","","hdfs","","","","","","","","new","metrics","record","comes","","file","size","never","updated","","jira","add","background","thread","sink","eagerly","close","file","top","hour","rollingfilesystemsink","eagerly","rotate","directories"],"features":{"type":0,"size":1000,"indices":[18,25,51,106,108,126,161,170,192,197,281,286,303,333,343,346,352,368,372,373,381,388,396,401,420,421,432,441,445,470,486,490,495,527,656,665,698,710,745,748,756,760,805,821,899,967,992],"values":[1.0,3.0,2.0,2.0,3.0,1.0,1.0,5.0,2.0,1.0,2.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,11.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":15}
{"_c0":"The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark      Also  the feature in SPARK      is strictly better than a correct implementation of that feature  We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work","_c1":"Remove references to preferredNodeLocalityData in javadoc and print warning when used","document":"The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark      Also  the feature in SPARK      is strictly better than a correct implementation of that feature  We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work Remove references to preferredNodeLocalityData in javadoc and print warning when used","words":["the","sparkcontext","constructor","that","takes","preferrednodelocalitydata","has","not","worked","since","before","spark","","","","","","also","","the","feature","in","spark","","","","","","is","strictly","better","than","a","correct","implementation","of","that","feature","","we","should","remove","any","documentation","references","to","that","feature","and","print","a","warning","when","it","is","used","saying","it","doesn","t","work","remove","references","to","preferrednodelocalitydata","in","javadoc","and","print","warning","when","used"],"filtered":["sparkcontext","constructor","takes","preferrednodelocalitydata","worked","since","spark","","","","","","also","","feature","spark","","","","","","strictly","better","correct","implementation","feature","","remove","documentation","references","feature","print","warning","used","saying","doesn","work","remove","references","preferrednodelocalitydata","javadoc","print","warning","used"],"features":{"type":0,"size":1000,"indices":[18,36,73,76,91,105,116,125,159,170,254,261,264,274,281,288,333,343,372,388,445,454,495,500,527,546,580,585,605,665,698,710,736,760,777,792,801,886,941,993,995],"values":[1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,1.0,12.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"The StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time  We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well","_c1":"Add API for updateStateByKey to provide batch time as input","document":"The StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time  We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well Add API for updateStateByKey to provide batch time as input","words":["the","statedstream","currently","does","not","provide","the","batch","time","as","input","to","the","state","update","function","","this","is","required","in","cases","where","the","behavior","depends","on","the","batch","start","time","","we","","conviva","","have","been","patching","it","manually","for","the","past","several","spark","versions","but","we","thought","it","might","be","useful","for","others","as","well","add","api","for","updatestatebykey","to","provide","batch","time","as","input"],"filtered":["statedstream","currently","provide","batch","time","input","state","update","function","","required","cases","behavior","depends","batch","start","time","","","conviva","","patching","manually","past","several","spark","versions","thought","might","useful","others","well","add","api","updatestatebykey","provide","batch","time","input"],"features":{"type":0,"size":1000,"indices":[0,18,32,36,82,83,87,103,105,139,157,272,275,281,288,298,299,313,343,349,362,372,373,388,432,445,495,504,505,535,537,547,553,572,576,644,656,671,698,710,735,763,916,985,993,996],"values":[2.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"The Syncable sync   was deprecated in       We should remove it","_c1":"Remove the deprecated Syncable sync   method","document":"The Syncable sync   was deprecated in       We should remove it Remove the deprecated Syncable sync   method","words":["the","syncable","sync","","","was","deprecated","in","","","","","","","we","should","remove","it","remove","the","deprecated","syncable","sync","","","method"],"filtered":["syncable","sync","","","deprecated","","","","","","","remove","remove","deprecated","syncable","sync","","","method"],"features":{"type":0,"size":1000,"indices":[234,288,372,445,495,620,654,665,710,893,919,993],"values":[1.0,2.0,10.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0]},"cluster_label":2}
{"_c0":"The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids  If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier","_c1":"Add computeCost and clusterCenters to KMeansModel in spark ml package","document":"The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids  If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier Add computeCost and clusterCenters to KMeansModel in spark ml package","words":["the","transformer","version","of","kmeansmodel","does","not","currently","have","methods","to","computecost","or","get","the","centers","of","the","cluster","centroids","","if","there","could","be","a","way","to","get","this","either","by","exposing","the","parentmodel","or","by","adding","these","method","it","would","make","things","easier","add","computecost","and","clustercenters","to","kmeansmodel","in","spark","ml","package"],"filtered":["transformer","version","kmeansmodel","currently","methods","computecost","get","centers","cluster","centroids","","way","get","either","exposing","parentmodel","adding","method","make","things","easier","add","computecost","clustercenters","kmeansmodel","spark","ml","package"],"features":{"type":0,"size":1000,"indices":[18,105,129,159,163,170,187,213,223,231,266,299,315,324,333,343,372,373,382,388,432,445,461,474,495,522,525,572,604,628,631,654,656,698,710,763,831,904,922,938,954,959,995],"values":[1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":0}
{"_c0":"The UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients","_c1":"UGI should contain authentication method","document":"The UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients UGI should contain authentication method","words":["the","usergroupinformation","should","contain","authentication","method","in","its","subject","","this","will","be","used","in","hdfs","to","issue","delegation","tokens","only","to","kerberos","authenticated","clients","ugi","should","contain","authentication","method"],"filtered":["usergroupinformation","contain","authentication","method","subject","","used","hdfs","issue","delegation","tokens","kerberos","authenticated","clients","ugi","contain","authentication","method"],"features":{"type":0,"size":1000,"indices":[146,296,301,360,372,373,388,399,420,445,514,605,652,654,656,665,674,710,724,748,805,899,967],"values":[2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The catalyst package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime","_c1":"Remove private sql  and private spark  from catalyst package","document":"The catalyst package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql  and private spark  from catalyst package","words":["the","catalyst","package","is","meant","to","be","internal","","and","as","a","result","it","does","not","make","sense","to","mark","things","as","private","sql","","or","private","spark","","","it","simply","makes","debugging","harder","when","spark","developers","need","to","inspect","the","plans","at","runtime","remove","private","sql","","and","private","spark","","from","catalyst","package"],"filtered":["catalyst","package","meant","internal","","result","make","sense","mark","things","private","sql","","private","spark","","","simply","makes","debugging","harder","spark","developers","need","inspect","plans","runtime","remove","private","sql","","private","spark","","catalyst","package"],"features":{"type":0,"size":1000,"indices":[5,18,38,76,105,170,187,200,266,281,288,295,315,319,327,333,372,374,388,495,508,525,537,572,651,656,686,691,698,704,710,738,756,865,897,904,921],"values":[1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes","_c1":"Move o a h record to hadoop streaming","document":"The classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes Move o a h record to hadoop streaming","words":["the","classes","in","o","a","h","record","have","been","deprecated","for","more","than","a","year","and","a","half","","they","should","be","removed","","as","the","first","step","","the","jira","moves","all","these","classes","into","the","hadoop","streaming","project","","which","is","the","only","user","of","these","classes","move","o","a","h","record","to","hadoop","streaming"],"filtered":["classes","o","h","record","deprecated","year","half","","removed","","first","step","","jira","moves","classes","hadoop","streaming","project","","user","classes","move","o","h","record","hadoop","streaming"],"features":{"type":0,"size":1000,"indices":[36,48,170,181,183,261,263,281,282,286,299,333,343,372,388,445,461,512,535,558,572,597,620,629,656,665,671,703,710,780,809,821,880,882,891,899,968],"values":[1.0,1.0,4.0,2.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"The current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results","_c1":"Allow custom timing control in microbenchmarks","document":"The current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results Allow custom timing control in microbenchmarks","words":["the","current","benchmark","framework","runs","a","code","block","for","several","iterations","and","reports","statistics","","however","there","is","no","way","to","exclude","per","iteration","setup","time","from","the","overall","results","allow","custom","timing","control","in","microbenchmarks"],"filtered":["current","benchmark","framework","runs","code","block","several","iterations","reports","statistics","","however","way","exclude","per","iteration","setup","time","overall","results","allow","custom","timing","control","microbenchmarks"],"features":{"type":0,"size":1000,"indices":[36,96,106,142,157,159,170,231,281,333,337,346,363,372,388,420,435,440,445,457,511,547,599,620,673,675,695,710,739,792,831,874,921],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess","_c1":"Considering output for statistics of logical plan","document":"The current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess Considering output for statistics of logical plan","words":["the","current","implementation","of","statistics","of","unarynode","does","not","considering","output","","for","example","","project","","","we","should","considering","it","to","have","a","better","guess","considering","output","for","statistics","of","logical","plan"],"filtered":["current","implementation","statistics","unarynode","considering","output","","example","","project","","","considering","better","guess","considering","output","statistics","logical","plan"],"features":{"type":0,"size":1000,"indices":[18,36,122,123,142,170,243,247,299,343,372,373,388,495,597,665,671,698,710,941,993],"values":[1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,4.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"The current way we generate these build artifacts is awful  Plus they are ugly and  in the case of release notes  very hard to pick out what is important","_c1":"Rework the changelog and releasenotes","document":"The current way we generate these build artifacts is awful  Plus they are ugly and  in the case of release notes  very hard to pick out what is important Rework the changelog and releasenotes","words":["the","current","way","we","generate","these","build","artifacts","is","awful","","plus","they","are","ugly","and","","in","the","case","of","release","notes","","very","hard","to","pick","out","what","is","important","rework","the","changelog","and","releasenotes"],"filtered":["current","way","generate","build","artifacts","awful","","plus","ugly","","case","release","notes","","hard","pick","important","rework","changelog","releasenotes"],"features":{"type":0,"size":1000,"indices":[48,83,125,138,159,281,318,333,342,343,371,372,388,434,441,445,461,526,536,537,560,654,710,721,830,860,944,948,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"The execution package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime","_c1":"Remove private sql  and private spark  from sql execution package","document":"The execution package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql  and private spark  from sql execution package","words":["the","execution","package","is","meant","to","be","internal","","and","as","a","result","it","does","not","make","sense","to","mark","things","as","private","sql","","or","private","spark","","","it","simply","makes","debugging","harder","when","spark","developers","need","to","inspect","the","plans","at","runtime","remove","private","sql","","and","private","spark","","from","sql","execution","package"],"filtered":["execution","package","meant","internal","","result","make","sense","mark","things","private","sql","","private","spark","","","simply","makes","debugging","harder","spark","developers","need","inspect","plans","runtime","remove","private","sql","","private","spark","","sql","execution","package"],"features":{"type":0,"size":1000,"indices":[5,18,38,76,105,170,187,200,266,281,284,288,295,315,319,327,333,372,374,388,495,508,525,537,572,656,686,691,698,704,710,738,756,865,897,904,921],"values":[1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,6.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP            Some of them are not related to hdfs  e g  TestFTPFileSystem  These files should be moved out from hdfs and should not use hdfs codes    Some of them are testing hdfs features  e g  TestStickyBit  They should be defined under org apache hadoop hdfs package","_c1":"fs tests should not be placed in hdfs","document":"The following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP            Some of them are not related to hdfs  e g  TestFTPFileSystem  These files should be moved out from hdfs and should not use hdfs codes    Some of them are testing hdfs features  e g  TestStickyBit  They should be defined under org apache hadoop hdfs package fs tests should not be placed in hdfs","words":["the","following","tests","are","under","the","org","apache","hadoop","fs","package","but","were","moved","to","hdfs","sub","directory","by","hadoop","","","","","","","","","","","","some","of","them","are","not","related","to","hdfs","","e","g","","testftpfilesystem","","these","files","should","be","moved","out","from","hdfs","and","should","not","use","hdfs","codes","","","","some","of","them","are","testing","hdfs","features","","e","g","","teststickybit","","they","should","be","defined","under","org","apache","hadoop","hdfs","package","fs","tests","should","not","be","placed","in","hdfs"],"filtered":["following","tests","org","apache","hadoop","fs","package","moved","hdfs","sub","directory","hadoop","","","","","","","","","","","","related","hdfs","","e","g","","testftpfilesystem","","files","moved","hdfs","use","hdfs","codes","","","","testing","hdfs","features","","e","g","","teststickybit","","defined","org","apache","hadoop","hdfs","package","fs","tests","placed","hdfs"],"features":{"type":0,"size":1000,"indices":[18,34,39,48,83,91,126,135,138,171,181,199,223,266,333,343,372,388,400,417,445,461,489,495,505,535,551,619,624,654,656,665,710,755,830,878,921,924,962,967,992],"values":[3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,20.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,4.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,7.0,1.0]},"cluster_label":11}
{"_c0":"The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided  I have defined multiple local disks defined for a datanode  dfs data dir  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn true When one of those disks breaks and is unmounted then the mountpoint  such as  data    in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed  The only way around this is to alter the configuration and omit that specific disk configuration  To my opinion  It would be more practical to let Hadoop daemons start when at least   disks partition in the provided list is in a usable state  This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing  This might also be configurable that at least X partitions out of he available ones are in OK state","_c1":"Allow daemon startup when at least    or configurable  disk is in an OK state","document":"The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided  I have defined multiple local disks defined for a datanode  dfs data dir  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn true When one of those disks breaks and is unmounted then the mountpoint  such as  data    in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed  The only way around this is to alter the configuration and omit that specific disk configuration  To my opinion  It would be more practical to let Hadoop daemons start when at least   disks partition in the provided list is in a usable state  This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing  This might also be configurable that at least X partitions out of he available ones are in OK state Allow daemon startup when at least    or configurable  disk is in an OK state","words":["the","given","example","is","if","datanode","disk","definitions","but","should","be","applicable","to","all","configuration","where","a","list","of","disks","are","provided","","i","have","defined","multiple","local","disks","defined","for","a","datanode","","dfs","data","dir","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","true","when","one","of","those","disks","breaks","and","is","unmounted","then","the","mountpoint","","such","as","","data","","","","in","this","example","","becomes","a","regular","directory","which","doesn","t","have","the","valid","permissions","and","possible","directory","structure","hadoop","is","expecting","","when","this","situation","happens","","the","datanode","fails","to","restart","because","of","this","while","actually","we","have","enough","disks","in","an","ok","state","to","proceed","","the","only","way","around","this","is","to","alter","the","configuration","and","omit","that","specific","disk","configuration","","to","my","opinion","","it","would","be","more","practical","to","let","hadoop","daemons","start","when","at","least","","","disks","partition","in","the","provided","list","is","in","a","usable","state","","this","prevents","having","to","roll","out","custom","configurations","for","systems","which","have","temporarily","a","disk","","and","therefor","directory","layout","","missing","","this","might","also","be","configurable","that","at","least","x","partitions","out","of","he","available","ones","are","in","ok","state","allow","daemon","startup","when","at","least","","","","or","configurable","","disk","is","in","an","ok","state"],"filtered":["given","example","datanode","disk","definitions","applicable","configuration","list","disks","provided","","defined","multiple","local","disks","defined","datanode","","dfs","data","dir","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","","data","","","","dfs","dn","true","one","disks","breaks","unmounted","mountpoint","","","data","","","","example","","becomes","regular","directory","doesn","valid","permissions","possible","directory","structure","hadoop","expecting","","situation","happens","","datanode","fails","restart","actually","enough","disks","ok","state","proceed","","way","around","alter","configuration","omit","specific","disk","configuration","","opinion","","practical","let","hadoop","daemons","start","least","","","disks","partition","provided","list","usable","state","","prevents","roll","custom","configurations","systems","temporarily","disk","","therefor","directory","layout","","missing","","might","also","configurable","least","x","partitions","available","ones","ok","state","allow","daemon","startup","least","","","","configurable","","disk","ok","state"],"features":{"type":0,"size":1000,"indices":[26,36,39,44,50,59,76,79,83,100,109,116,126,138,139,157,159,161,163,164,169,170,173,181,187,188,211,231,243,252,272,275,281,297,299,329,333,343,346,349,364,371,372,373,377,381,388,421,445,447,457,461,467,484,495,498,500,514,525,567,572,576,592,597,600,608,622,629,636,654,656,665,674,675,680,691,692,695,707,710,722,728,731,737,752,756,757,760,764,777,792,794,810,826,829,840,880,892,899,916,919,935,964,968,982,985,992,993,994,996,997],"values":[1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,6.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,8.0,6.0,1.0,4.0,1.0,4.0,4.0,2.0,4.0,1.0,1.0,47.0,6.0,1.0,1.0,7.0,1.0,6.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,8.0,1.0,7.0,1.0,2.0,2.0,1.0,2.0,3.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,4.0,1.0,1.0]},"cluster_label":5}
{"_c0":"The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit  This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v  external interface that doesn t involve a complex type hierarchy","_c1":"Simplify accumulators and task metrics","document":"The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit  This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v  external interface that doesn t involve a complex type hierarchy Simplify accumulators and task metrics","words":["the","goal","of","the","ticket","is","to","simplify","both","the","external","interface","and","the","internal","implementation","for","accumulators","and","metrics","","they","are","unnecessarily","convoluted","and","we","should","be","able","to","simplify","them","quite","a","bit","","this","is","an","umbrella","ticket","and","i","will","iteratively","create","new","tasks","as","my","investigation","goes","on","","at","a","high","level","","i","d","would","like","to","create","better","abstractions","for","internal","implementations","","as","well","as","creating","a","simplified","accumulator","v","","external","interface","that","doesn","t","involve","a","complex","type","hierarchy","simplify","accumulators","and","task","metrics"],"filtered":["goal","ticket","simplify","external","interface","internal","implementation","accumulators","metrics","","unnecessarily","convoluted","able","simplify","quite","bit","","umbrella","ticket","iteratively","create","new","tasks","investigation","goes","","high","level","","d","like","create","better","abstractions","internal","implementations","","well","creating","simplified","accumulator","v","","external","interface","doesn","involve","complex","type","hierarchy","simplify","accumulators","task","metrics"],"features":{"type":0,"size":1000,"indices":[9,19,25,36,48,82,89,94,106,116,138,157,163,170,173,210,236,255,265,281,295,329,330,333,340,343,359,372,373,388,392,414,420,443,446,451,472,477,496,500,526,556,572,586,656,665,676,698,710,752,756,760,767,777,848,863,916,924,925,941,977,993],"values":[1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0,2.0,1.0,5.0,1.0,1.0,1.0,6.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"The hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for   x","_c1":"Remove hadoop ant from hadoop tools","document":"The hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for   x Remove hadoop ant from hadoop tools","words":["the","hadoop","ant","code","is","an","ancient","kludge","unlikely","to","have","any","users","","still","","we","can","delete","it","from","trunk","as","a","","scream","test","","for","","","x","remove","hadoop","ant","from","hadoop","tools"],"filtered":["hadoop","ant","code","ancient","kludge","unlikely","users","","still","","delete","trunk","","scream","test","","","","x","remove","hadoop","ant","hadoop","tools"],"features":{"type":0,"size":1000,"indices":[36,83,91,110,152,170,181,209,281,288,298,299,372,388,420,495,572,586,710,752,755,800,810,820,833,921,953,970,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The idea is that most of the logic of calling Python actually has nothing to do with RDD  it is really just communicating with a socket    there is nothing distributed about it   and it is only currently depending on RDD because it was written this way  If we extract that functionality out  we can apply it to area of the code that doesn t depend on RDDs  and also make it easier to test","_c1":"Refactor PythonRDD to decouple iterator computation from PythonRDD","document":"The idea is that most of the logic of calling Python actually has nothing to do with RDD  it is really just communicating with a socket    there is nothing distributed about it   and it is only currently depending on RDD because it was written this way  If we extract that functionality out  we can apply it to area of the code that doesn t depend on RDDs  and also make it easier to test Refactor PythonRDD to decouple iterator computation from PythonRDD","words":["the","idea","is","that","most","of","the","logic","of","calling","python","actually","has","nothing","to","do","with","rdd","","it","is","really","just","communicating","with","a","socket","","","","there","is","nothing","distributed","about","it","","","and","it","is","only","currently","depending","on","rdd","because","it","was","written","this","way","","if","we","extract","that","functionality","out","","we","can","apply","it","to","area","of","the","code","that","doesn","t","depend","on","rdds","","and","also","make","it","easier","to","test","refactor","pythonrdd","to","decouple","iterator","computation","from","pythonrdd"],"filtered":["idea","logic","calling","python","actually","nothing","rdd","","really","communicating","socket","","","","nothing","distributed","","","currently","depending","rdd","written","way","","extract","functionality","","apply","area","code","doesn","depend","rdds","","also","make","easier","test","refactor","pythonrdd","decouple","iterator","computation","pythonrdd"],"features":{"type":0,"size":1000,"indices":[35,82,84,109,130,154,159,169,170,177,234,252,281,304,307,310,333,343,365,372,373,388,393,420,421,447,474,482,495,500,525,534,580,583,586,589,623,649,650,654,659,661,663,710,760,763,770,777,792,831,833,870,899,921,958,993],"values":[1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,3.0,3.0,1.0,9.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]},"cluster_label":15}
{"_c0":"The jira proposes an improvement over HADOOP       to remove webhdfs dependencies from the ADL file system client and build out a standalone client  At a high level  this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake  The scheme used for accessing the file system will continue to be adl    azuredatalake net path to file  The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface  The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store","_c1":"Refactor Azure Data Lake Store as an independent FileSystem","document":"The jira proposes an improvement over HADOOP       to remove webhdfs dependencies from the ADL file system client and build out a standalone client  At a high level  this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake  The scheme used for accessing the file system will continue to be adl    azuredatalake net path to file  The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface  The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store Refactor Azure Data Lake Store as an independent FileSystem","words":["the","jira","proposes","an","improvement","over","hadoop","","","","","","","to","remove","webhdfs","dependencies","from","the","adl","file","system","client","and","build","out","a","standalone","client","","at","a","high","level","","this","approach","would","extend","the","hadoop","file","system","class","to","provide","an","implementation","for","accessing","azure","data","lake","","the","scheme","used","for","accessing","the","file","system","will","continue","to","be","adl","","","","azuredatalake","net","path","to","file","","the","azure","data","lake","cloud","store","will","continue","to","provide","a","webhdfs","rest","interface","","the","client","will","access","the","adls","store","using","webhdfs","rest","apis","provided","by","the","adls","store","refactor","azure","data","lake","store","as","an","independent","filesystem"],"filtered":["jira","proposes","improvement","hadoop","","","","","","","remove","webhdfs","dependencies","adl","file","system","client","build","standalone","client","","high","level","","approach","extend","hadoop","file","system","class","provide","implementation","accessing","azure","data","lake","","scheme","used","accessing","file","system","continue","adl","","","","azuredatalake","net","path","file","","azure","data","lake","cloud","store","continue","provide","webhdfs","rest","interface","","client","access","adls","store","using","webhdfs","rest","apis","provided","adls","store","refactor","azure","data","lake","store","independent","filesystem"],"features":{"type":0,"size":1000,"indices":[36,91,95,108,116,135,163,170,181,196,223,224,236,259,280,288,333,352,364,372,373,388,420,424,534,536,556,558,572,605,622,623,624,639,654,656,668,695,698,704,708,710,723,730,731,743,752,756,770,810,821,834,842,866,915,921,956],"values":[2.0,1.0,1.0,4.0,1.0,3.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,14.0,1.0,5.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,9.0,2.0,2.0,1.0,6.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"The newly added Kafka module defines the Kafka dependency as","_c1":"Reduce Kafka dependencies in hadoop kafka module","document":"The newly added Kafka module defines the Kafka dependency as Reduce Kafka dependencies in hadoop kafka module","words":["the","newly","added","kafka","module","defines","the","kafka","dependency","as","reduce","kafka","dependencies","in","hadoop","kafka","module"],"filtered":["newly","added","kafka","module","defines","kafka","dependency","reduce","kafka","dependencies","hadoop","kafka","module"],"features":{"type":0,"size":1000,"indices":[181,196,227,299,384,445,494,502,572,588,710,847],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0]},"cluster_label":13}
{"_c0":"The protoc maven plugin currently generates new Java classes every time  which means Maven always picks up changed files in the build  It would be better if the protoc plugin only generated new Java classes when the source protoc files change","_c1":"Support for incremental generation in the protoc plugin","document":"The protoc maven plugin currently generates new Java classes every time  which means Maven always picks up changed files in the build  It would be better if the protoc plugin only generated new Java classes when the source protoc files change Support for incremental generation in the protoc plugin","words":["the","protoc","maven","plugin","currently","generates","new","java","classes","every","time","","which","means","maven","always","picks","up","changed","files","in","the","build","","it","would","be","better","if","the","protoc","plugin","only","generated","new","java","classes","when","the","source","protoc","files","change","support","for","incremental","generation","in","the","protoc","plugin"],"filtered":["protoc","maven","plugin","currently","generates","new","java","classes","every","time","","means","maven","always","picks","changed","files","build","","better","protoc","plugin","generated","new","java","classes","source","protoc","files","change","support","incremental","generation","protoc","plugin"],"features":{"type":0,"size":1000,"indices":[13,25,36,70,76,117,128,157,158,163,170,195,315,372,392,394,445,495,536,551,562,597,617,656,695,710,763,786,809,899,936,941,967,980],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0]},"cluster_label":0}
{"_c0":"The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP      added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics  As per  RFC      http   www w  org Protocols rfc     rfc     html    HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information    This mechanism is initiated by server sending the      Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI    In case server supports multiple authentication schemes  it may return multiple challenges with a      Authenticate  response  and each challenge may use a different auth scheme    A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge  The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc       Understanding HTTP Authentication https   msdn microsoft com en us library ms         v vs        aspx  On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication  Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags     Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication","_c1":"Support multiple authentication schemes via AuthenticationFilter","document":"The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP      added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics  As per  RFC      http   www w  org Protocols rfc     rfc     html    HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information    This mechanism is initiated by server sending the      Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI    In case server supports multiple authentication schemes  it may return multiple challenges with a      Authenticate  response  and each challenge may use a different auth scheme    A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge  The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc       Understanding HTTP Authentication https   msdn microsoft com en us library ms         v vs        aspx  On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication  Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags     Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication Support multiple authentication schemes via AuthenticationFilter","words":["the","requirement","is","to","support","ldap","based","authentication","scheme","via","hadoop","authenticationfilter","","hadoop","","","","","","added","a","support","to","plug","in","custom","authentication","scheme","","in","addition","to","kerberos","","via","altkerberosauthenticationhandler","class","","but","it","is","based","on","selecting","the","authentication","mechanism","based","on","user","agent","http","header","which","does","not","conform","to","http","protocol","semantics","","as","per","","rfc","","","","","","http","","","www","w","","org","protocols","rfc","","","","","rfc","","","","","html","","","","http","protocol","provides","a","simple","challenge","response","authentication","mechanism","that","can","be","used","by","a","server","to","challenge","a","client","request","and","by","a","client","to","provide","the","necessary","authentication","information","","","","this","mechanism","is","initiated","by","server","sending","the","","","","","","authenticate","","response","with","","www","authenticate","","header","which","includes","at","least","one","challenge","that","indicates","the","authentication","scheme","s","","and","parameters","applicable","to","the","request","uri","","","","in","case","server","supports","multiple","authentication","schemes","","it","may","return","multiple","challenges","with","a","","","","","","authenticate","","response","","and","each","challenge","may","use","a","different","auth","scheme","","","","a","user","agent","must","choose","to","use","the","strongest","auth","scheme","it","understands","and","request","credentials","from","the","user","based","upon","that","challenge","","the","existing","hadoop","authentication","filter","implementation","supports","kerberos","authentication","scheme","and","uses","","negotiate","","as","the","challenge","as","part","of","","www","authenticate","","response","header","","as","per","the","following","documentation","","","negotiate","","challenge","scheme","is","only","applicable","to","kerberos","","and","windows","ntlm","","authentication","schemes","","","spnego","based","kerberos","and","ntlm","http","authentication","http","","","tools","ietf","org","html","rfc","","","","","","","understanding","http","authentication","https","","","msdn","microsoft","com","en","us","library","ms","","","","","","","","","v","vs","","","","","","","","aspx","","on","the","other","hand","for","ldap","authentication","","typically","","basic","","authentication","scheme","is","used","","note","tls","is","mandatory","with","basic","authentication","scheme","","","http","","","httpd","apache","org","docs","trunk","mod","mod","authnz","ldap","html","hence","for","this","feature","","the","idea","would","be","to","provide","a","custom","implementation","of","hadoop","authenticationhandler","and","authenticator","interfaces","which","would","support","both","schemes","","","kerberos","","via","negotiate","auth","challenge","","and","ldap","","via","basic","auth","challenge","","","during","the","authentication","phase","","it","would","send","both","the","challenges","and","let","client","pick","the","appropriate","one","","if","client","responds","with","an","","authorization","","header","tagged","with","","negotiate","","","","it","will","use","kerberos","authentication","","if","client","responds","with","an","","authorization","","header","tagged","with","","basic","","","","it","will","use","ldap","authentication","","note","","","some","http","clients","","e","g","","curl","or","apache","http","java","client","","need","to","be","configured","to","use","one","scheme","over","the","other","e","g","","","","curl","tool","supports","option","to","use","either","kerberos","","via","","","negotiate","flag","","or","username","password","based","authentication","","via","","","basic","and","","u","flags","","","","","apache","httpclient","library","can","be","configured","to","use","specific","authentication","scheme","","http","","","hc","apache","org","httpcomponents","client","ga","tutorial","html","authentication","html","typically","web","browsers","automatically","choose","an","authentication","scheme","based","on","a","notion","of","","strength","","of","security","","e","g","","take","a","look","at","the","","design","of","chrome","browser","for","http","authentication","https","","","www","chromium","org","developers","design","documents","http","authentication","support","multiple","authentication","schemes","via","authenticationfilter"],"filtered":["requirement","support","ldap","based","authentication","scheme","via","hadoop","authenticationfilter","","hadoop","","","","","","added","support","plug","custom","authentication","scheme","","addition","kerberos","","via","altkerberosauthenticationhandler","class","","based","selecting","authentication","mechanism","based","user","agent","http","header","conform","http","protocol","semantics","","per","","rfc","","","","","","http","","","www","w","","org","protocols","rfc","","","","","rfc","","","","","html","","","","http","protocol","provides","simple","challenge","response","authentication","mechanism","used","server","challenge","client","request","client","provide","necessary","authentication","information","","","","mechanism","initiated","server","sending","","","","","","authenticate","","response","","www","authenticate","","header","includes","least","one","challenge","indicates","authentication","scheme","","parameters","applicable","request","uri","","","","case","server","supports","multiple","authentication","schemes","","may","return","multiple","challenges","","","","","","authenticate","","response","","challenge","may","use","different","auth","scheme","","","","user","agent","must","choose","use","strongest","auth","scheme","understands","request","credentials","user","based","upon","challenge","","existing","hadoop","authentication","filter","implementation","supports","kerberos","authentication","scheme","uses","","negotiate","","challenge","part","","www","authenticate","","response","header","","per","following","documentation","","","negotiate","","challenge","scheme","applicable","kerberos","","windows","ntlm","","authentication","schemes","","","spnego","based","kerberos","ntlm","http","authentication","http","","","tools","ietf","org","html","rfc","","","","","","","understanding","http","authentication","https","","","msdn","microsoft","com","en","us","library","ms","","","","","","","","","v","vs","","","","","","","","aspx","","hand","ldap","authentication","","typically","","basic","","authentication","scheme","used","","note","tls","mandatory","basic","authentication","scheme","","","http","","","httpd","apache","org","docs","trunk","mod","mod","authnz","ldap","html","hence","feature","","idea","provide","custom","implementation","hadoop","authenticationhandler","authenticator","interfaces","support","schemes","","","kerberos","","via","negotiate","auth","challenge","","ldap","","via","basic","auth","challenge","","","authentication","phase","","send","challenges","let","client","pick","appropriate","one","","client","responds","","authorization","","header","tagged","","negotiate","","","","use","kerberos","authentication","","client","responds","","authorization","","header","tagged","","basic","","","","use","ldap","authentication","","note","","","http","clients","","e","g","","curl","apache","http","java","client","","need","configured","use","one","scheme","e","g","","","","curl","tool","supports","option","use","either","kerberos","","via","","","negotiate","flag","","username","password","based","authentication","","via","","","basic","","u","flags","","","","","apache","httpclient","library","configured","use","specific","authentication","scheme","","http","","","hc","apache","org","httpcomponents","client","ga","tutorial","html","authentication","html","typically","web","browsers","automatically","choose","authentication","scheme","based","notion","","strength","","security","","e","g","","take","look","","design","chrome","browser","http","authentication","https","","","www","chromium","org","developers","design","documents","http","authentication","support","multiple","authentication","schemes","via","authenticationfilter"],"features":{"type":0,"size":1000,"indices":[3,18,23,31,36,43,44,50,59,80,82,83,84,89,91,92,100,111,118,124,135,146,163,164,170,181,187,189,197,205,208,221,222,223,224,226,229,232,239,261,275,277,281,287,288,314,315,333,342,343,346,352,366,371,372,373,384,388,399,400,409,411,417,418,420,423,428,430,440,441,445,446,452,467,477,489,491,495,499,505,507,523,526,528,534,535,537,540,545,572,592,595,597,605,610,614,615,625,627,634,640,645,646,650,652,653,656,659,665,666,674,695,697,698,704,706,710,722,724,729,735,736,740,752,756,757,760,764,771,788,793,798,800,801,805,808,811,812,825,833,842,848,852,855,863,865,870,878,881,882,885,890,894,899,909,915,921,925,932,939,945,956,957,958,960,967,970,978,979,980,992,994,998,999],"values":[3.0,1.0,1.0,2.0,3.0,2.0,3.0,1.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,7.0,25.0,3.0,1.0,14.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,5.0,2.0,1.0,1.0,1.0,5.0,1.0,6.0,1.0,2.0,1.0,1.0,11.0,1.0,5.0,2.0,1.0,1.0,1.0,158.0,2.0,2.0,14.0,7.0,1.0,1.0,3.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,7.0,2.0,4.0,2.0,1.0,8.0,1.0,10.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,5.0,3.0,7.0,3.0,3.0,4.0,1.0,2.0,7.0,2.0,1.0,1.0,1.0,1.0,7.0,5.0,1.0,4.0,1.0,14.0,2.0,2.0,4.0,6.0,3.0,5.0,2.0,18.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,9.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,12.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0]},"cluster_label":10}
{"_c0":"The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed","_c1":"Ability to clean up subprocesses spawned by Shell when the process exits","document":"The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed Ability to clean up subprocesses spawned by Shell when the process exits","words":["the","runcommand","code","in","shell","java","can","get","into","a","situation","where","it","will","ignore","interruptedexceptions","and","refuse","to","shutdown","due","to","being","in","i","o","waiting","for","the","return","value","of","the","subprocess","that","was","spawned","","we","need","to","allow","for","the","subprocess","to","be","interrupted","and","killed","when","the","shell","process","gets","killed","","currently","the","jvm","will","shutdown","and","all","of","the","subprocesses","will","be","orphaned","and","not","killed","ability","to","clean","up","subprocesses","spawned","by","shell","when","the","process","exits"],"filtered":["runcommand","code","shell","java","get","situation","ignore","interruptedexceptions","refuse","shutdown","due","o","waiting","return","value","subprocess","spawned","","need","allow","subprocess","interrupted","killed","shell","process","gets","killed","","currently","jvm","shutdown","subprocesses","orphaned","killed","ability","clean","subprocesses","spawned","shell","process","exits"],"features":{"type":0,"size":1000,"indices":[18,22,36,76,113,118,123,128,139,170,218,223,231,234,274,300,313,329,333,340,343,352,372,374,388,420,439,445,495,537,556,568,598,622,656,688,697,710,753,760,763,768,833,859,880,891,911,941,959,967,968,993],"values":[1.0,2.0,2.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,2.0,2.0,1.0,5.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"The stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions  Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems","_c1":"Add support for DataFrameStatFunctions in SparkR","document":"The stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions  Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems Add support for DataFrameStatFunctions in SparkR","words":["the","stat","functions","are","defined","in","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframestatfunctions","","currently","only","crosstab","","","is","supported","","functions","to","be","supported","include","","corr","","cov","","freqitems","add","support","for","dataframestatfunctions","in","sparkr"],"filtered":["stat","functions","defined","http","","","spark","apache","org","docs","latest","api","scala","index","html","org","apache","spark","sql","dataframestatfunctions","","currently","crosstab","","","supported","","functions","supported","include","","corr","","cov","","freqitems","add","support","dataframestatfunctions","sparkr"],"features":{"type":0,"size":1000,"indices":[36,105,126,138,281,286,306,307,372,388,401,432,445,490,495,498,535,544,545,552,587,593,597,610,644,652,656,665,686,695,710,763,767,899],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"The system should be able to read in user defined env vars from    hadooprc","_c1":"Add support for  hadooprc","document":"The system should be able to read in user defined env vars from    hadooprc Add support for  hadooprc","words":["the","system","should","be","able","to","read","in","user","defined","env","vars","from","","","","hadooprc","add","support","for","","hadooprc"],"filtered":["system","able","read","user","defined","env","vars","","","","hadooprc","add","support","","hadooprc"],"features":{"type":0,"size":1000,"indices":[36,126,136,372,388,432,445,470,496,537,639,650,656,665,695,710,882,921],"values":[1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"The time windowing function  window  was added to Datasets  This JIRA is to track the status for the R  Python and SQL API","_c1":"Dateset Time Windowing API for Python  R  and SQL","document":"The time windowing function  window  was added to Datasets  This JIRA is to track the status for the R  Python and SQL API Dateset Time Windowing API for Python  R  and SQL","words":["the","time","windowing","function","","window","","was","added","to","datasets","","this","jira","is","to","track","the","status","for","the","r","","python","and","sql","api","dateset","time","windowing","api","for","python","","r","","and","sql"],"filtered":["time","windowing","function","","window","","added","datasets","","jira","track","status","r","","python","sql","api","dateset","time","windowing","api","python","","r","","sql"],"features":{"type":0,"size":1000,"indices":[36,157,234,281,313,325,333,372,373,384,388,484,493,497,511,570,589,644,686,710,821],"values":[2.0,2.0,1.0,3.0,1.0,1.0,2.0,6.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,3.0,1.0]},"cluster_label":2}
{"_c0":"The toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame","_c1":"Add toLocalIterator for Dataset","document":"The toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame Add toLocalIterator for Dataset","words":["the","tolocaliterator","of","rdd","is","super","slow","","we","should","have","a","optimized","implementation","for","dataset","dataframe","add","tolocaliterator","for","dataset"],"filtered":["tolocaliterator","rdd","super","slow","","optimized","implementation","dataset","dataframe","add","tolocaliterator","dataset"],"features":{"type":0,"size":1000,"indices":[36,64,161,170,203,281,299,343,372,432,493,501,659,665,698,710,870,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"The way metrics are currently exposed to the JMX in the NameNode is not helpful  since only the current counters in the record can be fetched and without any context those number mean little  For example the number of files created equal to     only means that in the last period there were     files created but when the new period will end is unknown so fetching     again will either mean another     files or we are fetching the same time period  One of the solutions for this problem will be to have a JMX context that will accumulate the data  being child class of AbstractMetricsContext  and expose different records to the JMX through custom MBeans  This way the information fetched from the JMX will represent the state of things in a more meaningful way","_c1":"JMX Context for Metrics","document":"The way metrics are currently exposed to the JMX in the NameNode is not helpful  since only the current counters in the record can be fetched and without any context those number mean little  For example the number of files created equal to     only means that in the last period there were     files created but when the new period will end is unknown so fetching     again will either mean another     files or we are fetching the same time period  One of the solutions for this problem will be to have a JMX context that will accumulate the data  being child class of AbstractMetricsContext  and expose different records to the JMX through custom MBeans  This way the information fetched from the JMX will represent the state of things in a more meaningful way JMX Context for Metrics","words":["the","way","metrics","are","currently","exposed","to","the","jmx","in","the","namenode","is","not","helpful","","since","only","the","current","counters","in","the","record","can","be","fetched","and","without","any","context","those","number","mean","little","","for","example","the","number","of","files","created","equal","to","","","","","only","means","that","in","the","last","period","there","were","","","","","files","created","but","when","the","new","period","will","end","is","unknown","so","fetching","","","","","again","will","either","mean","another","","","","","files","or","we","are","fetching","the","same","time","period","","one","of","the","solutions","for","this","problem","will","be","to","have","a","jmx","context","that","will","accumulate","the","data","","being","child","class","of","abstractmetricscontext","","and","expose","different","records","to","the","jmx","through","custom","mbeans","","this","way","the","information","fetched","from","the","jmx","will","represent","the","state","of","things","in","a","more","meaningful","way","jmx","context","for","metrics"],"filtered":["way","metrics","currently","exposed","jmx","namenode","helpful","","since","current","counters","record","fetched","without","context","number","mean","little","","example","number","files","created","equal","","","","","means","last","period","","","","","files","created","new","period","end","unknown","fetching","","","","","either","mean","another","","","","","files","fetching","time","period","","one","solutions","problem","jmx","context","accumulate","data","","child","class","abstractmetricscontext","","expose","different","records","jmx","custom","mbeans","","way","information","fetched","jmx","represent","state","things","meaningful","way","jmx","context","metrics"],"features":{"type":0,"size":1000,"indices":[12,18,25,36,44,76,83,89,91,101,106,109,128,138,157,159,170,187,208,243,262,281,284,286,299,333,343,346,349,351,368,372,373,374,388,391,394,402,420,436,445,459,461,487,534,543,551,572,583,585,600,612,629,656,657,685,695,710,716,735,760,763,779,824,826,831,833,859,860,884,899,904,909,921,962,971,978,981,989,993],"values":[2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,3.0,2.0,1.0,5.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,22.0,2.0,1.0,4.0,1.0,1.0,1.0,5.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,16.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"There could be same subquery within a single query  we could reuse the result without running it multiple times","_c1":"Reuse subqueries within single query","document":"There could be same subquery within a single query  we could reuse the result without running it multiple times Reuse subqueries within single query","words":["there","could","be","same","subquery","within","a","single","query","","we","could","reuse","the","result","without","running","it","multiple","times","reuse","subqueries","within","single","query"],"filtered":["subquery","within","single","query","","reuse","result","without","running","multiple","times","reuse","subqueries","within","single","query"],"features":{"type":0,"size":1000,"indices":[170,198,213,242,372,412,475,495,531,592,656,673,674,710,831,865,884,963,993],"values":[1.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There exists a chance that the prefixes keep growing to the maximum pattern length  Then the final local processing step becomes unnecessary","_c1":"Skip local processing in PrefixSpan if there are no small prefixes","document":"There exists a chance that the prefixes keep growing to the maximum pattern length  Then the final local processing step becomes unnecessary Skip local processing in PrefixSpan if there are no small prefixes","words":["there","exists","a","chance","that","the","prefixes","keep","growing","to","the","maximum","pattern","length","","then","the","final","local","processing","step","becomes","unnecessary","skip","local","processing","in","prefixspan","if","there","are","no","small","prefixes"],"filtered":["exists","chance","prefixes","keep","growing","maximum","pattern","length","","final","local","processing","step","becomes","unnecessary","skip","local","processing","prefixspan","small","prefixes"],"features":{"type":0,"size":1000,"indices":[53,138,140,170,242,260,286,311,346,362,372,381,388,422,427,445,558,585,594,608,636,710,742,760,813,831,869],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"There is a ToDo of GenericArrayData class  which is to eliminate boxing unboxing for a primitive array  described  here https   github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L     It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance","_c1":"Prepare GenericArrayData implementation specialized for a primitive array","document":"There is a ToDo of GenericArrayData class  which is to eliminate boxing unboxing for a primitive array  described  here https   github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L     It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance Prepare GenericArrayData implementation specialized for a primitive array","words":["there","is","a","todo","of","genericarraydata","class","","which","is","to","eliminate","boxing","unboxing","for","a","primitive","array","","described","","here","https","","","github","com","apache","spark","blob","master","sql","catalyst","src","main","scala","org","apache","spark","sql","catalyst","util","genericarraydata","scala","l","","","","","it","would","be","good","to","prepare","genericarraydata","implementation","specialized","for","a","primitive","array","to","eliminate","boxing","unboxing","from","the","view","of","runtime","memory","footprint","and","performance","prepare","genericarraydata","implementation","specialized","for","a","primitive","array"],"filtered":["todo","genericarraydata","class","","eliminate","boxing","unboxing","primitive","array","","described","","https","","","github","com","apache","spark","blob","master","sql","catalyst","src","main","scala","org","apache","spark","sql","catalyst","util","genericarraydata","scala","l","","","","","good","prepare","genericarraydata","implementation","specialized","primitive","array","eliminate","boxing","unboxing","view","runtime","memory","footprint","performance","prepare","genericarraydata","implementation","specialized","primitive","array"],"features":{"type":0,"size":1000,"indices":[6,36,105,135,163,165,168,170,221,245,270,281,282,293,333,343,372,374,388,396,472,490,495,500,510,534,535,597,637,651,656,686,698,699,706,710,715,726,748,759,788,831,915,921,982,988,998],"values":[1.0,3.0,2.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,9.0,1.0,3.0,1.0,2.0,2.0,3.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0]},"cluster_label":2}
{"_c0":"There is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths  We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory  Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives","_c1":"support wildcard in libjars argument","document":"There is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths  We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory  Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives support wildcard in libjars argument","words":["there","is","a","problem","when","a","user","job","adds","too","many","dependency","jars","in","their","command","line","","the","hadoop","classpath","part","can","be","addressed","","including","using","wildcards","","","","","","","but","the","same","cannot","be","done","with","the","","libjars","argument","","today","it","takes","only","fully","specified","file","paths","","we","may","want","to","consider","supporting","wildcards","as","a","way","to","help","users","in","this","situation","","the","idea","is","to","handle","it","the","same","way","the","jvm","does","it","","","","","expands","to","the","list","of","jars","in","that","directory","","it","does","not","traverse","into","any","child","directory","","also","","it","probably","would","be","a","good","idea","to","do","it","only","for","libjars","","i","e","","don","t","do","it","for","","files","and","","archives","support","wildcard","in","libjars","argument"],"filtered":["problem","user","job","adds","many","dependency","jars","command","line","","hadoop","classpath","part","addressed","","including","using","wildcards","","","","","","","done","","libjars","argument","","today","takes","fully","specified","file","paths","","may","want","consider","supporting","wildcards","way","help","users","situation","","idea","handle","way","jvm","","","","","expands","list","jars","directory","","traverse","child","directory","","also","","probably","good","idea","libjars","","e","","","files","","archives","support","wildcard","libjars","argument"],"features":{"type":0,"size":1000,"indices":[18,25,36,48,56,76,83,91,101,108,110,116,135,159,163,168,170,175,181,182,188,200,204,235,254,281,297,300,312,329,333,343,347,360,372,373,388,423,433,445,462,470,478,495,496,534,551,572,588,622,624,644,650,656,666,682,695,698,707,710,712,728,735,740,755,760,777,792,831,833,878,882,891,899,931,954,958,985,992,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,23.0,1.0,5.0,3.0,1.0,4.0,1.0,1.0,1.0,7.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"There is little to no reason for it to call hadoop daemon sh anymore","_c1":"hadoop daemons sh should just call hdfs directly","document":"There is little to no reason for it to call hadoop daemon sh anymore hadoop daemons sh should just call hdfs directly","words":["there","is","little","to","no","reason","for","it","to","call","hadoop","daemon","sh","anymore","hadoop","daemons","sh","should","just","call","hdfs","directly"],"filtered":["little","reason","call","hadoop","daemon","sh","anymore","hadoop","daemons","sh","call","hdfs","directly"],"features":{"type":0,"size":1000,"indices":[36,146,181,188,211,268,281,307,346,388,391,495,498,665,831,846,850,967],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"There is no unit test for KMeansSummary in spark ml  Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime   See PR for  SPARK","_c1":"Unit test for spark ml KMeansSummary","document":"There is no unit test for KMeansSummary in spark ml  Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime   See PR for  SPARK Unit test for spark ml KMeansSummary","words":["there","is","no","unit","test","for","kmeanssummary","in","spark","ml","","other","items","which","could","be","fixed","here","","","","add","since","version","to","kmeanssummary","class","","","modify","clustersizes","method","to","match","gmm","method","","to","be","robust","to","empty","clusters","","in","case","we","support","that","sometime","","","see","pr","for","","spark","unit","test","for","spark","ml","kmeanssummary"],"filtered":["unit","test","kmeanssummary","spark","ml","","items","fixed","","","","add","since","version","kmeanssummary","class","","","modify","clustersizes","method","match","gmm","method","","robust","empty","clusters","","case","support","sometime","","","see","pr","","spark","unit","test","spark","ml","kmeanssummary"],"features":{"type":0,"size":1000,"indices":[36,105,135,150,152,185,199,213,254,281,324,332,335,342,346,367,372,388,432,445,499,500,502,515,534,565,585,586,597,607,654,656,674,695,760,831,993,995],"values":[3.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,11.0,4.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"There is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well","_c1":"Add MessageHandler to KinesisUtils createStream similar to Direct Kafka","document":"There is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well Add MessageHandler to KinesisUtils createStream similar to Direct Kafka","words":["there","is","support","for","message","handler","in","direct","kafka","stream","","which","allows","arbitrary","t","to","be","the","output","of","the","stream","instead","of","array","byte","","","this","is","a","very","useful","function","","therefore","should","exist","in","kinesis","as","well","add","messagehandler","to","kinesisutils","createstream","similar","to","direct","kafka"],"filtered":["support","message","handler","direct","kafka","stream","","allows","arbitrary","output","stream","instead","array","byte","","","useful","function","","therefore","exist","kinesis","well","add","messagehandler","kinesisutils","createstream","similar","direct","kafka"],"features":{"type":0,"size":1000,"indices":[36,122,144,150,157,168,170,173,218,272,281,313,343,372,373,388,432,445,480,546,564,572,597,656,665,695,706,710,777,831,847,852,863,910,917,944,969,981],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,4.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"These two classes should be public  since they are used in public code","_c1":"Make DataFrameHolder and DatasetHolder public","document":"These two classes should be public  since they are used in public code Make DataFrameHolder and DatasetHolder public","words":["these","two","classes","should","be","public","","since","they","are","used","in","public","code","make","dataframeholder","and","datasetholder","public"],"filtered":["two","classes","public","","since","used","public","code","make","dataframeholder","datasetholder","public"],"features":{"type":0,"size":1000,"indices":[4,48,138,333,372,408,420,445,461,498,525,585,605,656,665,718,809],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"These two methods were added to Scala Datasets  but are not available in Python yet","_c1":"Add withWatermark and checkpoint to python dataframe","document":"These two methods were added to Scala Datasets  but are not available in Python yet Add withWatermark and checkpoint to python dataframe","words":["these","two","methods","were","added","to","scala","datasets","","but","are","not","available","in","python","yet","add","withwatermark","and","checkpoint","to","python","dataframe"],"filtered":["two","methods","added","scala","datasets","","available","python","yet","add","withwatermark","checkpoint","python","dataframe"],"features":{"type":0,"size":1000,"indices":[18,83,129,138,161,325,333,348,371,372,384,388,404,408,432,445,461,490,589,907,962],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":13}
{"_c0":"They don t bring much value since we now have better unit test coverage for hash joins  This will also help reduce the test time","_c1":"Remove HashJoinCompatibilitySuite","document":"They don t bring much value since we now have better unit test coverage for hash joins  This will also help reduce the test time Remove HashJoinCompatibilitySuite","words":["they","don","t","bring","much","value","since","we","now","have","better","unit","test","coverage","for","hash","joins","","this","will","also","help","reduce","the","test","time","remove","hashjoincompatibilitysuite"],"filtered":["bring","much","value","since","better","unit","test","coverage","hash","joins","","also","help","reduce","test","time","remove","hashjoincompatibilitysuite"],"features":{"type":0,"size":1000,"indices":[36,37,48,98,157,204,205,288,299,335,347,372,373,374,420,502,524,585,586,628,710,711,768,777,792,941,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"They were kept in SQLContext implicits object for binary backward compatibility  in the Spark   x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits","_c1":"Move StringToColumn implicit class into SQLImplicits","document":"They were kept in SQLContext implicits object for binary backward compatibility  in the Spark   x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits Move StringToColumn implicit class into SQLImplicits","words":["they","were","kept","in","sqlcontext","implicits","object","for","binary","backward","compatibility","","in","the","spark","","","x","series","","it","makes","more","sense","for","this","api","to","be","in","sqlimplicits","since","that","s","the","single","class","that","defines","all","the","sql","implicits","move","stringtocolumn","implicit","class","into","sqlimplicits"],"filtered":["kept","sqlcontext","implicits","object","binary","backward","compatibility","","spark","","","x","series","","makes","sense","api","sqlimplicits","since","single","class","defines","sql","implicits","move","stringtocolumn","implicit","class","sqlimplicits"],"features":{"type":0,"size":1000,"indices":[5,36,48,63,105,113,197,282,350,372,373,388,445,451,494,495,531,534,567,571,577,585,629,644,647,650,656,686,691,710,760,810,834,891,908,962,968],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages  Issue brought up by   eronwright   Descriptions below copied from  http   apache spark developers list         n  nabble com Make ML Developer APIs public post     td      html   We plan to make these APIs public in Spark      However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component  Nice to have a consistent format by reusing the trait    ProbabilisticClassifier  Third party components should leverage the complex logic around computing only selected columns  We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL  Users can copy the methods into their own  as needed","_c1":"Make some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier","document":"This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages  Issue brought up by   eronwright   Descriptions below copied from  http   apache spark developers list         n  nabble com Make ML Developer APIs public post     td      html   We plan to make these APIs public in Spark      However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component  Nice to have a consistent format by reusing the trait    ProbabilisticClassifier  Third party components should leverage the complex logic around computing only selected columns  We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL  Users can copy the methods into their own  as needed Make some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier","words":["this","jira","is","for","making","several","ml","apis","public","to","make","it","easier","for","users","to","write","their","own","pipeline","stages","","issue","brought","up","by","","","eronwright","","","descriptions","below","copied","from","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","make","ml","developer","apis","public","post","","","","","td","","","","","","html","","","we","plan","to","make","these","apis","public","in","spark","","","","","","however","","they","will","be","marked","developerapi","and","are","","very","likely","","to","be","broken","in","the","future","","","","vectorudt","","to","define","a","relation","with","a","vector","field","","vectorudt","must","be","instantiated","","","","identifiable","trait","","the","trait","generates","a","unique","identifier","for","the","associated","pipeline","component","","nice","to","have","a","consistent","format","by","reusing","the","trait","","","","probabilisticclassifier","","third","party","components","should","leverage","the","complex","logic","around","computing","only","selected","columns","","we","will","not","yet","make","these","public","","","","schemautils","","third","party","pipeline","components","have","a","need","for","checking","column","types","and","appending","columns","","","","","this","will","probably","be","moved","into","spark","sql","","users","can","copy","the","methods","into","their","own","","as","needed","make","some","ml","apis","public","","vectorudt","","identifiable","","probabilisticclassifier"],"filtered":["jira","making","several","ml","apis","public","make","easier","users","write","pipeline","stages","","issue","brought","","","eronwright","","","descriptions","copied","","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","make","ml","developer","apis","public","post","","","","","td","","","","","","html","","","plan","make","apis","public","spark","","","","","","however","","marked","developerapi","","likely","","broken","future","","","","vectorudt","","define","relation","vector","field","","vectorudt","must","instantiated","","","","identifiable","trait","","trait","generates","unique","identifier","associated","pipeline","component","","nice","consistent","format","reusing","trait","","","","probabilisticclassifier","","third","party","components","leverage","complex","logic","around","computing","selected","columns","","yet","make","public","","","","schemautils","","third","party","pipeline","components","need","checking","column","types","appending","columns","","","","","probably","moved","spark","sql","","users","copy","methods","","needed","make","ml","apis","public","","vectorudt","","identifiable","","probabilisticclassifier"],"features":{"type":0,"size":1000,"indices":[9,18,23,36,43,48,55,92,105,113,119,120,123,128,129,138,139,164,170,174,216,221,222,223,228,235,240,244,262,274,281,291,297,299,304,311,315,324,327,333,341,348,354,370,372,373,385,388,392,400,413,420,445,461,465,474,494,495,498,512,525,537,547,572,586,601,603,608,609,617,644,650,652,653,655,656,665,673,686,708,710,728,737,742,748,755,798,814,821,833,842,852,891,899,921,944,954,969,973,993,996,997],"values":[1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,64.0,3.0,1.0,6.0,1.0,1.0,1.0,3.0,5.0,2.0,2.0,1.0,1.0,2.0,5.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,3.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0]},"cluster_label":5}
{"_c0":"This JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    Token init  authenticate and request an identity token  then persist the token in token cache for later reuse    Token display  show the existing token with its info and attributes in the token cache    Token revoke  revoke a token so that the token will no longer be valid and cannot be used later    Token renew  extend the lifecycle of a token before it s expired","_c1":"Hadoop Token Command","document":"This JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    Token init  authenticate and request an identity token  then persist the token in token cache for later reuse    Token display  show the existing token with its info and attributes in the token cache    Token revoke  revoke a token so that the token will no longer be valid and cannot be used later    Token renew  extend the lifecycle of a token before it s expired Hadoop Token Command","words":["this","jira","is","to","define","commands","for","hadoop","token","","the","scope","of","this","task","is","highlighted","as","following","","","","token","init","","authenticate","and","request","an","identity","token","","then","persist","the","token","in","token","cache","for","later","reuse","","","","token","display","","show","the","existing","token","with","its","info","and","attributes","in","the","token","cache","","","","token","revoke","","revoke","a","token","so","that","the","token","will","no","longer","be","valid","and","cannot","be","used","later","","","","token","renew","","extend","the","lifecycle","of","a","token","before","it","s","expired","hadoop","token","command"],"filtered":["jira","define","commands","hadoop","token","","scope","task","highlighted","following","","","","token","init","","authenticate","request","identity","token","","persist","token","token","cache","later","reuse","","","","token","display","","show","existing","token","info","attributes","token","cache","","","","token","revoke","","revoke","token","token","longer","valid","used","later","","","","token","renew","","extend","lifecycle","token","expired","hadoop","token","command"],"features":{"type":0,"size":1000,"indices":[19,36,39,91,92,124,135,159,163,170,174,181,183,197,245,281,296,333,343,346,368,371,372,373,381,388,420,445,449,451,452,466,467,475,482,495,507,528,572,600,605,650,656,674,710,752,756,760,821,834,868,893,931,975,980],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,18.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,14.0,1.0,1.0,1.0,1.0,2.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous  e g  reject  file    c   Windows  Valid file URI syntax explained at http   blogs msdn com b ie archive            file uris in windows aspx  Also see https   issues apache org jira browse HADOOP","_c1":"Reject invalid Windows URIs","document":"This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous  e g  reject  file    c   Windows  Valid file URI syntax explained at http   blogs msdn com b ie archive            file uris in windows aspx  Also see https   issues apache org jira browse HADOOP Reject invalid Windows URIs","words":["this","jira","is","to","make","handling","of","improperly","constructed","file","uris","for","windows","local","paths","more","rigorous","","e","g","","reject","","file","","","","c","","","windows","","valid","file","uri","syntax","explained","at","http","","","blogs","msdn","com","b","ie","archive","","","","","","","","","","","","file","uris","in","windows","aspx","","also","see","https","","","issues","apache","org","jira","browse","hadoop","reject","invalid","windows","uris"],"filtered":["jira","make","handling","improperly","constructed","file","uris","windows","local","paths","rigorous","","e","g","","reject","","file","","","","c","","","windows","","valid","file","uri","syntax","explained","http","","","blogs","msdn","com","b","ie","archive","","","","","","","","","","","","file","uris","windows","aspx","","also","see","https","","","issues","apache","org","jira","browse","hadoop","reject","invalid","windows","uris"],"features":{"type":0,"size":1000,"indices":[36,39,48,60,83,108,154,176,181,208,221,281,299,313,343,361,372,373,388,417,428,445,474,475,491,495,515,525,535,577,588,608,629,665,722,756,792,798,821,825,878,893,928,998],"values":[1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,25.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"This JIRA is to upgrade the derby version from           to           Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade  The upgrade is due to an already disclosed vulnerability  CVE            in derby            We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this  This was raised on the mailing list at http   apache spark developers list         n  nabble com VOTE Release Apache Spark       RC  tp     p      html by Stephen Hellberg and replied to by Sean Owen  I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the     branch  so ideally we d backport this for all impacted Spark releases  I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag","_c1":"Upgrade derby to           from","document":"This JIRA is to upgrade the derby version from           to           Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade  The upgrade is due to an already disclosed vulnerability  CVE            in derby            We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this  This was raised on the mailing list at http   apache spark developers list         n  nabble com VOTE Release Apache Spark       RC  tp     p      html by Stephen Hellberg and replied to by Sean Owen  I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the     branch  so ideally we d backport this for all impacted Spark releases  I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag Upgrade derby to           from","words":["this","jira","is","to","upgrade","the","derby","version","from","","","","","","","","","","","to","","","","","","","","","","","sean","and","i","figured","that","we","only","use","derby","for","tests","and","so","the","initial","pull","request","was","to","not","include","it","in","the","jars","folder","for","spark","","i","now","believe","it","is","required","based","on","comments","for","the","pull","request","and","so","this","is","only","a","dependency","upgrade","","the","upgrade","is","due","to","an","already","disclosed","vulnerability","","cve","","","","","","","","","","","","in","derby","","","","","","","","","","","","we","used","https","","","www","versioneye","com","search","and","will","be","checking","for","any","other","problems","in","a","variety","of","libraries","too","","investigating","if","we","can","set","up","a","jenkins","job","to","check","our","pom","on","a","regular","basis","so","we","can","stay","ahead","of","the","game","for","matters","like","this","","this","was","raised","on","the","mailing","list","at","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","vote","release","apache","spark","","","","","","","rc","","tp","","","","","p","","","","","","html","by","stephen","hellberg","and","replied","to","by","sean","owen","","i","ve","checked","the","impact","to","previous","spark","releases","and","this","particular","version","of","derby","is","the","only","relatively","recent","and","without","vulnerabilities","version","","i","checked","up","to","the","","","","","branch","","so","ideally","we","d","backport","this","for","all","impacted","spark","releases","","i","ve","marked","this","as","critical","and","ticked","the","important","checkbox","as","it","s","going","to","impact","every","user","","there","isn","t","a","security","component","","should","we","add","one","","","and","hence","the","build","tag","upgrade","derby","to","","","","","","","","","","","from"],"filtered":["jira","upgrade","derby","version","","","","","","","","","","","","","","","","","","","","","sean","figured","use","derby","tests","initial","pull","request","include","jars","folder","spark","","believe","required","based","comments","pull","request","dependency","upgrade","","upgrade","due","already","disclosed","vulnerability","","cve","","","","","","","","","","","","derby","","","","","","","","","","","","used","https","","","www","versioneye","com","search","checking","problems","variety","libraries","","investigating","set","jenkins","job","check","pom","regular","basis","stay","ahead","game","matters","like","","raised","mailing","list","http","","","apache","spark","developers","list","","","","","","","","","n","","nabble","com","vote","release","apache","spark","","","","","","","rc","","tp","","","","","p","","","","","","html","stephen","hellberg","replied","sean","owen","","ve","checked","impact","previous","spark","releases","particular","version","derby","relatively","recent","without","vulnerabilities","version","","checked","","","","","branch","","ideally","d","backport","impacted","spark","releases","","ve","marked","critical","ticked","important","checkbox","going","impact","every","user","","isn","security","component","","add","one","","","hence","build","tag","upgrade","derby","","","","","","","","","",""],"features":{"type":0,"size":1000,"indices":[7,18,26,32,36,44,57,76,82,89,91,92,94,98,105,110,113,115,120,128,129,146,157,170,194,197,214,221,223,234,242,246,252,258,276,281,315,329,330,333,343,348,355,360,368,371,372,373,388,390,401,420,432,433,445,453,456,470,489,493,495,511,512,529,536,537,566,572,575,577,588,597,602,605,606,607,609,619,621,625,634,644,652,655,656,665,669,674,704,710,712,728,752,756,760,777,789,793,805,813,821,831,833,842,852,863,870,879,882,884,899,911,918,921,927,936,956,968,975,983,986,987,993,995,998],"values":[1.0,1.0,1.0,1.0,6.0,1.0,1.0,5.0,3.0,2.0,1.0,3.0,1.0,1.0,5.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,3.0,2.0,2.0,4.0,2.0,2.0,1.0,1.0,5.0,1.0,5.0,1.0,9.0,3.0,1.0,2.0,1.0,4.0,1.0,98.0,7.0,10.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,12.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,3.0,1.0]},"cluster_label":10}
{"_c0":"This allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues","_c1":"Support MetricsSource interface for DecayRpcScheduler Metrics","document":"This allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues Support MetricsSource interface for DecayRpcScheduler Metrics","words":["this","allows","metrics","collector","such","as","ams","to","collect","it","with","metricssink","","the","per","user","rpc","call","counts","","schedule","decisions","and","per","priority","response","time","will","be","useful","to","detect","and","trouble","shoot","hadoop","rpc","server","such","as","namenode","overload","issues","support","metricssource","interface","for","decayrpcscheduler","metrics"],"filtered":["allows","metrics","collector","ams","collect","metricssink","","per","user","rpc","call","counts","","schedule","decisions","per","priority","response","time","useful","detect","trouble","shoot","hadoop","rpc","server","namenode","overload","issues","support","metricssource","interface","decayrpcscheduler","metrics"],"features":{"type":0,"size":1000,"indices":[0,36,106,110,146,157,181,193,207,272,315,333,372,373,388,411,420,423,425,440,445,475,480,495,534,556,557,572,594,650,656,675,695,710,882,919,935,943,971],"values":[1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"This continues the work of SPARK        SPARK       and SPARK       to expose R like model summary in more family and link functions","_c1":"Expose R like summary statistics in SparkR  glm for more family and link functions","document":"This continues the work of SPARK        SPARK       and SPARK       to expose R like model summary in more family and link functions Expose R like summary statistics in SparkR  glm for more family and link functions","words":["this","continues","the","work","of","spark","","","","","","","","spark","","","","","","","and","spark","","","","","","","to","expose","r","like","model","summary","in","more","family","and","link","functions","expose","r","like","summary","statistics","in","sparkr","","glm","for","more","family","and","link","functions"],"filtered":["continues","work","spark","","","","","","","","spark","","","","","","","spark","","","","","","","expose","r","like","model","summary","family","link","functions","expose","r","like","summary","statistics","sparkr","","glm","family","link","functions"],"features":{"type":0,"size":1000,"indices":[36,66,105,109,142,330,333,341,343,372,373,388,445,527,570,587,629,658,710,767,857,873,999],"values":[1.0,2.0,3.0,2.0,1.0,2.0,3.0,2.0,1.0,20.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":11}
{"_c0":"This depends on some internal interface of Spark SQL  should be done after merging into Spark","_c1":"DataFrame UDFs in R","document":"This depends on some internal interface of Spark SQL  should be done after merging into Spark DataFrame UDFs in R","words":["this","depends","on","some","internal","interface","of","spark","sql","","should","be","done","after","merging","into","spark","dataframe","udfs","in","r"],"filtered":["depends","internal","interface","spark","sql","","done","merging","spark","dataframe","udfs","r"],"features":{"type":0,"size":1000,"indices":[77,82,105,161,188,189,295,298,343,372,373,400,445,556,570,656,665,686,707,891],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"This is a debug only version of SPARK        for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark","_c1":"Add debug only socket source in Structured Streaming","document":"This is a debug only version of SPARK        for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark Add debug only socket source in Structured Streaming","words":["this","is","a","debug","only","version","of","spark","","","","","","","","for","tutorials","and","debugging","of","streaming","apps","","it","would","be","nice","to","have","a","text","based","socket","source","similar","to","the","one","in","spark","streaming","","it","will","clearly","be","marked","as","debug","only","so","that","users","don","t","try","to","run","it","in","production","applications","","because","this","type","of","source","cannot","provide","ha","without","storing","a","lot","of","state","in","spark","add","debug","only","socket","source","in","structured","streaming"],"filtered":["debug","version","spark","","","","","","","","tutorials","debugging","streaming","apps","","nice","text","based","socket","source","similar","one","spark","streaming","","clearly","marked","debug","users","try","run","production","applications","","type","source","provide","ha","without","storing","lot","state","spark","add","debug","socket","source","structured","streaming"],"features":{"type":0,"size":1000,"indices":[6,36,44,70,105,148,159,163,169,170,204,214,263,281,288,299,313,333,343,349,364,368,370,372,373,388,393,420,421,432,445,495,512,526,572,625,656,658,697,710,725,735,738,742,755,760,777,811,884,899,910,931,995],"values":[1.0,1.0,1.0,3.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,10.0,2.0,3.0,2.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This is a follow up jira from HADOOP           Now with the findbug warning     As discussed in HADOOP        bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby       rc  release     Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier","_c1":"Follow on fixups after upgraded mini kdc using Kerby","document":"This is a follow up jira from HADOOP           Now with the findbug warning     As discussed in HADOOP        bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby       rc  release     Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier Follow on fixups after upgraded mini kdc using Kerby","words":["this","is","a","follow","up","jira","from","hadoop","","","","","","","","","","","now","with","the","findbug","warning","","","","","as","discussed","in","hadoop","","","","","","","","bq","","why","was","this","committed","with","a","findbugs","errors","rather","than","adding","the","necessary","plumbing","in","pom","xml","to","make","it","go","away","","we","will","add","the","findbugsexcludefile","xml","and","will","get","rid","of","this","given","kerby","","","","","","","rc","","release","","","","","add","the","kerby","version","hadoop","project","pom","xml","bq","","hadoop","project","pom","xml","contains","the","dependencies","of","all","libraries","used","in","all","modules","of","hadoop","","under","dependencymanagement","","only","here","version","will","be","mentioned","","all","other","hadoop","modules","will","inherit","hadoop","project","","so","all","submodules","will","use","the","same","version","","in","submodule","","version","need","not","be","mentioned","in","pom","xml","","this","will","make","version","management","easier","follow","on","fixups","after","upgraded","mini","kdc","using","kerby"],"filtered":["follow","jira","hadoop","","","","","","","","","","","findbug","warning","","","","","discussed","hadoop","","","","","","","","bq","","committed","findbugs","errors","rather","adding","necessary","plumbing","pom","xml","make","go","away","","add","findbugsexcludefile","xml","get","rid","given","kerby","","","","","","","rc","","release","","","","","add","kerby","version","hadoop","project","pom","xml","bq","","hadoop","project","pom","xml","contains","dependencies","libraries","used","modules","hadoop","","dependencymanagement","","version","mentioned","","hadoop","modules","inherit","hadoop","project","","submodules","use","version","","submodule","","version","need","mentioned","pom","xml","","make","version","management","easier","follow","fixups","upgraded","mini","kdc","using","kerby"],"features":{"type":0,"size":1000,"indices":[17,18,39,48,54,60,77,82,92,98,128,135,158,170,177,181,194,196,200,216,231,234,252,261,281,333,343,360,368,371,372,373,388,420,432,437,445,446,474,489,495,525,537,546,547,562,566,572,605,624,650,656,657,671,674,697,710,726,728,737,751,774,781,809,821,828,899,921,943,959,968,973,991,993,995],"values":[1.0,3.0,1.0,1.0,1.0,3.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,4.0,1.0,1.0,42.0,4.0,1.0,6.0,2.0,1.0,5.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,3.0,1.0,1.0,6.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,5.0]},"cluster_label":5}
{"_c0":"This is an umbrella ticket to list issues I found with APIs for the     release","_c1":"Spark     SQL API audit","document":"This is an umbrella ticket to list issues I found with APIs for the     release Spark     SQL API audit","words":["this","is","an","umbrella","ticket","to","list","issues","i","found","with","apis","for","the","","","","","release","spark","","","","","sql","api","audit"],"filtered":["umbrella","ticket","list","issues","found","apis","","","","","release","spark","","","","","sql","api","audit"],"features":{"type":0,"size":1000,"indices":[36,105,255,281,329,371,372,373,388,443,475,533,644,650,686,710,728,752,842,955],"values":[1.0,1.0,1.0,1.0,1.0,1.0,8.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog","_c1":"Remove Direct Usage of HiveClient in InsertIntoHiveTable","document":"This is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog Remove Direct Usage of HiveClient in InsertIntoHiveTable","words":["this","is","another","step","to","get","rid","of","hiveclient","from","","hivesessionstate","","","all","the","metastore","interactions","should","be","through","","externalcatalog","","interface","","however","","the","existing","implementation","of","","insertintohivetable","","","still","requires","hive","clients","","thus","","we","can","remove","hiveclient","by","moving","the","metastore","interactions","into","","externalcatalog","remove","direct","usage","of","hiveclient","in","insertintohivetable"],"filtered":["another","step","get","rid","hiveclient","","hivesessionstate","","","metastore","interactions","","externalcatalog","","interface","","however","","existing","implementation","","insertintohivetable","","","still","requires","hive","clients","","thus","","remove","hiveclient","moving","metastore","interactions","","externalcatalog","remove","direct","usage","hiveclient","insertintohivetable"],"features":{"type":0,"size":1000,"indices":[17,154,219,223,281,288,343,370,371,372,373,388,445,477,513,543,546,556,558,599,643,656,665,673,684,698,710,724,766,779,800,833,852,891,921,938,959,968,993],"values":[1.0,1.0,3.0,1.0,1.0,2.0,3.0,2.0,1.0,13.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d   t rWe  x tq e AOfrsM qb  m BRuv fel i PqR  edit usp sharing                    Old description                           Structured streaming doesn t have support for kafka yet  I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka        https   cwiki apache org confluence display KAFKA KIP      Add a time based log index","_c1":"Structured streaming support for consuming from Kafka","document":"This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d   t rWe  x tq e AOfrsM qb  m BRuv fel i PqR  edit usp sharing                    Old description                           Structured streaming doesn t have support for kafka yet  I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka        https   cwiki apache org confluence display KAFKA KIP      Add a time based log index Structured streaming support for consuming from Kafka","words":["this","is","the","parent","jira","to","track","all","the","work","for","the","building","a","kafka","source","for","structured","streaming","","here","is","the","design","doc","for","an","initial","version","of","the","kafka","source","","https","","","docs","google","com","document","d","","","t","rwe","","x","tq","e","aofrsm","qb","","m","bruv","fel","i","pqr","","edit","usp","sharing","","","","","","","","","","","","","","","","","","","","old","description","","","","","","","","","","","","","","","","","","","","","","","","","","","structured","streaming","doesn","t","have","support","for","kafka","yet","","i","personally","feel","like","time","based","indexing","would","make","for","a","much","better","interface","","but","it","s","been","pushed","back","to","kafka","","","","","","","","https","","","cwiki","apache","org","confluence","display","kafka","kip","","","","","","add","a","time","based","log","index","structured","streaming","support","for","consuming","from","kafka"],"filtered":["parent","jira","track","work","building","kafka","source","structured","streaming","","design","doc","initial","version","kafka","source","","https","","","docs","google","com","document","d","","","rwe","","x","tq","e","aofrsm","qb","","m","bruv","fel","pqr","","edit","usp","sharing","","","","","","","","","","","","","","","","","","","","old","description","","","","","","","","","","","","","","","","","","","","","","","","","","","structured","streaming","doesn","support","kafka","yet","","personally","feel","like","time","based","indexing","make","much","better","interface","","pushed","back","kafka","","","","","","","","https","","","cwiki","apache","org","confluence","display","kafka","kip","","","","","","add","time","based","log","index","structured","streaming","support","consuming","kafka"],"features":{"type":0,"size":1000,"indices":[6,36,53,66,70,83,94,108,113,115,122,135,137,157,163,170,189,197,219,221,253,263,281,299,307,313,329,330,343,348,372,373,388,430,432,436,439,441,449,484,493,495,500,513,524,525,527,535,545,556,596,616,625,631,638,666,672,673,679,681,695,710,752,761,777,810,820,821,847,852,878,921,936,941,968,995,998],"values":[1.0,6.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,70.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":3}
{"_c0":"This is to support order by position in SQL  e g     This should be controlled by config option spark sql groupByOrdinal","_c1":"Support group by ordinal in SQL","document":"This is to support order by position in SQL  e g     This should be controlled by config option spark sql groupByOrdinal Support group by ordinal in SQL","words":["this","is","to","support","order","by","position","in","sql","","e","g","","","","","this","should","be","controlled","by","config","option","spark","sql","groupbyordinal","support","group","by","ordinal","in","sql"],"filtered":["support","order","position","sql","","e","g","","","","","controlled","config","option","spark","sql","groupbyordinal","support","group","ordinal","sql"],"features":{"type":0,"size":1000,"indices":[52,105,161,194,222,223,281,339,352,372,373,388,417,445,656,665,685,686,695,718,878],"values":[1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,5.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This issue aims to expose Scala  bround  function in Python R API   bround  function is implemented in SPARK       by extending current  round  function  We used the following semantics from  Hive https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java","_c1":"Add  bround  function in Python R","document":"This issue aims to expose Scala  bround  function in Python R API   bround  function is implemented in SPARK       by extending current  round  function  We used the following semantics from  Hive https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java Add  bround  function in Python R","words":["this","issue","aims","to","expose","scala","","bround","","function","in","python","r","api","","","bround","","function","is","implemented","in","spark","","","","","","","by","extending","current","","round","","function","","we","used","the","following","semantics","from","","hive","https","","","github","com","apache","hive","blob","master","ql","src","java","org","apache","hadoop","hive","ql","udf","generic","roundutils","java","add","","bround","","function","in","python","r"],"filtered":["issue","aims","expose","scala","","bround","","function","python","r","api","","","bround","","function","implemented","spark","","","","","","","extending","current","","round","","function","","used","following","semantics","","hive","https","","","github","com","apache","hive","blob","master","ql","src","java","org","apache","hadoop","hive","ql","udf","generic","roundutils","java","add","","bround","","function","python","r"],"features":{"type":0,"size":1000,"indices":[4,50,91,105,109,177,181,221,223,270,281,313,327,372,373,388,432,445,483,490,495,510,535,570,589,599,605,644,678,710,726,748,773,844,882,921,967,988,993,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,19.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"This issue replaces all deprecated  SQLContext  occurrences with  SparkSession  in  ML MLLib  module except the following two classes  These two classes use  SQLContext  as their function arguments    ReadWrite scala   TreeModels scala","_c1":"Replace SQLContext with SparkSession in ML MLLib","document":"This issue replaces all deprecated  SQLContext  occurrences with  SparkSession  in  ML MLLib  module except the following two classes  These two classes use  SQLContext  as their function arguments    ReadWrite scala   TreeModels scala Replace SQLContext with SparkSession in ML MLLib","words":["this","issue","replaces","all","deprecated","","sqlcontext","","occurrences","with","","sparksession","","in","","ml","mllib","","module","except","the","following","two","classes","","these","two","classes","use","","sqlcontext","","as","their","function","arguments","","","","readwrite","scala","","","treemodels","scala","replace","sqlcontext","with","sparksession","in","ml","mllib"],"filtered":["issue","replaces","deprecated","","sqlcontext","","occurrences","","sparksession","","","ml","mllib","","module","except","following","two","classes","","two","classes","use","","sqlcontext","","function","arguments","","","","readwrite","scala","","","treemodels","scala","replace","sqlcontext","sparksession","ml","mllib"],"features":{"type":0,"size":1000,"indices":[91,195,235,276,299,313,324,350,372,373,408,445,451,461,489,490,513,521,572,609,620,650,710,748,787,800,809,942,968],"values":[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,14.0,1.0,2.0,2.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":17}
{"_c0":"This method survived the code review and it has been there since v       It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly","_c1":"Hide ALS solveLeastSquares","document":"This method survived the code review and it has been there since v       It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly Hide ALS solveLeastSquares","words":["this","method","survived","the","code","review","and","it","has","been","there","since","v","","","","","","","it","exposes","jblas","types","","let","s","remove","it","from","the","public","api","","i","expect","that","no","one","calls","it","directly","hide","als","solveleastsquares"],"filtered":["method","survived","code","review","since","v","","","","","","","exposes","jblas","types","","let","remove","public","api","","expect","one","calls","directly","hide","als","solveleastsquares"],"features":{"type":0,"size":1000,"indices":[44,164,188,197,288,292,329,333,344,346,372,373,420,465,477,495,498,535,580,585,605,644,654,667,710,760,831,843,846,866,921,978],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This patch removes the blind fallback into Hive for functions  Instead  it creates a whitelist and adds only a small number of functions to the whitelist  i e  the ones we intend to support in the long run in Spark","_c1":"Whitelist the list of Hive fallback functions","document":"This patch removes the blind fallback into Hive for functions  Instead  it creates a whitelist and adds only a small number of functions to the whitelist  i e  the ones we intend to support in the long run in Spark Whitelist the list of Hive fallback functions","words":["this","patch","removes","the","blind","fallback","into","hive","for","functions","","instead","","it","creates","a","whitelist","and","adds","only","a","small","number","of","functions","to","the","whitelist","","i","e","","the","ones","we","intend","to","support","in","the","long","run","in","spark","whitelist","the","list","of","hive","fallback","functions"],"filtered":["patch","removes","blind","fallback","hive","functions","","instead","","creates","whitelist","adds","small","number","functions","whitelist","","e","","ones","intend","support","long","run","spark","whitelist","list","hive","fallback","functions"],"features":{"type":0,"size":1000,"indices":[36,57,105,137,170,171,329,333,343,364,372,373,388,445,479,487,495,567,583,587,599,605,615,695,710,728,742,863,878,891,899,904,985,993],"values":[1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,3.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"This requires some discussion  I m not sure whether  runs  is a useful parameter  It certainly complicates the implementation  We might want to optimize the k means implementation with block matrix operations  In this case  having  runs  may not be worth the trade offs","_c1":"Remove runs from KMeans under the pipeline API","document":"This requires some discussion  I m not sure whether  runs  is a useful parameter  It certainly complicates the implementation  We might want to optimize the k means implementation with block matrix operations  In this case  having  runs  may not be worth the trade offs Remove runs from KMeans under the pipeline API","words":["this","requires","some","discussion","","i","m","not","sure","whether","","runs","","is","a","useful","parameter","","it","certainly","complicates","the","implementation","","we","might","want","to","optimize","the","k","means","implementation","with","block","matrix","operations","","in","this","case","","having","","runs","","may","not","be","worth","the","trade","offs","remove","runs","from","kmeans","under","the","pipeline","api"],"filtered":["requires","discussion","","m","sure","whether","","runs","","useful","parameter","","certainly","complicates","implementation","","might","want","optimize","k","means","implementation","block","matrix","operations","","case","","","runs","","may","worth","trade","offs","remove","runs","kmeans","pipeline","api"],"features":{"type":0,"size":1000,"indices":[18,24,39,96,170,272,281,288,305,329,342,372,373,388,394,400,443,445,456,495,511,531,551,603,638,644,650,656,666,675,695,698,710,712,734,735,743,744,807,821,878,921,938,985,993],"values":[2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,9.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"This was recently released  and it has many improvements  especially the following   quote  Python side  IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes  objects  and members  This makes Py J an ideal tool to explore complex Java APIs  e g   the Eclipse API   Thanks to  jonahkichwacoders  quote  Normally we wrap all the APIs in spark  but for the ones that aren t  this would make it easier to offroad by using the java proxy objects","_c1":"Upgrade pyspark to use py j","document":"This was recently released  and it has many improvements  especially the following   quote  Python side  IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes  objects  and members  This makes Py J an ideal tool to explore complex Java APIs  e g   the Eclipse API   Thanks to  jonahkichwacoders  quote  Normally we wrap all the APIs in spark  but for the ones that aren t  this would make it easier to offroad by using the java proxy objects Upgrade pyspark to use py j","words":["this","was","recently","released","","and","it","has","many","improvements","","especially","the","following","","","quote","","python","side","","ides","and","interactive","interpreters","such","as","ipython","can","now","get","help","text","autocompletion","for","java","classes","","objects","","and","members","","this","makes","py","j","an","ideal","tool","to","explore","complex","java","apis","","e","g","","","the","eclipse","api","","","thanks","to","","jonahkichwacoders","","quote","","normally","we","wrap","all","the","apis","in","spark","","but","for","the","ones","that","aren","t","","this","would","make","it","easier","to","offroad","by","using","the","java","proxy","objects","upgrade","pyspark","to","use","py","j"],"filtered":["recently","released","","many","improvements","","especially","following","","","quote","","python","side","","ides","interactive","interpreters","ipython","get","help","text","autocompletion","java","classes","","objects","","members","","makes","py","j","ideal","tool","explore","complex","java","apis","","e","g","","","eclipse","api","","","thanks","","jonahkichwacoders","","quote","","normally","wrap","apis","spark","","ones","aren","","make","easier","offroad","using","java","proxy","objects","upgrade","pyspark","use","py","j"],"features":{"type":0,"size":1000,"indices":[9,26,34,36,83,91,96,98,100,105,111,135,149,163,169,188,223,226,230,234,242,272,280,281,288,333,336,347,352,372,373,388,417,439,445,474,489,495,509,525,528,543,567,571,572,580,589,612,624,644,691,710,723,746,749,752,760,777,780,781,809,833,842,878,959,967,968,993,997],"values":[1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,19.0,3.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"This will depend a bit on both user demand and the commitment level of maintainers  but I d like to propose the following timeline for yarn alpha support  Spark      Deprecate YARN alpha Spark      Remove YARN alpha  i e  require YARN stable  Since YARN alpha is clearly identified as an alpha API  it seems reasonable to drop support for it in a minor release  However  it does depend a bit whether anyone uses this outside of Yahoo   and that I m not sure of  In the past this API has been used and maintained by Yahoo  but they ll be migrating soon to the stable API s","_c1":"Deprecate and later remove YARN alpha support","document":"This will depend a bit on both user demand and the commitment level of maintainers  but I d like to propose the following timeline for yarn alpha support  Spark      Deprecate YARN alpha Spark      Remove YARN alpha  i e  require YARN stable  Since YARN alpha is clearly identified as an alpha API  it seems reasonable to drop support for it in a minor release  However  it does depend a bit whether anyone uses this outside of Yahoo   and that I m not sure of  In the past this API has been used and maintained by Yahoo  but they ll be migrating soon to the stable API s Deprecate and later remove YARN alpha support","words":["this","will","depend","a","bit","on","both","user","demand","and","the","commitment","level","of","maintainers","","but","i","d","like","to","propose","the","following","timeline","for","yarn","alpha","support","","spark","","","","","","deprecate","yarn","alpha","spark","","","","","","remove","yarn","alpha","","i","e","","require","yarn","stable","","since","yarn","alpha","is","clearly","identified","as","an","alpha","api","","it","seems","reasonable","to","drop","support","for","it","in","a","minor","release","","however","","it","does","depend","a","bit","whether","anyone","uses","this","outside","of","yahoo","","","and","that","i","m","not","sure","of","","in","the","past","this","api","has","been","used","and","maintained","by","yahoo","","but","they","ll","be","migrating","soon","to","the","stable","api","s","deprecate","and","later","remove","yarn","alpha","support"],"filtered":["depend","bit","user","demand","commitment","level","maintainers","","d","like","propose","following","timeline","yarn","alpha","support","","spark","","","","","","deprecate","yarn","alpha","spark","","","","","","remove","yarn","alpha","","e","","require","yarn","stable","","since","yarn","alpha","clearly","identified","alpha","api","","seems","reasonable","drop","support","minor","release","","however","","depend","bit","whether","anyone","uses","outside","yahoo","","","m","sure","","past","api","used","maintained","yahoo","","ll","migrating","soon","stable","api","deprecate","later","remove","yarn","alpha","support"],"features":{"type":0,"size":1000,"indices":[13,18,36,41,48,82,83,91,94,105,111,125,136,167,170,197,209,223,236,275,281,286,288,329,330,333,335,343,371,372,373,388,420,445,446,466,482,495,497,500,511,535,564,572,577,580,585,586,605,638,644,653,656,673,680,688,693,695,697,698,710,752,760,788,793,807,821,850,863,878,882],"values":[1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,3.0,1.0,4.0,1.0,3.0,1.0,22.0,3.0,3.0,1.0,2.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,4.0,1.0,1.0,6.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"ThreadLocalRandom should be used when available in place of ThreadLocal  For JDK  the difference is minimal  but JDK  starts including optimizations for ThreadLocalRandom","_c1":"Replace uses of ThreadLocal with JDK  ThreadLocalRandom","document":"ThreadLocalRandom should be used when available in place of ThreadLocal  For JDK  the difference is minimal  but JDK  starts including optimizations for ThreadLocalRandom Replace uses of ThreadLocal with JDK  ThreadLocalRandom","words":["threadlocalrandom","should","be","used","when","available","in","place","of","threadlocal","","for","jdk","","the","difference","is","minimal","","but","jdk","","starts","including","optimizations","for","threadlocalrandom","replace","uses","of","threadlocal","with","jdk","","threadlocalrandom"],"filtered":["threadlocalrandom","used","available","place","threadlocal","","jdk","","difference","minimal","","jdk","","starts","including","optimizations","threadlocalrandom","replace","uses","threadlocal","jdk","","threadlocalrandom"],"features":{"type":0,"size":1000,"indices":[36,76,83,89,111,115,191,281,343,371,372,392,445,486,501,605,650,656,665,710,729,787,866,954],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0]},"cluster_label":2}
{"_c0":"To implement DDL commands  we added several analyzer rules in sql hive module to analyze DDL related plans  However  our Analyzer currently only have one extending interface  extendedResolutionRules  which defines extra rules that will be run together with other rules in the resolution batch  and doesn t fit DDL rules well  because     DDL rules may do some checking and normalization  but we may do it many times as the resolution batch will run rules again and again  until fixed point  and it s hard to tell if a DDL rule has already done its checking and normalization  It s fine because DDL rules are idempotent  but it s bad for analysis performance    some DDL rules may depend on others  and it s pretty hard to write if conditions to guarantee the dependencies  It will be good if we have a batch which run rules in one pass  so that we can guarantee the dependencies by rules order","_c1":"add a new extending interface in Analyzer for post hoc resolution","document":"To implement DDL commands  we added several analyzer rules in sql hive module to analyze DDL related plans  However  our Analyzer currently only have one extending interface  extendedResolutionRules  which defines extra rules that will be run together with other rules in the resolution batch  and doesn t fit DDL rules well  because     DDL rules may do some checking and normalization  but we may do it many times as the resolution batch will run rules again and again  until fixed point  and it s hard to tell if a DDL rule has already done its checking and normalization  It s fine because DDL rules are idempotent  but it s bad for analysis performance    some DDL rules may depend on others  and it s pretty hard to write if conditions to guarantee the dependencies  It will be good if we have a batch which run rules in one pass  so that we can guarantee the dependencies by rules order add a new extending interface in Analyzer for post hoc resolution","words":["to","implement","ddl","commands","","we","added","several","analyzer","rules","in","sql","hive","module","to","analyze","ddl","related","plans","","however","","our","analyzer","currently","only","have","one","extending","interface","","extendedresolutionrules","","which","defines","extra","rules","that","will","be","run","together","with","other","rules","in","the","resolution","batch","","and","doesn","t","fit","ddl","rules","well","","because","","","","","ddl","rules","may","do","some","checking","and","normalization","","but","we","may","do","it","many","times","as","the","resolution","batch","will","run","rules","again","and","again","","until","fixed","point","","and","it","s","hard","to","tell","if","a","ddl","rule","has","already","done","its","checking","and","normalization","","it","s","fine","because","ddl","rules","are","idempotent","","but","it","s","bad","for","analysis","performance","","","","some","ddl","rules","may","depend","on","others","","and","it","s","pretty","hard","to","write","if","conditions","to","guarantee","the","dependencies","","it","will","be","good","if","we","have","a","batch","which","run","rules","in","one","pass","","so","that","we","can","guarantee","the","dependencies","by","rules","order","add","a","new","extending","interface","in","analyzer","for","post","hoc","resolution"],"filtered":["implement","ddl","commands","","added","several","analyzer","rules","sql","hive","module","analyze","ddl","related","plans","","however","","analyzer","currently","one","extending","interface","","extendedresolutionrules","","defines","extra","rules","run","together","rules","resolution","batch","","doesn","fit","ddl","rules","well","","","","","","ddl","rules","may","checking","normalization","","may","many","times","resolution","batch","run","rules","","fixed","point","","hard","tell","ddl","rule","already","done","checking","normalization","","fine","ddl","rules","idempotent","","bad","analysis","performance","","","","ddl","rules","may","depend","others","","pretty","hard","write","conditions","guarantee","dependencies","","good","batch","run","rules","one","pass","","guarantee","dependencies","rules","order","add","new","extending","interface","analyzer","post","hoc","resolution"],"features":{"type":0,"size":1000,"indices":[8,25,36,44,57,82,83,86,87,103,113,125,138,150,157,168,170,188,196,197,199,223,231,296,299,300,312,316,329,333,343,364,368,372,375,382,384,388,392,400,420,421,431,432,445,467,472,482,487,494,495,497,500,534,547,556,572,580,597,599,644,650,656,666,673,674,686,704,707,710,718,745,755,759,760,763,777,780,805,833,850,882,897,899,973,993],"values":[1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,6.0,1.0,2.0,4.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,2.0,6.0,1.0,3.0,1.0,22.0,1.0,1.0,1.0,5.0,1.0,2.0,3.0,2.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,1.0,13.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,4.0,2.0,1.0,2.0,3.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,7.0]},"cluster_label":11}
{"_c0":"TypeCheck no longer applies in the new Tungsten world","_c1":"Remove TypeCheck in debug package","document":"TypeCheck no longer applies in the new Tungsten world Remove TypeCheck in debug package","words":["typecheck","no","longer","applies","in","the","new","tungsten","world","remove","typecheck","in","debug","package"],"filtered":["typecheck","longer","applies","new","tungsten","world","remove","typecheck","debug","package"],"features":{"type":0,"size":1000,"indices":[25,150,266,285,288,346,365,445,674,710,811,868],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"Umbrella for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details","_c1":"Umbrella  Dynamic subcommands for hadoop shell scripts","document":"Umbrella for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details Umbrella  Dynamic subcommands for hadoop shell scripts","words":["umbrella","for","converting","hadoop","","hdfs","","mapred","","and","yarn","to","allow","for","dynamic","subcommands","","see","first","comment","for","more","details","umbrella","","dynamic","subcommands","for","hadoop","shell","scripts"],"filtered":["umbrella","converting","hadoop","","hdfs","","mapred","","yarn","allow","dynamic","subcommands","","see","first","comment","details","umbrella","","dynamic","subcommands","hadoop","shell","scripts"],"features":{"type":0,"size":1000,"indices":[36,123,128,149,181,183,231,255,294,333,372,388,515,564,573,629,967,970,988,999],"values":[4.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients  We are currently using version       of the SDK  Version       brings some breaking changes  Need to fix code to resolve all these breaking changes and certify that everything works properly","_c1":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients","document":"Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients  We are currently using version       of the SDK  Version       brings some breaking changes  Need to fix code to resolve all these breaking changes and certify that everything works properly Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients","words":["update","wasb","driver","to","use","the","latest","version","","","","","","","","","of","sdk","for","microsoft","azure","storage","clients","","we","are","currently","using","version","","","","","","","of","the","sdk","","version","","","","","","","brings","some","breaking","changes","","need","to","fix","code","to","resolve","all","these","breaking","changes","and","certify","that","everything","works","properly","update","wasb","driver","to","use","the","latest","version","","","","","","","","","of","sdk","for","microsoft","azure","storage","clients"],"filtered":["update","wasb","driver","use","latest","version","","","","","","","","","sdk","microsoft","azure","storage","clients","","currently","using","version","","","","","","","sdk","","version","","","","","","","brings","breaking","changes","","need","fix","code","resolve","breaking","changes","certify","everything","works","properly","update","wasb","driver","use","latest","version","","","","","","","","","sdk","microsoft","azure","storage","clients"],"features":{"type":0,"size":1000,"indices":[36,74,135,138,225,333,343,344,363,372,388,394,400,409,420,445,461,489,491,498,510,537,624,644,710,724,734,760,763,810,827,920,968,993,995],"values":[2.0,1.0,2.0,1.0,1.0,1.0,5.0,2.0,2.0,31.0,4.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0]},"cluster_label":1}
{"_c0":"Update Yetus to","_c1":"Update Yetus to","document":"Update Yetus to Update Yetus to","words":["update","yetus","to","update","yetus","to"],"filtered":["update","yetus","update","yetus"],"features":{"type":0,"size":1000,"indices":[343,388,831],"values":[2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"Upgrade yetus wrapper to be       now that it has passed vote","_c1":"Upgrade to Apache Yetus","document":"Upgrade yetus wrapper to be       now that it has passed vote Upgrade to Apache Yetus","words":["upgrade","yetus","wrapper","to","be","","","","","","","now","that","it","has","passed","vote","upgrade","to","apache","yetus"],"filtered":["upgrade","yetus","wrapper","","","","","","","passed","vote","upgrade","apache","yetus"],"features":{"type":0,"size":1000,"indices":[98,122,214,242,372,388,495,580,656,760,762,831],"values":[1.0,1.0,1.0,2.0,6.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases","_c1":"Use sqlContext from MLlibTestSparkContext for spark ml test suites","document":"Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases Use sqlContext from MLlibTestSparkContext for spark ml test suites","words":["use","sqlcontext","from","mllibtestsparkcontext","rather","than","creating","new","one","for","spark","ml","test","cases","use","sqlcontext","from","mllibtestsparkcontext","for","spark","ml","test","suites"],"filtered":["use","sqlcontext","mllibtestsparkcontext","rather","creating","new","one","spark","ml","test","cases","use","sqlcontext","mllibtestsparkcontext","spark","ml","test","suites"],"features":{"type":0,"size":1000,"indices":[25,36,44,105,261,324,437,451,489,576,586,641,767,921,931],"values":[1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"ViewFileSystem doesn t override FileSystem getLinkTarget    So  when view filesystem is used to resolve the symbolic links  the default FileSystem implementation throws UnsupportedOperationException  The proposal is to define getLinkTarget   for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links  Path thus returned is preferred to be a viewfs qualified path  so that it can be used again on the ViewFileSystem handle","_c1":"Implement getLinkTarget for ViewFileSystem","document":"ViewFileSystem doesn t override FileSystem getLinkTarget    So  when view filesystem is used to resolve the symbolic links  the default FileSystem implementation throws UnsupportedOperationException  The proposal is to define getLinkTarget   for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links  Path thus returned is preferred to be a viewfs qualified path  so that it can be used again on the ViewFileSystem handle Implement getLinkTarget for ViewFileSystem","words":["viewfilesystem","doesn","t","override","filesystem","getlinktarget","","","","so","","when","view","filesystem","is","used","to","resolve","the","symbolic","links","","the","default","filesystem","implementation","throws","unsupportedoperationexception","","the","proposal","is","to","define","getlinktarget","","","for","viewfilesystem","and","invoke","the","target","filesystem","for","resolving","the","symbolic","links","","path","thus","returned","is","preferred","to","be","a","viewfs","qualified","path","","so","that","it","can","be","used","again","on","the","viewfilesystem","handle","implement","getlinktarget","for","viewfilesystem"],"filtered":["viewfilesystem","doesn","override","filesystem","getlinktarget","","","","","view","filesystem","used","resolve","symbolic","links","","default","filesystem","implementation","throws","unsupportedoperationexception","","proposal","define","getlinktarget","","","viewfilesystem","invoke","target","filesystem","resolving","symbolic","links","","path","thus","returned","preferred","viewfs","qualified","path","","used","viewfilesystem","handle","implement","getlinktarget","viewfilesystem"],"features":{"type":0,"size":1000,"indices":[36,56,76,80,82,91,139,166,170,174,209,280,281,293,315,333,364,368,372,381,388,445,472,487,495,500,559,605,612,627,644,656,668,684,698,710,718,760,777,827,833,931,997],"values":[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,4.0,2.0,10.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,4.0,1.0,2.0,2.0,2.0,1.0,1.0,6.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":15}
{"_c0":"Watchdog is watching for ChukwaAgent only once every   minutes  so there s no point in retrying more than once every   mins  In practice  if the watchdog is not able to automatically restart the agent  it will take more than    minutes to get Ops to restart it  Also Ops want us to limit the number of communications between Hadoop and Chukwa  that s why    minutes","_c1":"ChukwaAgent controller should retry to register for a longer period but not as frequent as now","document":"Watchdog is watching for ChukwaAgent only once every   minutes  so there s no point in retrying more than once every   mins  In practice  if the watchdog is not able to automatically restart the agent  it will take more than    minutes to get Ops to restart it  Also Ops want us to limit the number of communications between Hadoop and Chukwa  that s why    minutes ChukwaAgent controller should retry to register for a longer period but not as frequent as now","words":["watchdog","is","watching","for","chukwaagent","only","once","every","","","minutes","","so","there","s","no","point","in","retrying","more","than","once","every","","","mins","","in","practice","","if","the","watchdog","is","not","able","to","automatically","restart","the","agent","","it","will","take","more","than","","","","minutes","to","get","ops","to","restart","it","","also","ops","want","us","to","limit","the","number","of","communications","between","hadoop","and","chukwa","","that","s","why","","","","minutes","chukwaagent","controller","should","retry","to","register","for","a","longer","period","but","not","as","frequent","as","now"],"filtered":["watchdog","watching","chukwaagent","every","","","minutes","","point","retrying","every","","","mins","","practice","","watchdog","able","automatically","restart","agent","","take","","","","minutes","get","ops","restart","","also","ops","want","us","limit","number","communications","hadoop","chukwa","","","","","minutes","chukwaagent","controller","retry","register","longer","period","frequent"],"features":{"type":0,"size":1000,"indices":[18,36,73,83,98,128,147,170,181,197,208,229,236,261,275,280,281,299,333,343,346,368,372,382,388,420,445,481,495,496,505,540,564,572,583,615,629,638,665,696,710,712,717,726,760,792,831,832,840,855,868,899,936,959,972,973],"values":[2.0,2.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,16.0,1.0,5.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,3.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"We already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day","_c1":"Track current database in SQL HiveContext","document":"We already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day Track current database in SQL HiveContext","words":["we","already","have","internal","apis","for","hive","to","do","this","","we","should","do","it","for","sqlcontext","too","so","we","can","merge","these","code","paths","one","day","track","current","database","in","sql","hivecontext"],"filtered":["already","internal","apis","hive","","sqlcontext","merge","code","paths","one","day","track","current","database","sql","hivecontext"],"features":{"type":0,"size":1000,"indices":[36,44,48,57,295,299,368,372,373,388,420,445,451,453,461,493,495,534,599,605,644,665,686,710,773,833,842,858,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0]},"cluster_label":13}
{"_c0":"We can just rewrite distinct using groupby  i e  aggregate operator","_c1":"Remove physical Distinct operator in favor of Aggregate","document":"We can just rewrite distinct using groupby  i e  aggregate operator Remove physical Distinct operator in favor of Aggregate","words":["we","can","just","rewrite","distinct","using","groupby","","i","e","","aggregate","operator","remove","physical","distinct","operator","in","favor","of","aggregate"],"filtered":["rewrite","distinct","using","groupby","","e","","aggregate","operator","remove","physical","distinct","operator","favor","aggregate"],"features":{"type":0,"size":1000,"indices":[199,236,288,307,329,343,372,438,445,624,649,738,833,878,908,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We can provides the option to choose JSON parser can be enabled to accept quoting of all character or not  For example  if JSON file that includes not listed by JSON backslash quoting specification  it returns corrupt record    This issue similar to HIVE        HIVE","_c1":"Add option to accept quoting of all character backslash quoting mechanism","document":"We can provides the option to choose JSON parser can be enabled to accept quoting of all character or not  For example  if JSON file that includes not listed by JSON backslash quoting specification  it returns corrupt record    This issue similar to HIVE        HIVE Add option to accept quoting of all character backslash quoting mechanism","words":["we","can","provides","the","option","to","choose","json","parser","can","be","enabled","to","accept","quoting","of","all","character","or","not","","for","example","","if","json","file","that","includes","not","listed","by","json","backslash","quoting","specification","","it","returns","corrupt","record","","","","this","issue","similar","to","hive","","","","","","","","hive","add","option","to","accept","quoting","of","all","character","backslash","quoting","mechanism"],"filtered":["provides","option","choose","json","parser","enabled","accept","quoting","character","","example","","json","file","includes","listed","json","backslash","quoting","specification","","returns","corrupt","record","","","","issue","similar","hive","","","","","","","","hive","add","option","accept","quoting","character","backslash","quoting","mechanism"],"features":{"type":0,"size":1000,"indices":[18,36,108,170,187,203,208,222,223,239,243,244,286,343,372,373,388,432,455,495,529,599,656,662,706,710,723,748,760,824,833,846,852,910,945,954,968,993],"values":[2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,13.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,5.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"We currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark      we want to provide native implementations for DDLs for both SQLContext and HiveContext  The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step   Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API","_c1":"Create native DDL commands","document":"We currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark      we want to provide native implementations for DDLs for both SQLContext and HiveContext  The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step   Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API Create native DDL commands","words":["we","currently","delegate","most","ddls","directly","to","hive","","through","nativeplaceholder","in","hiveql","scala","","in","spark","","","","","","we","want","to","provide","native","implementations","for","ddls","for","both","sqlcontext","and","hivecontext","","the","first","step","is","to","properly","parse","these","ddls","","and","then","create","logical","commands","that","encapsulate","them","","the","actual","implementation","can","still","delegate","to","hivenativecommand","","as","an","example","","we","should","define","a","command","for","renametable","with","the","proper","fields","","and","just","delegate","the","implementation","to","hivenativecommand","","we","might","need","to","track","the","original","sql","query","in","order","to","run","hivenativecommand","","but","we","can","remove","the","sql","query","in","the","future","once","we","do","the","next","step","","","once","we","flush","out","the","internal","persistent","catalog","api","","we","can","then","switch","the","implementation","of","these","newly","added","commands","to","use","the","catalog","api","create","native","ddl","commands"],"filtered":["currently","delegate","ddls","directly","hive","","nativeplaceholder","hiveql","scala","","spark","","","","","","want","provide","native","implementations","ddls","sqlcontext","hivecontext","","first","step","properly","parse","ddls","","create","logical","commands","encapsulate","","actual","implementation","still","delegate","hivenativecommand","","example","","define","command","renametable","proper","fields","","delegate","implementation","hivenativecommand","","might","need","track","original","sql","query","order","run","hivenativecommand","","remove","sql","query","future","next","step","","","flush","internal","persistent","catalog","api","","switch","implementation","newly","added","commands","use","catalog","api","create","native","ddl","commands"],"features":{"type":0,"size":1000,"indices":[36,55,72,78,83,100,105,135,147,170,174,183,187,188,227,242,243,247,265,273,281,288,295,307,313,333,343,364,372,381,384,388,389,441,445,451,453,461,467,489,490,493,495,510,534,537,543,546,558,572,599,640,644,650,654,665,667,686,688,698,710,712,718,749,752,760,763,770,800,817,833,863,909,924,925,953,985,993],"values":[3.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,18.0,2.0,1.0,8.0,1.0,1.0,4.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,11.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,8.0]},"cluster_label":15}
{"_c0":"We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work  For example  there are various options in parquet mr that users might want to set  but the data source API does not expose a per job way to set it  This patch propagates the user specified options also into Hadoop Configuration","_c1":"Propagate data source options to Hadoop configurations","document":"We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work  For example  there are various options in parquet mr that users might want to set  but the data source API does not expose a per job way to set it  This patch propagates the user specified options also into Hadoop Configuration Propagate data source options to Hadoop configurations","words":["we","currently","have","no","way","for","users","to","propagate","options","to","the","underlying","library","that","rely","in","hadoop","configurations","to","work","","for","example","","there","are","various","options","in","parquet","mr","that","users","might","want","to","set","","but","the","data","source","api","does","not","expose","a","per","job","way","to","set","it","","this","patch","propagates","the","user","specified","options","also","into","hadoop","configuration","propagate","data","source","options","to","hadoop","configurations"],"filtered":["currently","way","users","propagate","options","underlying","library","rely","hadoop","configurations","work","","example","","various","options","parquet","mr","users","might","want","set","","data","source","api","expose","per","job","way","set","","patch","propagates","user","specified","options","also","hadoop","configuration","propagate","data","source","options","hadoop","configurations"],"features":{"type":0,"size":1000,"indices":[18,36,70,79,83,109,137,138,159,170,172,175,181,243,272,299,317,346,372,373,388,440,445,446,470,495,527,579,587,644,651,691,695,698,710,712,755,760,763,792,813,824,831,882,891,985,993,999],"values":[1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,6.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,1.0,2.0,1.0,3.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We don t sufficiently test the  path work well","_c1":"Remove the option to turn off unsafe and codegen","document":"We don t sufficiently test the  path work well Remove the option to turn off unsafe and codegen","words":["we","don","t","sufficiently","test","the","","path","work","well","remove","the","option","to","turn","off","unsafe","and","codegen"],"filtered":["sufficiently","test","","path","work","well","remove","option","turn","unsafe","codegen"],"features":{"type":0,"size":1000,"indices":[157,204,222,242,263,288,333,372,388,441,497,527,586,668,710,777,886,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We had an application sitting on top of Hadoop and got problems using jsch once we switched to java    Got this exception     Upgrading to jsch        from jsch        fixed the issue for us  but then it got in conflict with hadoop s jsch version  we fixed this for us by jarjar ing our jsch version   So i think jsch got introduce by namenode HA  HDFS        So you guys should check if the ssh part is properly working for java  or preventively upgrade the jsch lib to jsch         Some references to problems reported    http   sourceforge net p jsch mailman jsch users thread loom         T           post gmane org    https   issues apache org bugzilla show bug cgi id","_c1":"Upgrade jsch lib to jsch        to avoid problems running on java","document":"We had an application sitting on top of Hadoop and got problems using jsch once we switched to java    Got this exception     Upgrading to jsch        from jsch        fixed the issue for us  but then it got in conflict with hadoop s jsch version  we fixed this for us by jarjar ing our jsch version   So i think jsch got introduce by namenode HA  HDFS        So you guys should check if the ssh part is properly working for java  or preventively upgrade the jsch lib to jsch         Some references to problems reported    http   sourceforge net p jsch mailman jsch users thread loom         T           post gmane org    https   issues apache org bugzilla show bug cgi id Upgrade jsch lib to jsch        to avoid problems running on java","words":["we","had","an","application","sitting","on","top","of","hadoop","and","got","problems","using","jsch","once","we","switched","to","java","","","","got","this","exception","","","","","upgrading","to","jsch","","","","","","","","from","jsch","","","","","","","","fixed","the","issue","for","us","","but","then","it","got","in","conflict","with","hadoop","s","jsch","version","","we","fixed","this","for","us","by","jarjar","ing","our","jsch","version","","","so","i","think","jsch","got","introduce","by","namenode","ha","","hdfs","","","","","","","","so","you","guys","should","check","if","the","ssh","part","is","properly","working","for","java","","or","preventively","upgrade","the","jsch","lib","to","jsch","","","","","","","","","some","references","to","problems","reported","","","","http","","","sourceforge","net","p","jsch","mailman","jsch","users","thread","loom","","","","","","","","","t","","","","","","","","","","","post","gmane","org","","","","https","","","issues","apache","org","bugzilla","show","bug","cgi","id","upgrade","jsch","lib","to","jsch","","","","","","","","to","avoid","problems","running","on","java"],"filtered":["application","sitting","top","hadoop","got","problems","using","jsch","switched","java","","","","got","exception","","","","","upgrading","jsch","","","","","","","","jsch","","","","","","","","fixed","issue","us","","got","conflict","hadoop","jsch","version","","fixed","us","jarjar","ing","jsch","version","","","think","jsch","got","introduce","namenode","ha","","hdfs","","","","","","","","guys","check","ssh","part","properly","working","java","","preventively","upgrade","jsch","lib","jsch","","","","","","","","","references","problems","reported","","","","http","","","sourceforge","net","p","jsch","mailman","jsch","users","thread","loom","","","","","","","","","","","","","","","","","","","post","gmane","org","","","","https","","","issues","apache","org","bugzilla","show","bug","cgi","id","upgrade","jsch","lib","jsch","","","","","","","","avoid","problems","running","java"],"features":{"type":0,"size":1000,"indices":[7,11,19,20,36,82,83,92,109,112,118,125,147,150,160,169,170,174,181,187,197,208,216,223,242,249,281,329,333,343,368,372,373,381,385,388,400,404,425,441,445,453,475,494,495,535,564,593,624,644,650,658,665,704,710,740,748,752,755,777,781,805,825,870,882,903,915,916,921,959,963,967,968,971,973,985,991,993,994,995,998],"values":[4.0,2.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,77.0,2.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,12.0,1.0,3.0,1.0,3.0,1.0]},"cluster_label":3}
{"_c0":"We have seen many cases with customers deleting data inadvertently with  skipTrash  The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though  skipTrash is being used","_c1":"Add  safely flag to rm to prompt when deleting many files","document":"We have seen many cases with customers deleting data inadvertently with  skipTrash  The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though  skipTrash is being used Add  safely flag to rm to prompt when deleting many files","words":["we","have","seen","many","cases","with","customers","deleting","data","inadvertently","with","","skiptrash","","the","fsshell","should","prompt","user","if","the","size","of","the","data","or","the","number","of","files","being","deleted","is","bigger","than","a","threshold","even","though","","skiptrash","is","being","used","add","","safely","flag","to","rm","to","prompt","when","deleting","many","files"],"filtered":["seen","many","cases","customers","deleting","data","inadvertently","","skiptrash","","fsshell","prompt","user","size","data","number","files","deleted","bigger","threshold","even","though","","skiptrash","used","add","","safely","flag","rm","prompt","deleting","many","files"],"features":{"type":0,"size":1000,"indices":[76,85,103,170,187,188,192,258,261,280,281,299,320,343,372,374,388,390,406,432,551,576,579,583,605,609,630,650,665,695,706,710,817,871,882,928,932,993],"values":[1.0,2.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,4.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We implemented dspr with sparse vector support in  RowMatrix   This method is also used in WeightedLeastSquares and other places  It would be useful to move it to  linalg BLAS","_c1":"move RowMatrix dspr to BLAS","document":"We implemented dspr with sparse vector support in  RowMatrix   This method is also used in WeightedLeastSquares and other places  It would be useful to move it to  linalg BLAS move RowMatrix dspr to BLAS","words":["we","implemented","dspr","with","sparse","vector","support","in","","rowmatrix","","","this","method","is","also","used","in","weightedleastsquares","and","other","places","","it","would","be","useful","to","move","it","to","","linalg","blas","move","rowmatrix","dspr","to","blas"],"filtered":["implemented","dspr","sparse","vector","support","","rowmatrix","","","method","also","used","weightedleastsquares","places","","useful","move","","linalg","blas","move","rowmatrix","dspr","blas"],"features":{"type":0,"size":1000,"indices":[114,127,163,177,207,272,281,282,333,372,373,388,438,445,460,495,605,650,654,656,674,695,742,792,805,906,993],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,5.0,1.0,3.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We introduced the Netty network module for shuffle in Spark      and has turned it on by default for   releases  The old ConnectionManager is difficult to maintain  It s time to remove it","_c1":"Remove ConnectionManager","document":"We introduced the Netty network module for shuffle in Spark      and has turned it on by default for   releases  The old ConnectionManager is difficult to maintain  It s time to remove it Remove ConnectionManager","words":["we","introduced","the","netty","network","module","for","shuffle","in","spark","","","","","","and","has","turned","it","on","by","default","for","","","releases","","the","old","connectionmanager","is","difficult","to","maintain","","it","s","time","to","remove","it","remove","connectionmanager"],"filtered":["introduced","netty","network","module","shuffle","spark","","","","","","turned","default","","","releases","","old","connectionmanager","difficult","maintain","","time","remove","remove","connectionmanager"],"features":{"type":0,"size":1000,"indices":[36,40,74,78,82,96,105,157,197,223,247,281,288,299,333,372,381,388,445,453,495,568,580,672,710,713,838,993],"values":[2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,9.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We need a configurable mapping from full user names  eg  omalley APACHE ORG  to local user names  eg  omalley   For many organizations it is sufficient to just use the prefix  however  in the case of shared clusters there may be duplicated prefixes  A configurable mapping will let administrators resolve the issue","_c1":"Need mapping from long principal names to local OS user names","document":"We need a configurable mapping from full user names  eg  omalley APACHE ORG  to local user names  eg  omalley   For many organizations it is sufficient to just use the prefix  however  in the case of shared clusters there may be duplicated prefixes  A configurable mapping will let administrators resolve the issue Need mapping from long principal names to local OS user names","words":["we","need","a","configurable","mapping","from","full","user","names","","eg","","omalley","apache","org","","to","local","user","names","","eg","","omalley","","","for","many","organizations","it","is","sufficient","to","just","use","the","prefix","","however","","in","the","case","of","shared","clusters","there","may","be","duplicated","prefixes","","a","configurable","mapping","will","let","administrators","resolve","the","issue","need","mapping","from","long","principal","names","to","local","os","user","names"],"filtered":["need","configurable","mapping","full","user","names","","eg","","omalley","apache","org","","local","user","names","","eg","","omalley","","","many","organizations","sufficient","use","prefix","","however","","case","shared","clusters","may","duplicated","prefixes","","configurable","mapping","let","administrators","resolve","issue","need","mapping","long","principal","names","local","os","user","names"],"features":{"type":0,"size":1000,"indices":[9,36,164,170,188,276,281,293,307,311,328,332,333,334,342,343,372,388,420,445,489,495,535,537,608,638,656,666,673,710,748,783,822,827,831,852,859,882,897,904,917,921,964,993],"values":[2.0,1.0,1.0,2.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,10.0,3.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,2.0,2.0,2.0,1.0]},"cluster_label":17}
{"_c0":"We need hashCode and euqals in UnsafeMapData because of the      behaivour of UnsafeMapData is different from that of ArrayBasedMapData","_c1":"Remove hashCode and euqals in ArrayBasedMapData","document":"We need hashCode and euqals in UnsafeMapData because of the      behaivour of UnsafeMapData is different from that of ArrayBasedMapData Remove hashCode and euqals in ArrayBasedMapData","words":["we","need","hashcode","and","euqals","in","unsafemapdata","because","of","the","","","","","","behaivour","of","unsafemapdata","is","different","from","that","of","arraybasedmapdata","remove","hashcode","and","euqals","in","arraybasedmapdata"],"filtered":["need","hashcode","euqals","unsafemapdata","","","","","","behaivour","unsafemapdata","different","arraybasedmapdata","remove","hashcode","euqals","arraybasedmapdata"],"features":{"type":0,"size":1000,"indices":[89,281,288,333,343,372,421,432,445,537,611,655,659,710,760,921,993,997],"values":[1.0,1.0,1.0,2.0,3.0,5.0,1.0,2.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances","_c1":"we need some rpc retry framework","document":"We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances we need some rpc retry framework","words":["we","need","some","mechanism","for","rpc","calls","that","get","exceptions","to","automatically","retry","the","call","under","certain","circumstances","","in","particular","","we","often","end","up","with","calls","to","rpcs","being","wrapped","with","retry","loops","for","timeouts","","we","should","be","able","to","make","a","retrying","proxy","that","will","call","the","rpc","and","retry","in","some","circumstances","we","need","some","rpc","retry","framework"],"filtered":["need","mechanism","rpc","calls","get","exceptions","automatically","retry","call","certain","circumstances","","particular","","often","end","calls","rpcs","wrapped","retry","loops","timeouts","","able","make","retrying","proxy","call","rpc","retry","circumstances","need","rpc","retry","framework"],"features":{"type":0,"size":1000,"indices":[19,36,39,100,128,146,170,181,284,333,372,374,388,400,401,420,428,445,496,505,510,525,537,599,621,638,650,656,665,710,712,717,760,804,866,880,945,959,965,993],"values":[1.0,2.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,3.0,1.0,3.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,4.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,4.0]},"cluster_label":0}
{"_c0":"We only parse create function command  In order to support native drop function command  we need to parse it too","_c1":"Parse Drop Function DDL command","document":"We only parse create function command  In order to support native drop function command  we need to parse it too Parse Drop Function DDL command","words":["we","only","parse","create","function","command","","in","order","to","support","native","drop","function","command","","we","need","to","parse","it","too","parse","drop","function","ddl","command"],"filtered":["parse","create","function","command","","order","support","native","drop","function","command","","need","parse","parse","drop","function","ddl","command"],"features":{"type":0,"size":1000,"indices":[135,187,265,313,372,388,441,445,495,537,577,644,695,718,899,993],"values":[3.0,1.0,1.0,3.0,2.0,2.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"We re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless  Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting","_c1":"Eliminate needless uses of FileSystem  exists    isFile    isDirectory","document":"We re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless  Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting Eliminate needless uses of FileSystem  exists    isFile    isDirectory","words":["we","re","cleaning","up","hive","and","spark","s","use","of","filesystem","exists","","because","it","is","often","the","case","we","see","code","of","exists","open","","exists","delete","","when","the","exists","probe","is","needless","","against","object","stores","","expensive","needless","","hadoop","can","set","an","example","here","by","stripping","them","out","","it","will","also","show","where","there","are","opportunities","to","optimise","things","better","and","or","improve","reporting","eliminate","needless","uses","of","filesystem","","exists","","","","isfile","","","","isdirectory"],"filtered":["re","cleaning","hive","spark","use","filesystem","exists","","often","case","see","code","exists","open","","exists","delete","","exists","probe","needless","","object","stores","","expensive","needless","","hadoop","set","example","stripping","","also","show","opportunities","optimise","things","better","improve","reporting","eliminate","needless","uses","filesystem","","exists","","","","isfile","","","","isdirectory"],"features":{"type":0,"size":1000,"indices":[19,21,53,62,76,105,111,128,135,138,139,181,187,197,223,243,245,256,281,333,342,343,348,364,372,382,388,401,417,420,421,425,489,495,515,522,570,590,599,650,654,710,727,752,783,792,813,826,831,833,838,904,924,941,953,993],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,3.0,1.0,2.0,14.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":17}
{"_c0":"We re currently pulling in version       incubating   I think we should upgrade to the latest       incubating","_c1":"Upgrade HTrace version","document":"We re currently pulling in version       incubating   I think we should upgrade to the latest       incubating Upgrade HTrace version","words":["we","re","currently","pulling","in","version","","","","","","","incubating","","","i","think","we","should","upgrade","to","the","latest","","","","","","","incubating","upgrade","htrace","version"],"filtered":["re","currently","pulling","version","","","","","","","incubating","","","think","upgrade","latest","","","","","","","incubating","upgrade","htrace","version"],"features":{"type":0,"size":1000,"indices":[55,173,242,329,372,388,425,445,498,564,665,689,710,763,993,995],"values":[1.0,2.0,2.0,1.0,14.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0]},"cluster_label":17}
{"_c0":"We re using  asML  to convert the mllib vector matrix to ml vector matrix now  Using  as  is more correct given that this conversion actually shares the same underline data structure  As a result  in this PR   toBreeze  will be changed to  asBreeze   This is a private API  as a result  it will not affect any user s application","_c1":"Change  toBreeze  to  asBreeze  in Vector and Matrix","document":"We re using  asML  to convert the mllib vector matrix to ml vector matrix now  Using  as  is more correct given that this conversion actually shares the same underline data structure  As a result  in this PR   toBreeze  will be changed to  asBreeze   This is a private API  as a result  it will not affect any user s application Change  toBreeze  to  asBreeze  in Vector and Matrix","words":["we","re","using","","asml","","to","convert","the","mllib","vector","matrix","to","ml","vector","matrix","now","","using","","as","","is","more","correct","given","that","this","conversion","actually","shares","the","same","underline","data","structure","","as","a","result","","in","this","pr","","","tobreeze","","will","be","changed","to","","asbreeze","","","this","is","a","private","api","","as","a","result","","it","will","not","affect","any","user","s","application","change","","tobreeze","","to","","asbreeze","","in","vector","and","matrix"],"filtered":["re","using","","asml","","convert","mllib","vector","matrix","ml","vector","matrix","","using","","","correct","given","conversion","actually","shares","underline","data","structure","","result","","pr","","","tobreeze","","changed","","asbreeze","","","private","api","","result","","affect","user","application","change","","tobreeze","","","asbreeze","","vector","matrix"],"features":{"type":0,"size":1000,"indices":[18,91,98,116,158,169,170,173,194,197,209,252,281,324,329,333,356,372,373,388,392,420,425,445,447,495,521,572,590,607,624,627,629,644,656,695,704,710,734,742,760,865,882,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,19.0,3.0,4.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,3.0,3.0,1.0,2.0,1.0,1.0]},"cluster_label":11}
{"_c0":"We removed some classes in Spark      If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version","_c1":"Display a better message for not finding classes removed in Spark","document":"We removed some classes in Spark      If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version Display a better message for not finding classes removed in Spark","words":["we","removed","some","classes","in","spark","","","","","","if","the","user","uses","an","incompatible","library","","he","may","see","classnotfoundexception","","it","s","better","to","give","an","instruction","to","ask","people","using","a","correct","version","display","a","better","message","for","not","finding","classes","removed","in","spark"],"filtered":["removed","classes","spark","","","","","","user","uses","incompatible","library","","may","see","classnotfoundexception","","better","give","instruction","ask","people","using","correct","version","display","better","message","finding","classes","removed","spark"],"features":{"type":0,"size":1000,"indices":[9,18,36,105,111,114,116,170,197,198,372,388,400,445,446,449,486,495,512,515,624,666,710,752,783,803,809,882,941,981,993,995,997],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,7.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"We should add a config to disable the  logs endpoint in HttpServer   Listing a directory like this can be dangerous from a security perspective  We can keep it enabled by default for compatibility though","_c1":"Add a config to disable the  logs endpoints","document":"We should add a config to disable the  logs endpoint in HttpServer   Listing a directory like this can be dangerous from a security perspective  We can keep it enabled by default for compatibility though Add a config to disable the  logs endpoints","words":["we","should","add","a","config","to","disable","the","","logs","endpoint","in","httpserver","","","listing","a","directory","like","this","can","be","dangerous","from","a","security","perspective","","we","can","keep","it","enabled","by","default","for","compatibility","though","add","a","config","to","disable","the","","logs","endpoints"],"filtered":["add","config","disable","","logs","endpoint","httpserver","","","listing","directory","like","dangerous","security","perspective","","keep","enabled","default","compatibility","though","add","config","disable","","logs","endpoints"],"features":{"type":0,"size":1000,"indices":[36,170,203,223,297,330,352,372,373,381,388,432,437,445,455,495,504,579,582,594,634,656,665,688,695,710,833,834,876,921,992,993],"values":[1.0,4.0,1.0,1.0,2.0,1.0,2.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"We should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel  This will be a temp fix until we have proper evaluators defined for clustering","_c1":"Add computeCost to KMeansModel in spark ml","document":"We should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel  This will be a temp fix until we have proper evaluators defined for clustering Add computeCost to KMeansModel in spark ml","words":["we","should","add","a","method","analogous","to","spark","mllib","clustering","kmeansmodel","computecost","to","spark","ml","clustering","kmeansmodel","","this","will","be","a","temp","fix","until","we","have","proper","evaluators","defined","for","clustering","add","computecost","to","kmeansmodel","in","spark","ml"],"filtered":["add","method","analogous","spark","mllib","clustering","kmeansmodel","computecost","spark","ml","clustering","kmeansmodel","","temp","fix","proper","evaluators","defined","clustering","add","computecost","kmeansmodel","spark","ml"],"features":{"type":0,"size":1000,"indices":[36,105,126,170,299,324,345,372,373,388,420,432,445,521,628,640,654,656,665,688,745,774,796,954,993],"values":[1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,2.0]},"cluster_label":13}
{"_c0":"We should add an interface to the GLR summaries in Python for feature parity","_c1":"Python API for Generalized Linear Regression Summary","document":"We should add an interface to the GLR summaries in Python for feature parity Python API for Generalized Linear Regression Summary","words":["we","should","add","an","interface","to","the","glr","summaries","in","python","for","feature","parity","python","api","for","generalized","linear","regression","summary"],"filtered":["add","interface","glr","summaries","python","feature","parity","python","api","generalized","linear","regression","summary"],"features":{"type":0,"size":1000,"indices":[36,126,130,329,341,388,420,432,445,556,589,644,665,695,710,736,752,984,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should add text to DataFrameReader and DataFrameWriter","_c1":"Python API for text data source","document":"We should add text to DataFrameReader and DataFrameWriter Python API for text data source","words":["we","should","add","text","to","dataframereader","and","dataframewriter","python","api","for","text","data","source"],"filtered":["add","text","dataframereader","dataframewriter","python","api","text","data","source"],"features":{"type":0,"size":1000,"indices":[36,70,169,210,256,333,388,432,589,644,665,695,993],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should allow users to use the more compact form of xml elements  For example  we could allow     The old format would also be supported","_c1":"Allow compact property description in xml","document":"We should allow users to use the more compact form of xml elements  For example  we could allow     The old format would also be supported Allow compact property description in xml","words":["we","should","allow","users","to","use","the","more","compact","form","of","xml","elements","","for","example","","we","could","allow","","","","","the","old","format","would","also","be","supported","allow","compact","property","description","in","xml"],"filtered":["allow","users","use","compact","form","xml","elements","","example","","allow","","","","","old","format","also","supported","allow","compact","property","description","xml"],"features":{"type":0,"size":1000,"indices":[31,36,92,163,213,222,231,243,343,372,388,439,445,489,593,596,621,629,656,665,672,710,733,755,792,993],"values":[2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"We should be able to start a kdc server for unit tests  so that security could be turned on  This will greatly improve the coverage of unit tests","_c1":"Add capability to turn on security in unit tests","document":"We should be able to start a kdc server for unit tests  so that security could be turned on  This will greatly improve the coverage of unit tests Add capability to turn on security in unit tests","words":["we","should","be","able","to","start","a","kdc","server","for","unit","tests","","so","that","security","could","be","turned","on","","this","will","greatly","improve","the","coverage","of","unit","tests","add","capability","to","turn","on","security","in","unit","tests"],"filtered":["able","start","kdc","server","unit","tests","","security","turned","","greatly","improve","coverage","unit","tests","add","capability","turn","security","unit","tests"],"features":{"type":0,"size":1000,"indices":[36,81,82,170,200,213,335,343,368,372,373,388,411,420,432,445,496,522,619,628,634,656,665,710,760,838,886,993,996],"values":[1.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We should consolidate LocalScheduler and ClusterScheduler  given most of the functionalities are duplicated in both  This can be done by removing the LocalScheduler  and create a LocalSchedulerBackend that connects directly to an Executor","_c1":"Consolidate local scheduler and cluster scheduler","document":"We should consolidate LocalScheduler and ClusterScheduler  given most of the functionalities are duplicated in both  This can be done by removing the LocalScheduler  and create a LocalSchedulerBackend that connects directly to an Executor Consolidate local scheduler and cluster scheduler","words":["we","should","consolidate","localscheduler","and","clusterscheduler","","given","most","of","the","functionalities","are","duplicated","in","both","","this","can","be","done","by","removing","the","localscheduler","","and","create","a","localschedulerbackend","that","connects","directly","to","an","executor","consolidate","local","scheduler","and","cluster","scheduler"],"filtered":["consolidate","localscheduler","clusterscheduler","","given","functionalities","duplicated","","done","removing","localscheduler","","create","localschedulerbackend","connects","directly","executor","consolidate","local","scheduler","cluster","scheduler"],"features":{"type":0,"size":1000,"indices":[63,138,141,170,188,223,252,265,333,343,366,372,373,388,424,445,482,522,608,656,665,707,710,752,760,770,821,833,863,961,968,978,993],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":0}
{"_c0":"We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson  The script would execute the following  and take just the password as an argument","_c1":"Create a test patch script for Hudson","document":"We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson  The script would execute the following  and take just the password as an argument Create a test patch script for Hudson","words":["we","should","create","a","script","that","hudson","uses","to","execute","test","patch","that","is","in","source","control","so","modifications","to","test","patch","sh","arguments","can","be","done","w","o","updating","hudson","","the","script","would","execute","the","following","","and","take","just","the","password","as","an","argument","create","a","test","patch","script","for","hudson"],"filtered":["create","script","hudson","uses","execute","test","patch","source","control","modifications","test","patch","sh","arguments","done","w","o","updating","hudson","","script","execute","following","","take","password","argument","create","test","patch","script","hudson"],"features":{"type":0,"size":1000,"indices":[36,70,91,111,116,137,163,170,209,265,281,307,333,350,368,372,388,445,481,536,572,586,587,656,665,707,710,752,760,800,811,833,846,855,874,880,915,993],"values":[1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We should deprecate ConnectionManager in     before removing it in","_c1":"Deprecate NIO ConnectionManager","document":"We should deprecate ConnectionManager in     before removing it in Deprecate NIO ConnectionManager","words":["we","should","deprecate","connectionmanager","in","","","","","before","removing","it","in","deprecate","nio","connectionmanager"],"filtered":["deprecate","connectionmanager","","","","","removing","deprecate","nio","connectionmanager"],"features":{"type":0,"size":1000,"indices":[96,159,275,372,445,495,665,951,968,993],"values":[2.0,1.0,2.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"We should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get","_c1":"Metrics for codegen size and perf","document":"We should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get Metrics for codegen size and perf","words":["we","should","expose","codahale","metrics","for","the","codegen","source","text","size","and","how","long","it","takes","to","compile","","the","size","is","particularly","interesting","","since","the","jvm","does","have","hard","limits","on","how","large","methods","can","get","metrics","for","codegen","size","and","perf"],"filtered":["expose","codahale","metrics","codegen","source","text","size","long","takes","compile","","size","particularly","interesting","","since","jvm","hard","limits","large","methods","get","metrics","codegen","size","perf"],"features":{"type":0,"size":1000,"indices":[36,70,82,106,109,125,129,169,192,237,254,263,281,299,300,333,372,388,443,463,495,512,585,665,698,710,753,765,782,833,836,904,959,993],"values":[2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We should make an effort to clean up the shell env var name space by removing unsafe variables  See comments for list","_c1":"Rename remove non HADOOP    etc from the shell scripts","document":"We should make an effort to clean up the shell env var name space by removing unsafe variables  See comments for list Rename remove non HADOOP    etc from the shell scripts","words":["we","should","make","an","effort","to","clean","up","the","shell","env","var","name","space","by","removing","unsafe","variables","","see","comments","for","list","rename","remove","non","hadoop","","","","etc","from","the","shell","scripts"],"filtered":["make","effort","clean","shell","env","var","name","space","removing","unsafe","variables","","see","comments","list","rename","remove","non","hadoop","","","","etc","shell","scripts"],"features":{"type":0,"size":1000,"indices":[15,36,101,123,126,128,136,181,207,223,224,242,272,275,288,340,372,388,515,525,573,577,665,710,728,752,867,921,968,993],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"We should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices I made these private originally  but they will be useful for users transitioning workloads","_c1":"Make the mllib ml linalg type conversion APIs public","document":"We should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices I made these private originally  but they will be useful for users transitioning workloads Make the mllib ml linalg type conversion APIs public","words":["we","should","open","up","the","apis","for","converting","between","new","","old","linear","algebra","types","","in","spark","mllib","linalg","","","","","vector","asml","","","vectors","fromml","","","same","for","sparse","dense","and","for","matrices","i","made","these","private","originally","","but","they","will","be","useful","for","users","transitioning","workloads","make","the","mllib","ml","linalg","type","conversion","apis","public"],"filtered":["open","apis","converting","new","","old","linear","algebra","types","","spark","mllib","linalg","","","","","vector","asml","","","vectors","fromml","","","sparse","dense","matrices","made","private","originally","","useful","users","transitioning","workloads","make","mllib","ml","linalg","type","conversion","apis","public"],"features":{"type":0,"size":1000,"indices":[18,25,36,48,83,105,114,128,149,167,217,237,272,324,329,333,344,372,420,445,460,461,465,481,498,521,525,526,528,635,656,665,672,704,710,742,755,782,783,842,993,994],"values":[1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,11.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":17}
{"_c0":"We should share the SQLTab across sessions","_c1":"SQLTab should be shared by across sessions","document":"We should share the SQLTab across sessions SQLTab should be shared by across sessions","words":["we","should","share","the","sqltab","across","sessions","sqltab","should","be","shared","by","across","sessions"],"filtered":["share","sqltab","across","sessions","sqltab","shared","across","sessions"],"features":{"type":0,"size":1000,"indices":[223,274,293,310,593,656,665,710,942,993],"values":[1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"We should upgrade snappy java to         across all of our maintenance branches  This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues    for more context  This should be a major help in the Snappy debugging work that I ve been doing","_c1":"Upgrade snappy java to","document":"We should upgrade snappy java to         across all of our maintenance branches  This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues    for more context  This should be a major help in the Snappy debugging work that I ve been doing Upgrade snappy java to","words":["we","should","upgrade","snappy","java","to","","","","","","","","","across","all","of","our","maintenance","branches","","this","release","improves","error","messages","when","attempting","to","deserialize","empty","inputs","using","snappyinputstream","","this","operation","is","always","an","error","","but","the","old","error","messages","made","it","hard","to","distinguish","failures","due","to","empty","streams","from","ones","due","to","reading","invalid","","","corrupted","streams","","","see","https","","","github","com","xerial","snappy","java","issues","","","","for","more","context","","this","should","be","a","major","help","in","the","snappy","debugging","work","that","i","ve","been","doing","upgrade","snappy","java","to"],"filtered":["upgrade","snappy","java","","","","","","","","","across","maintenance","branches","","release","improves","error","messages","attempting","deserialize","empty","inputs","using","snappyinputstream","","operation","always","error","","old","error","messages","made","hard","distinguish","failures","due","empty","streams","ones","due","reading","invalid","","","corrupted","streams","","","see","https","","","github","com","xerial","snappy","java","issues","","","","context","","major","help","snappy","debugging","work","ve","upgrade","snappy","java"],"features":{"type":0,"size":1000,"indices":[13,36,76,83,100,115,125,170,190,199,221,242,257,281,310,320,329,330,333,343,344,345,347,371,372,373,385,388,415,445,449,469,475,488,495,510,514,515,517,527,535,567,624,629,656,665,672,704,710,738,752,760,801,826,833,890,893,911,921,935,967,968,983,993,998],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,21.0,3.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r","_c1":"Refactor GLMs code in SparkRWrappers","document":"We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r Refactor GLMs code in SparkRWrappers","words":["we","use","a","single","object","","sparkrwrappers","","","https","","","github","com","apache","spark","blob","master","mllib","src","main","scala","org","apache","spark","ml","r","sparkrwrappers","scala","","to","wrap","method","calls","to","glm","and","kmeans","in","sparkr","","this","is","quite","hard","to","maintain","","we","should","refactor","them","into","separate","wrappers","","like","","aftsurvivalregressionwrapper","","and","","naivebayeswrapper","","","the","package","name","should","be","","spakr","ml","r","","instead","of","","spark","ml","api","r","refactor","glms","code","in","sparkrwrappers"],"filtered":["use","single","object","","sparkrwrappers","","","https","","","github","com","apache","spark","blob","master","mllib","src","main","scala","org","apache","spark","ml","r","sparkrwrappers","scala","","wrap","method","calls","glm","kmeans","sparkr","","quite","hard","maintain","","refactor","separate","wrappers","","like","","aftsurvivalregressionwrapper","","","naivebayeswrapper","","","package","name","","spakr","ml","r","","instead","","spark","ml","api","r","refactor","glms","code","sparkrwrappers"],"features":{"type":0,"size":1000,"indices":[5,15,105,125,170,208,221,247,255,266,270,281,305,324,330,333,343,372,373,388,420,445,489,490,495,510,521,524,531,535,555,570,578,622,623,637,644,650,654,656,665,710,718,726,749,767,863,866,891,924,988,993,998,999],"values":[1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,17.0,1.0,3.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":11}
{"_c0":"We use scala collection mutable BitSet in BroadcastNestedLoopJoin now  We should use Spark s BitSet","_c1":"Use Spark BitSet in BroadcastNestedLoopJoin","document":"We use scala collection mutable BitSet in BroadcastNestedLoopJoin now  We should use Spark s BitSet Use Spark BitSet in BroadcastNestedLoopJoin","words":["we","use","scala","collection","mutable","bitset","in","broadcastnestedloopjoin","now","","we","should","use","spark","s","bitset","use","spark","bitset","in","broadcastnestedloopjoin"],"filtered":["use","scala","collection","mutable","bitset","broadcastnestedloopjoin","","use","spark","bitset","use","spark","bitset","broadcastnestedloopjoin"],"features":{"type":0,"size":1000,"indices":[98,105,171,197,230,372,445,489,490,638,665,921,993],"values":[1.0,2.0,2.0,1.0,1.0,1.0,2.0,3.0,1.0,3.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"We want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib  To support the changes we want to make  we should move the implementation from spark mllib to spark ml  We will generalize and modify it  but will also ensure that we do not change the behavior of the old API  There are several steps to this     Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation  The current spark ml tests will ensure that the   implementations learn exactly the same models  Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions     Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation  The spark ml tests will again ensure that we do not change any behavior     Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence  This JIRA is now for step   only  Steps   and   will be in separate JIRAs  After these updates  we can more safely generalize and improve the spark ml implementation","_c1":"Move tree forest implementation from spark mllib to spark ml","document":"We want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib  To support the changes we want to make  we should move the implementation from spark mllib to spark ml  We will generalize and modify it  but will also ensure that we do not change the behavior of the old API  There are several steps to this     Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation  The current spark ml tests will ensure that the   implementations learn exactly the same models  Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions     Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation  The spark ml tests will again ensure that we do not change any behavior     Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence  This JIRA is now for step   only  Steps   and   will be in separate JIRAs  After these updates  we can more safely generalize and improve the spark ml implementation Move tree forest implementation from spark mllib to spark ml","words":["we","want","to","change","and","improve","the","spark","ml","api","for","trees","and","ensembles","","but","we","cannot","change","the","old","api","in","spark","mllib","","to","support","the","changes","we","want","to","make","","we","should","move","the","implementation","from","spark","mllib","to","spark","ml","","we","will","generalize","and","modify","it","","but","will","also","ensure","that","we","do","not","change","the","behavior","of","the","old","api","","there","are","several","steps","to","this","","","","","copy","the","implementation","over","to","spark","ml","and","change","the","spark","ml","classes","to","use","that","implementation","","rather","than","calling","the","spark","mllib","implementation","","the","current","spark","ml","tests","will","ensure","that","the","","","implementations","learn","exactly","the","same","models","","note","","this","should","include","performance","testing","to","make","sure","the","updated","code","does","not","have","any","regressions","","","","","","","update","","","i","have","run","tests","using","spark","perf","","and","there","were","no","regressions","","","","","remove","the","spark","mllib","implementation","","and","make","the","spark","mllib","apis","wrappers","around","the","spark","ml","implementation","","the","spark","ml","tests","will","again","ensure","that","we","do","not","change","any","behavior","","","","","move","the","unit","tests","to","spark","ml","","and","change","the","spark","mllib","unit","tests","to","verify","model","equivalence","","this","jira","is","now","for","step","","","only","","steps","","","and","","","will","be","in","separate","jiras","","after","these","updates","","we","can","more","safely","generalize","and","improve","the","spark","ml","implementation","move","tree","forest","implementation","from","spark","mllib","to","spark","ml"],"filtered":["want","change","improve","spark","ml","api","trees","ensembles","","change","old","api","spark","mllib","","support","changes","want","make","","move","implementation","spark","mllib","spark","ml","","generalize","modify","","also","ensure","change","behavior","old","api","","several","steps","","","","","copy","implementation","spark","ml","change","spark","ml","classes","use","implementation","","rather","calling","spark","mllib","implementation","","current","spark","ml","tests","ensure","","","implementations","learn","exactly","models","","note","","include","performance","testing","make","sure","updated","code","regressions","","","","","","","update","","","run","tests","using","spark","perf","","regressions","","","","","remove","spark","mllib","implementation","","make","spark","mllib","apis","wrappers","around","spark","ml","implementation","","spark","ml","tests","ensure","change","behavior","","","","","move","unit","tests","spark","ml","","change","spark","mllib","unit","tests","verify","model","equivalence","","jira","step","","","","steps","","","","","separate","jiras","","updates","","safely","generalize","improve","spark","ml","implementation","move","tree","forest","implementation","spark","mllib","spark","ml"],"features":{"type":0,"size":1000,"indices":[5,18,36,55,77,83,91,98,105,138,158,167,193,216,254,261,281,282,288,291,299,317,324,326,329,333,335,343,346,352,363,364,372,373,388,401,420,427,437,443,445,461,487,489,490,495,521,522,525,534,547,558,583,619,624,629,640,644,646,656,665,672,674,695,698,710,712,718,735,737,753,759,760,775,792,809,821,831,833,842,857,866,871,899,909,921,925,928,931,952,962,993],"values":[1.0,3.0,2.0,1.0,1.0,2.0,2.0,1.0,18.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,2.0,1.0,10.0,1.0,1.0,9.0,2.0,2.0,1.0,1.0,1.0,1.0,46.0,3.0,11.0,1.0,6.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,7.0,2.0,3.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,2.0,3.0,2.0,2.0,2.0,2.0,1.0,1.0,9.0,21.0,2.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,8.0]},"cluster_label":5}
{"_c0":"We want to support JSON serialization of vectors in order to support SPARK","_c1":"JSON serialization of Vectors","document":"We want to support JSON serialization of vectors in order to support SPARK JSON serialization of Vectors","words":["we","want","to","support","json","serialization","of","vectors","in","order","to","support","spark","json","serialization","of","vectors"],"filtered":["want","support","json","serialization","vectors","order","support","spark","json","serialization","vectors"],"features":{"type":0,"size":1000,"indices":[105,343,388,445,662,695,712,718,843,993,994],"values":[1.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"When SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer","_c1":"Remove noop SortOrder in Sort","document":"When SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer Remove noop SortOrder in Sort","words":["when","sortorder","does","not","contain","any","reference","","it","has","no","effect","on","the","sorting","","remove","the","noop","sortorder","in","optimizer","remove","noop","sortorder","in","sort"],"filtered":["sortorder","contain","reference","","effect","sorting","","remove","noop","sortorder","optimizer","remove","noop","sortorder","sort"],"features":{"type":0,"size":1000,"indices":[18,76,82,91,152,288,295,301,346,372,445,495,580,698,706,709,710,720,792,793],"values":[1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0]},"cluster_label":13}
{"_c0":"When a block is persisted in the MemoryStore at a serialized storage level  the current MemoryStore putIterator   code will unroll the entire iterator as Java objects in memory  then will turn around and serialize an iterator obtained from the unrolled array  This is inefficient and doubles our peak memory requirements  Instead  I think that we should incrementally serialize blocks while unrolling them  A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk  However  I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case","_c1":"Incrementally serialize blocks while unrolling them in MemoryStore","document":"When a block is persisted in the MemoryStore at a serialized storage level  the current MemoryStore putIterator   code will unroll the entire iterator as Java objects in memory  then will turn around and serialize an iterator obtained from the unrolled array  This is inefficient and doubles our peak memory requirements  Instead  I think that we should incrementally serialize blocks while unrolling them  A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk  However  I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case Incrementally serialize blocks while unrolling them in MemoryStore","words":["when","a","block","is","persisted","in","the","memorystore","at","a","serialized","storage","level","","the","current","memorystore","putiterator","","","code","will","unroll","the","entire","iterator","as","java","objects","in","memory","","then","will","turn","around","and","serialize","an","iterator","obtained","from","the","unrolled","array","","this","is","inefficient","and","doubles","our","peak","memory","requirements","","instead","","i","think","that","we","should","incrementally","serialize","blocks","while","unrolling","them","","a","downside","to","incremental","serialization","is","the","fact","that","we","will","need","to","deserialize","the","partially","unrolled","data","in","case","there","is","not","enough","space","to","unroll","the","block","and","the","block","cannot","be","dropped","to","disk","","however","","i","m","hoping","that","the","memory","efficiency","improvements","will","outweigh","any","performance","losses","as","a","result","of","extra","serialization","in","that","hopefully","rare","case","incrementally","serialize","blocks","while","unrolling","them","in","memorystore"],"filtered":["block","persisted","memorystore","serialized","storage","level","","current","memorystore","putiterator","","","code","unroll","entire","iterator","java","objects","memory","","turn","around","serialize","iterator","obtained","unrolled","array","","inefficient","doubles","peak","memory","requirements","","instead","","think","incrementally","serialize","blocks","unrolling","","downside","incremental","serialization","fact","need","deserialize","partially","unrolled","data","case","enough","space","unroll","block","block","dropped","disk","","however","","m","hoping","memory","efficiency","improvements","outweigh","performance","losses","result","extra","serialization","hopefully","rare","case","incrementally","serialize","blocks","unrolling","memorystore"],"features":{"type":0,"size":1000,"indices":[8,18,20,76,91,96,139,147,165,170,207,215,220,222,235,236,241,281,329,333,336,342,343,372,373,379,381,388,394,397,401,420,445,491,511,537,564,572,587,612,625,638,655,656,663,665,673,695,704,706,707,710,737,752,756,759,760,780,786,788,806,816,831,843,863,865,880,886,914,921,924,931,948,963,967,983,993,994],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,3.0,4.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,4.0,2.0,3.0,1.0,2.0,1.0,10.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,5.0,5.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,10.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0]},"cluster_label":15}
{"_c0":"When a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching  This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels  There are two places where we request serialized bytes from the BlockStore     getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store     the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation  Therefore  I think this is a safe change","_c1":"Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills","document":"When a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching  This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels  There are two places where we request serialized bytes from the BlockStore     getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store     the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation  Therefore  I think this is a safe change Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills","words":["when","a","cached","block","is","spilled","to","disk","and","read","back","in","serialized","form","","i","e","","as","bytes","","","the","current","blockmanager","implementation","will","attempt","to","re","insert","the","serialized","block","into","the","memorystore","even","if","the","block","s","storage","level","requests","deserialized","caching","","this","behavior","adds","some","complexity","to","the","memorystore","but","i","don","t","think","it","offers","many","performance","benefits","and","i","d","like","to","remove","it","in","order","to","simplify","a","larger","refactoring","patch","","therefore","","i","propose","to","change","the","behavior","such","that","disk","store","reads","will","only","cache","bytes","in","the","memory","store","for","blocks","with","serialized","storage","levels","","there","are","two","places","where","we","request","serialized","bytes","from","the","blockstore","","","","","getlocalbytes","","","","which","is","only","called","when","reading","local","copies","of","torrentbroadcast","pieces","","broadcast","pieces","are","always","cached","using","a","serialized","storage","level","","so","this","won","t","lead","to","a","mismatch","in","serialization","forms","if","spilled","bytes","read","from","disk","are","cached","as","bytes","in","the","memory","store","","","","","the","non","shuffle","block","branch","in","getblockdata","","","","which","is","only","called","by","the","nettyblockrpcserver","when","responding","to","requests","to","read","remote","blocks","","caching","the","serialized","bytes","in","memory","will","only","benefit","us","if","those","cached","bytes","are","read","before","they","re","evicted","and","the","likelihood","of","that","happening","seems","low","since","the","frequency","of","remote","reads","of","non","broadcast","cached","blocks","seems","very","low","","caching","these","bytes","when","they","have","a","low","probability","of","being","read","is","bad","if","it","risks","the","eviction","of","blocks","which","are","cached","in","their","expected","serialized","deserialized","forms","","since","those","blocks","seem","more","likely","to","be","read","in","local","computation","","therefore","","i","think","this","is","a","safe","change","don","t","cache","memory","and","disk","blocks","as","bytes","in","memory","store","when","reading","spills"],"filtered":["cached","block","spilled","disk","read","back","serialized","form","","e","","bytes","","","current","blockmanager","implementation","attempt","re","insert","serialized","block","memorystore","even","block","storage","level","requests","deserialized","caching","","behavior","adds","complexity","memorystore","think","offers","many","performance","benefits","d","like","remove","order","simplify","larger","refactoring","patch","","therefore","","propose","change","behavior","disk","store","reads","cache","bytes","memory","store","blocks","serialized","storage","levels","","two","places","request","serialized","bytes","blockstore","","","","","getlocalbytes","","","","called","reading","local","copies","torrentbroadcast","pieces","","broadcast","pieces","always","cached","using","serialized","storage","level","","won","lead","mismatch","serialization","forms","spilled","bytes","read","disk","cached","bytes","memory","store","","","","","non","shuffle","block","branch","getblockdata","","","","called","nettyblockrpcserver","responding","requests","read","remote","blocks","","caching","serialized","bytes","memory","benefit","us","cached","bytes","read","re","evicted","likelihood","happening","seems","low","since","frequency","remote","reads","non","broadcast","cached","blocks","seems","low","","caching","bytes","low","probability","read","bad","risks","eviction","blocks","cached","expected","serialized","deserialized","forms","","since","blocks","seem","likely","read","local","computation","","therefore","","think","safe","change","cache","memory","disk","blocks","bytes","memory","store","reading","spills"],"features":{"type":0,"size":1000,"indices":[8,13,16,19,20,36,48,64,71,76,83,86,92,94,109,127,137,138,139,140,150,158,159,162,163,164,165,170,179,188,197,204,208,223,224,232,235,236,272,274,281,288,299,329,330,333,343,356,368,372,373,374,381,388,394,398,400,406,408,415,420,425,430,445,450,461,495,497,511,520,530,533,541,564,568,572,575,585,588,591,597,600,608,617,618,621,624,629,642,650,656,689,693,698,710,718,735,736,739,740,743,759,760,777,783,788,792,813,821,831,843,878,891,899,910,921,936,939,944,945,960,963,975,978,985,990,993,994,997],"values":[1.0,1.0,1.0,1.0,6.0,1.0,3.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,2.0,10.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,9.0,5.0,1.0,1.0,5.0,1.0,4.0,6.0,1.0,1.0,29.0,3.0,1.0,2.0,10.0,4.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,1.0,10.0,6.0,3.0,3.0,2.0,4.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,2.0,1.0,1.0,3.0,3.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,7.0,1.0,3.0,1.0,1.0,16.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,2.0,3.0,2.0,5.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,7.0,2.0,1.0,1.0,1.0,1.0,4.0,1.0]},"cluster_label":1}
{"_c0":"When a new file is added  the source is  dev null  rather than the root of the tree  which would mean a a b prefix   Allow for this","_c1":"Allow smart apply patch sh to add new files in binary git patches","document":"When a new file is added  the source is  dev null  rather than the root of the tree  which would mean a a b prefix   Allow for this Allow smart apply patch sh to add new files in binary git patches","words":["when","a","new","file","is","added","","the","source","is","","dev","null","","rather","than","the","root","of","the","tree","","which","would","mean","a","a","b","prefix","","","allow","for","this","allow","smart","apply","patch","sh","to","add","new","files","in","binary","git","patches"],"filtered":["new","file","added","","source","","dev","null","","rather","root","tree","","mean","b","prefix","","","allow","allow","smart","apply","patch","sh","add","new","files","binary","git","patches"],"features":{"type":0,"size":1000,"indices":[25,35,36,70,76,108,137,163,170,193,231,261,281,343,361,369,372,373,384,388,432,437,445,464,551,567,597,647,710,783,820,846,871,909,968],"values":[2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump  the client currently just gets a non useful error message like  EOFException   Instead  the IPC server code can speak just enough of prior IPC protocols to send back a  fatal  message indicating the version mismatch","_c1":"Send back nicer error to clients using outdated IPC version","document":"When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump  the client currently just gets a non useful error message like  EOFException   Instead  the IPC server code can speak just enough of prior IPC protocols to send back a  fatal  message indicating the version mismatch Send back nicer error to clients using outdated IPC version","words":["when","an","older","hadoop","version","tries","to","contact","a","newer","hadoop","version","across","an","ipc","protocol","version","bump","","the","client","currently","just","gets","a","non","useful","error","message","like","","eofexception","","","instead","","the","ipc","server","code","can","speak","just","enough","of","prior","ipc","protocols","to","send","back","a","","fatal","","message","indicating","the","version","mismatch","send","back","nicer","error","to","clients","using","outdated","ipc","version"],"filtered":["older","hadoop","version","tries","contact","newer","hadoop","version","across","ipc","protocol","version","bump","","client","currently","gets","non","useful","error","message","like","","eofexception","","","instead","","ipc","server","code","speak","enough","prior","ipc","protocols","send","back","","fatal","","message","indicating","version","mismatch","send","back","nicer","error","clients","using","outdated","ipc","version"],"features":{"type":0,"size":1000,"indices":[16,47,57,67,69,76,135,146,156,166,168,170,181,224,245,272,307,310,330,333,343,372,388,411,420,430,441,483,523,624,688,710,722,724,752,763,768,833,863,880,981,990,995],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,7.0,3.0,1.0,1.0,3.0,1.0,4.0,2.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,5.0]},"cluster_label":2}
{"_c0":"When o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar","_c1":"Remove bin rcc script","document":"When o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar Remove bin rcc script","words":["when","o","a","h","record","was","moved","","bin","rcc","was","never","updated","to","pull","those","classes","from","the","streaming","jar","remove","bin","rcc","script"],"filtered":["o","h","record","moved","","bin","rcc","never","updated","pull","classes","streaming","jar","remove","bin","rcc","script"],"features":{"type":0,"size":1000,"indices":[76,126,138,170,234,237,244,263,282,286,288,350,372,388,490,597,600,603,710,809,880,921],"values":[1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"When reading data from the DiskStore and attempting to cache it back into the memory store  we should guard against race conditions where multiple readers are attempting to re cache the same block in memory","_c1":"Guard against race condition when re caching spilled bytes in memory","document":"When reading data from the DiskStore and attempting to cache it back into the memory store  we should guard against race conditions where multiple readers are attempting to re cache the same block in memory Guard against race condition when re caching spilled bytes in memory","words":["when","reading","data","from","the","diskstore","and","attempting","to","cache","it","back","into","the","memory","store","","we","should","guard","against","race","conditions","where","multiple","readers","are","attempting","to","re","cache","the","same","block","in","memory","guard","against","race","condition","when","re","caching","spilled","bytes","in","memory"],"filtered":["reading","data","diskstore","attempting","cache","back","memory","store","","guard","race","conditions","multiple","readers","attempting","re","cache","block","memory","guard","race","condition","re","caching","spilled","bytes","memory"],"features":{"type":0,"size":1000,"indices":[76,138,139,163,274,310,333,342,372,388,389,415,425,430,445,454,495,497,511,579,592,656,665,689,695,710,743,788,821,891,921,935,949,993],"values":[2.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":0}
{"_c0":"When setting up a compression codec in an MR job the full class name of the codec must be used  To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip    instead their full codec class name  Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs","_c1":"Add capability to resolve compression codec based on codec name","document":"When setting up a compression codec in an MR job the full class name of the codec must be used  To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip    instead their full codec class name  Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs Add capability to resolve compression codec based on codec name","words":["when","setting","up","a","compression","codec","in","an","mr","job","the","full","class","name","of","the","codec","must","be","used","","to","ease","usability","","compression","codecs","should","be","resolved","by","their","codec","name","","ie","","gzip","","","","deflate","","","","zlib","","","","bzip","","","","instead","their","full","codec","class","name","","besides","easy","of","use","for","hadoop","users","who","would","use","the","codec","alias","instead","the","full","codec","class","name","","it","could","simplify","how","hbase","resolves","loads","the","codecs","add","capability","to","resolve","compression","codec","based","on","codec","name"],"filtered":["setting","compression","codec","mr","job","full","class","name","codec","must","used","","ease","usability","","compression","codecs","resolved","codec","name","","ie","","gzip","","","","deflate","","","","zlib","","","","bzip","","","","instead","full","codec","class","name","","besides","easy","use","hadoop","users","use","codec","alias","instead","full","codec","class","name","","simplify","hbase","resolves","loads","codecs","add","capability","resolve","compression","codec","based","codec","name"],"features":{"type":0,"size":1000,"indices":[15,19,23,29,36,62,76,81,82,128,163,170,176,181,213,217,223,235,237,272,287,323,336,343,372,388,402,432,441,445,470,489,495,534,578,591,605,625,656,659,662,665,710,731,752,755,760,787,812,827,863,897,956],"values":[5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,8.0,2.0,18.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,3.0,3.0,1.0]},"cluster_label":11}
{"_c0":"When starting a stream with a lot of backfill and maxFilesPerTrigger  the user could often want to start with most recent files first  This would let you keep low latency for recent data and slowly backfill historical data  It s better to add an option to control this behavior","_c1":"Make FileStream be able to start with most recent files","document":"When starting a stream with a lot of backfill and maxFilesPerTrigger  the user could often want to start with most recent files first  This would let you keep low latency for recent data and slowly backfill historical data  It s better to add an option to control this behavior Make FileStream be able to start with most recent files","words":["when","starting","a","stream","with","a","lot","of","backfill","and","maxfilespertrigger","","the","user","could","often","want","to","start","with","most","recent","files","first","","this","would","let","you","keep","low","latency","for","recent","data","and","slowly","backfill","historical","data","","it","s","better","to","add","an","option","to","control","this","behavior","make","filestream","be","able","to","start","with","most","recent","files"],"filtered":["starting","stream","lot","backfill","maxfilespertrigger","","user","often","want","start","recent","files","first","","let","keep","low","latency","recent","data","slowly","backfill","historical","data","","better","add","option","control","behavior","make","filestream","able","start","recent","files"],"features":{"type":0,"size":1000,"indices":[36,76,163,164,170,183,197,213,218,222,309,333,343,372,373,388,401,425,432,487,495,496,525,551,594,604,616,618,650,656,695,703,710,712,735,752,765,770,842,874,882,941,996],"values":[1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,4.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,1.0,2.0,2.0]},"cluster_label":0}
{"_c0":"When token is used for authentication over RPC  information other than username may be needed for access authorization  This information is typically specified in TokenIdentifier  This is especially true for block tokens used for client to datanode accesses  where authorization is based on access permissions specified in TokenIdentifier  and not on username  Block tokens used to be called access tokens and one can think of them as capability tokens  See HADOOP      for more info","_c1":"Add authenticated TokenIdentifiers to UGI so that they can be used for authorization","document":"When token is used for authentication over RPC  information other than username may be needed for access authorization  This information is typically specified in TokenIdentifier  This is especially true for block tokens used for client to datanode accesses  where authorization is based on access permissions specified in TokenIdentifier  and not on username  Block tokens used to be called access tokens and one can think of them as capability tokens  See HADOOP      for more info Add authenticated TokenIdentifiers to UGI so that they can be used for authorization","words":["when","token","is","used","for","authentication","over","rpc","","information","other","than","username","may","be","needed","for","access","authorization","","this","information","is","typically","specified","in","tokenidentifier","","this","is","especially","true","for","block","tokens","used","for","client","to","datanode","accesses","","where","authorization","is","based","on","access","permissions","specified","in","tokenidentifier","","and","not","on","username","","block","tokens","used","to","be","called","access","tokens","and","one","can","think","of","them","as","capability","tokens","","see","hadoop","","","","","","for","more","info","add","authenticated","tokenidentifiers","to","ugi","so","that","they","can","be","used","for","authorization"],"filtered":["token","used","authentication","rpc","","information","username","may","needed","access","authorization","","information","typically","specified","tokenidentifier","","especially","true","block","tokens","used","client","datanode","accesses","","authorization","based","access","permissions","specified","tokenidentifier","","username","","block","tokens","used","called","access","tokens","one","think","capability","tokens","","see","hadoop","","","","","","info","add","authenticated","tokenidentifiers","ugi","used","authorization"],"features":{"type":0,"size":1000,"indices":[3,18,36,43,44,48,76,81,82,135,139,146,175,181,188,224,244,261,281,333,343,352,368,372,373,384,388,398,432,445,453,511,514,515,528,564,572,605,615,625,629,652,656,666,674,680,732,760,826,833,924,978,980],"values":[1.0,1.0,6.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,12.0,2.0,2.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,8.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":17}
{"_c0":"When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation","_c1":"Data Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider","document":"When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation Data Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider","words":["when","users","try","to","implement","a","data","source","api","with","extending","only","relationprovider","and","creatablerelationprovider","","they","will","hit","an","error","when","resolving","the","relation","data","source","apis","","extending","relationprovider","and","creatablerelationprovider","without","schemarelationprovider"],"filtered":["users","try","implement","data","source","api","extending","relationprovider","creatablerelationprovider","","hit","error","resolving","relation","data","source","apis","","extending","relationprovider","creatablerelationprovider","without","schemarelationprovider"],"features":{"type":0,"size":1000,"indices":[48,70,76,159,170,280,333,372,379,388,420,472,609,644,650,695,708,710,752,755,842,882,884,899,946],"values":[1.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"When we generate code for join  we copy the output row  because there could be multiple output row from single input row  We could avoid this copy when there is no join  or the join will not generate multiple output rows from single input row","_c1":"Avoid the copy in whole stage codegen when there is no joins","document":"When we generate code for join  we copy the output row  because there could be multiple output row from single input row  We could avoid this copy when there is no join  or the join will not generate multiple output rows from single input row Avoid the copy in whole stage codegen when there is no joins","words":["when","we","generate","code","for","join","","we","copy","the","output","row","","because","there","could","be","multiple","output","row","from","single","input","row","","we","could","avoid","this","copy","when","there","is","no","join","","or","the","join","will","not","generate","multiple","output","rows","from","single","input","row","avoid","the","copy","in","whole","stage","codegen","when","there","is","no","joins"],"filtered":["generate","code","join","","copy","output","row","","multiple","output","row","single","input","row","","avoid","copy","join","","join","generate","multiple","output","rows","single","input","row","avoid","copy","whole","stage","codegen","joins"],"features":{"type":0,"size":1000,"indices":[0,18,36,76,109,122,187,205,213,216,263,281,346,372,373,420,421,445,531,592,656,670,710,780,788,830,831,903,921,952,993],"values":[2.0,1.0,1.0,3.0,2.0,3.0,1.0,1.0,2.0,3.0,1.0,2.0,2.0,4.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,3.0,1.0,4.0,2.0,3.0,1.0,2.0,3.0,3.0]},"cluster_label":0}
{"_c0":"When you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass    However  when reflect on this we try and load   org mycompany project MyClass","_c1":"Support for classes defined in package objects","document":"When you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass    However  when reflect on this we try and load   org mycompany project MyClass Support for classes defined in package objects","words":["when","you","define","a","class","inside","of","a","package","object","","the","name","ends","up","being","something","like","","","org","mycompany","project","package","myclass","","","","however","","when","reflect","on","this","we","try","and","load","","","org","mycompany","project","myclass","support","for","classes","defined","in","package","objects"],"filtered":["define","class","inside","package","object","","name","ends","something","like","","","org","mycompany","project","package","myclass","","","","however","","reflect","try","load","","","org","mycompany","project","myclass","support","classes","defined","package","objects"],"features":{"type":0,"size":1000,"indices":[15,36,76,82,122,126,128,159,170,174,256,258,266,330,333,336,338,343,372,373,374,425,445,463,534,535,553,650,671,673,695,710,809,993,999],"values":[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,9.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"When you have an error in your R code using the RDD API  you always get as error message  Error in if  returnStatus           argument is of length zero This is not very useful and I think it might be better to catch the R exception and show it instead","_c1":"Improve error messages for RDD API","document":"When you have an error in your R code using the RDD API  you always get as error message  Error in if  returnStatus           argument is of length zero This is not very useful and I think it might be better to catch the R exception and show it instead Improve error messages for RDD API","words":["when","you","have","an","error","in","your","r","code","using","the","rdd","api","","you","always","get","as","error","message","","error","in","if","","returnstatus","","","","","","","","","","","argument","is","of","length","zero","this","is","not","very","useful","and","i","think","it","might","be","better","to","catch","the","r","exception","and","show","it","instead","improve","error","messages","for","rdd","api"],"filtered":["error","r","code","using","rdd","api","","always","get","error","message","","error","","returnstatus","","","","","","","","","","","argument","length","zero","useful","think","might","better","catch","r","exception","show","instead","improve","error","messages","rdd","api"],"features":{"type":0,"size":1000,"indices":[13,18,19,36,76,116,153,170,257,263,272,281,299,329,333,343,372,373,388,420,425,445,495,522,564,570,572,593,624,644,656,691,710,752,813,863,870,941,944,959,981,985],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,6.0,1.0,13.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"Whenever a new patch is submitted for verification   test patch   process has to make sure that none of Herriot bindings were broken","_c1":"test patch needs to verify Herriot integrity","document":"Whenever a new patch is submitted for verification   test patch   process has to make sure that none of Herriot bindings were broken test patch needs to verify Herriot integrity","words":["whenever","a","new","patch","is","submitted","for","verification","","","test","patch","","","process","has","to","make","sure","that","none","of","herriot","bindings","were","broken","test","patch","needs","to","verify","herriot","integrity"],"filtered":["whenever","new","patch","submitted","verification","","","test","patch","","","process","make","sure","none","herriot","bindings","broken","test","patch","needs","verify","herriot","integrity"],"features":{"type":0,"size":1000,"indices":[22,25,36,61,67,137,170,266,281,327,343,356,372,388,443,461,525,580,586,666,760,821,831,962,979],"values":[1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":0}
{"_c0":"Whenever we aggregate data by event time  we want to consider data is late and out of order in terms of its event time  Since we keep aggregate keyed by the time as state  the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data  Since the state is a store in memory  we have to prevent building up of this unbounded state  Hence  we need a watermarking mechanism by which we will mark data that is older beyond a threshold as  too late   and stop updating the aggregates with them  This would allow us to remove old aggregates that are never going to be updated  thus bounding the size of the state  Here is the design doc   https   docs google com document d  z Pazs v rA  azvmYhu I xwqaNQl ZLIS  xhkfCQ edit usp sharing","_c1":"Observed delay based event time watermarks","document":"Whenever we aggregate data by event time  we want to consider data is late and out of order in terms of its event time  Since we keep aggregate keyed by the time as state  the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data  Since the state is a store in memory  we have to prevent building up of this unbounded state  Hence  we need a watermarking mechanism by which we will mark data that is older beyond a threshold as  too late   and stop updating the aggregates with them  This would allow us to remove old aggregates that are never going to be updated  thus bounding the size of the state  Here is the design doc   https   docs google com document d  z Pazs v rA  azvmYhu I xwqaNQl ZLIS  xhkfCQ edit usp sharing Observed delay based event time watermarks","words":["whenever","we","aggregate","data","by","event","time","","we","want","to","consider","data","is","late","and","out","of","order","in","terms","of","its","event","time","","since","we","keep","aggregate","keyed","by","the","time","as","state","","the","state","will","grow","unbounded","if","we","keep","around","all","old","aggregates","in","an","attempt","consider","arbitrarily","late","data","","since","the","state","is","a","store","in","memory","","we","have","to","prevent","building","up","of","this","unbounded","state","","hence","","we","need","a","watermarking","mechanism","by","which","we","will","mark","data","that","is","older","beyond","a","threshold","as","","too","late","","","and","stop","updating","the","aggregates","with","them","","this","would","allow","us","to","remove","old","aggregates","that","are","never","going","to","be","updated","","thus","bounding","the","size","of","the","state","","here","is","the","design","doc","","","https","","","docs","google","com","document","d","","z","pazs","v","ra","","azvmyhu","i","xwqanql","zlis","","xhkfcq","edit","usp","sharing","observed","delay","based","event","time","watermarks"],"filtered":["whenever","aggregate","data","event","time","","want","consider","data","late","order","terms","event","time","","since","keep","aggregate","keyed","time","state","","state","grow","unbounded","keep","around","old","aggregates","attempt","consider","arbitrarily","late","data","","since","state","store","memory","","prevent","building","unbounded","state","","hence","","need","watermarking","mechanism","mark","data","older","beyond","threshold","","late","","","stop","updating","aggregates","","allow","us","remove","old","aggregates","never","going","updated","","thus","bounding","size","state","","design","doc","","","https","","","docs","google","com","document","d","","z","pazs","v","ra","","azvmyhu","xwqanql","zlis","","xhkfcq","edit","usp","sharing","observed","delay","based","event","time","watermarks"],"features":{"type":0,"size":1000,"indices":[48,79,94,105,126,128,135,138,144,157,163,170,189,192,208,219,221,223,229,231,242,251,258,266,280,281,286,288,296,298,299,312,329,333,343,349,371,372,373,388,403,404,420,429,438,445,449,477,483,490,493,508,536,537,545,572,585,594,596,597,625,633,644,650,654,656,666,672,681,684,695,710,712,714,718,737,743,752,760,768,788,796,797,805,820,843,852,899,924,936,945,968,993,998],"values":[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,4.0,5.0,3.0,20.0,2.0,4.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,4.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0]},"cluster_label":11}
{"_c0":"While reviewing   yhuai  s patch for SPARK       I noticed that Exchange s   compatible   check may be incorrectly returning   false   in many cases  As far as I know  this is not actually a problem because the   compatible      meetsRequirements    and   needsAnySort   checks are serving only as short circuit performance optimizations that are not necessary for correctness  In order to reduce code complexity  I think that we should remove these checks and unconditionally rewrite the operator s children  This should be safe because we rewrite the tree in a single bottom up pass","_c1":"Remove compatibleWith  meetsRequirements  and needsAnySort checks from Exchange","document":"While reviewing   yhuai  s patch for SPARK       I noticed that Exchange s   compatible   check may be incorrectly returning   false   in many cases  As far as I know  this is not actually a problem because the   compatible      meetsRequirements    and   needsAnySort   checks are serving only as short circuit performance optimizations that are not necessary for correctness  In order to reduce code complexity  I think that we should remove these checks and unconditionally rewrite the operator s children  This should be safe because we rewrite the tree in a single bottom up pass Remove compatibleWith  meetsRequirements  and needsAnySort checks from Exchange","words":["while","reviewing","","","yhuai","","s","patch","for","spark","","","","","","","i","noticed","that","exchange","s","","","compatible","","","check","may","be","incorrectly","returning","","","false","","","in","many","cases","","as","far","as","i","know","","this","is","not","actually","a","problem","because","the","","","compatible","","","","","","meetsrequirements","","","","and","","","needsanysort","","","checks","are","serving","only","as","short","circuit","performance","optimizations","that","are","not","necessary","for","correctness","","in","order","to","reduce","code","complexity","","i","think","that","we","should","remove","these","checks","and","unconditionally","rewrite","the","operator","s","children","","this","should","be","safe","because","we","rewrite","the","tree","in","a","single","bottom","up","pass","remove","compatiblewith","","meetsrequirements","","and","needsanysort","checks","from","exchange"],"filtered":["reviewing","","","yhuai","","patch","spark","","","","","","","noticed","exchange","","","compatible","","","check","may","incorrectly","returning","","","false","","","many","cases","","far","know","","actually","problem","","","compatible","","","","","","meetsrequirements","","","","","","needsanysort","","","checks","serving","short","circuit","performance","optimizations","necessary","correctness","","order","reduce","code","complexity","","think","remove","checks","unconditionally","rewrite","operator","children","","safe","rewrite","tree","single","bottom","pass","remove","compatiblewith","","meetsrequirements","","needsanysort","checks","exchange"],"features":{"type":0,"size":1000,"indices":[18,36,44,58,101,105,128,137,138,170,188,197,199,281,288,318,329,333,361,372,373,383,388,392,420,421,445,447,456,461,476,502,530,531,564,572,576,642,656,665,666,671,677,690,697,707,710,716,718,737,738,745,756,759,760,779,818,871,882,899,921,923,951,964,993],"values":[2.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,2.0,1.0,5.0,3.0,1.0,38.0,2.0,1.0,1.0,2.0,1.0,2.0,3.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0]},"cluster_label":1}
{"_c0":"While running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below   Stack trace   org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java     org xerial snappy SnappyInputStream rawRead SnappyInputStream java      org xerial snappy SnappyInputStream read SnappyInputStream java      java io DataInputStream readFully DataInputStream java      java io DataInputStream readLong DataInputStream java      org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java     org apache spark util collection unsafe sort UnsafeSorterSpillMerger   loadNext UnsafeSorterSpillMerger java     org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to use with non JNI based System arrayCopy method in this case","_c1":"Significant amount of CPU is being consumed in SnappyNative arrayCopy method","document":"While running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below   Stack trace   org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java     org xerial snappy SnappyInputStream rawRead SnappyInputStream java      org xerial snappy SnappyInputStream read SnappyInputStream java      java io DataInputStream readFully DataInputStream java      java io DataInputStream readLong DataInputStream java      org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java     org apache spark util collection unsafe sort UnsafeSorterSpillMerger   loadNext UnsafeSorterSpillMerger java     org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to use with non JNI based System arrayCopy method in this case Significant amount of CPU is being consumed in SnappyNative arrayCopy method","words":["while","running","a","spark","job","which","is","spilling","a","lot","of","data","in","reduce","phase","","we","see","that","significant","amount","of","cpu","is","being","consumed","in","native","snappy","arraycopy","method","","please","see","the","stack","trace","below","","","stack","trace","","","org","xerial","snappy","snappynative","","","yjp","","arraycopy","native","method","","org","xerial","snappy","snappynative","arraycopy","snappynative","java","","org","xerial","snappy","snappy","arraycopy","snappy","java","","","","","org","xerial","snappy","snappyinputstream","rawread","snappyinputstream","java","","","","","","org","xerial","snappy","snappyinputstream","read","snappyinputstream","java","","","","","","java","io","datainputstream","readfully","datainputstream","java","","","","","","java","io","datainputstream","readlong","datainputstream","java","","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillreader","loadnext","unsafesorterspillreader","java","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillmerger","","","loadnext","unsafesorterspillmerger","java","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","the","reason","for","that","is","the","spillreader","does","a","lot","of","small","reads","from","the","underlying","snappy","compressed","stream","and","snappyinputstream","invokes","native","jni","arraycopy","method","to","copy","the","data","","which","is","expensive","","we","should","fix","snappy","","java","to","use","with","non","jni","based","system","arraycopy","method","in","this","case","significant","amount","of","cpu","is","being","consumed","in","snappynative","arraycopy","method"],"filtered":["running","spark","job","spilling","lot","data","reduce","phase","","see","significant","amount","cpu","consumed","native","snappy","arraycopy","method","","please","see","stack","trace","","","stack","trace","","","org","xerial","snappy","snappynative","","","yjp","","arraycopy","native","method","","org","xerial","snappy","snappynative","arraycopy","snappynative","java","","org","xerial","snappy","snappy","arraycopy","snappy","java","","","","","org","xerial","snappy","snappyinputstream","rawread","snappyinputstream","java","","","","","","org","xerial","snappy","snappyinputstream","read","snappyinputstream","java","","","","","","java","io","datainputstream","readfully","datainputstream","java","","","","","","java","io","datainputstream","readlong","datainputstream","java","","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillreader","loadnext","unsafesorterspillreader","java","","","","","org","apache","spark","util","collection","unsafe","sort","unsafesorterspillmerger","","","loadnext","unsafesorterspillmerger","java","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","org","apache","spark","sql","execution","unsafeexternalrowsorter","","","next","unsafeexternalrowsorter","java","","","","","","reason","spillreader","lot","small","reads","underlying","snappy","compressed","stream","snappyinputstream","invokes","native","jni","arraycopy","method","copy","data","","expensive","","fix","snappy","","java","use","non","jni","based","system","arraycopy","method","case","significant","amount","cpu","consumed","snappynative","arraycopy","method"],"features":{"type":0,"size":1000,"indices":[36,62,101,105,155,159,169,170,187,191,197,215,216,218,224,230,240,242,268,273,281,284,317,320,333,342,343,372,373,374,388,445,469,470,489,490,495,502,515,517,526,528,535,537,597,602,603,625,637,639,650,654,665,686,692,695,698,699,707,710,720,735,742,760,772,778,803,838,841,855,889,921,945,963,967,993],"values":[1.0,1.0,2.0,5.0,2.0,1.0,1.0,3.0,3.0,2.0,4.0,4.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,5.0,2.0,1.0,10.0,1.0,2.0,4.0,62.0,1.0,2.0,3.0,5.0,5.0,1.0,1.0,2.0,4.0,1.0,2.0,5.0,4.0,1.0,9.0,2.0,2.0,1.0,7.0,1.0,1.0,1.0,2.0,5.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,5.0,2.0,3.0,1.0,2.0,2.0,2.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,13.0,2.0]},"cluster_label":5}
{"_c0":"ZStandard  https   github com facebook zstd has been used in production for   months by facebook now  v    was recently released  Create a codec for this library","_c1":"Add Codec for ZStandard Compression","document":"ZStandard  https   github com facebook zstd has been used in production for   months by facebook now  v    was recently released  Create a codec for this library Add Codec for ZStandard Compression","words":["zstandard","","https","","","github","com","facebook","zstd","has","been","used","in","production","for","","","months","by","facebook","now","","v","","","","was","recently","released","","create","a","codec","for","this","library","add","codec","for","zstandard","compression"],"filtered":["zstandard","","https","","","github","com","facebook","zstd","used","production","","","months","facebook","","v","","","","recently","released","","create","codec","library","add","codec","zstandard","compression"],"features":{"type":0,"size":1000,"indices":[6,34,36,98,141,170,193,221,223,234,265,334,336,352,372,373,432,445,446,477,510,535,580,605,727,812,998],"values":[1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0]},"cluster_label":2}
{"_c0":"already has quotas for HDFS namespace  HADOOP        HADOOP      implements similar quotas for disk space on HDFS in       This jira proposes to port HADOOP      to","_c1":"Port HDFS space quotas to","document":"already has quotas for HDFS namespace  HADOOP        HADOOP      implements similar quotas for disk space on HDFS in       This jira proposes to port HADOOP      to Port HDFS space quotas to","words":["already","has","quotas","for","hdfs","namespace","","hadoop","","","","","","","","hadoop","","","","","","implements","similar","quotas","for","disk","space","on","hdfs","in","","","","","","","this","jira","proposes","to","port","hadoop","","","","","","to","port","hdfs","space","quotas","to"],"filtered":["already","quotas","hdfs","namespace","","hadoop","","","","","","","","hadoop","","","","","","implements","similar","quotas","disk","space","hdfs","","","","","","","jira","proposes","port","hadoop","","","","","","port","hdfs","space","quotas"],"features":{"type":0,"size":1000,"indices":[36,57,82,181,207,342,372,373,388,404,445,558,580,821,889,910,967,994],"values":[2.0,1.0,1.0,3.0,2.0,3.0,24.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0]},"cluster_label":11}
{"_c0":"cc   nongli    please attach the design doc","_c1":"bucketed table support","document":"cc   nongli    please attach the design doc bucketed table support","words":["cc","","","nongli","","","","please","attach","the","design","doc","bucketed","table","support"],"filtered":["cc","","","nongli","","","","please","attach","design","doc","bucketed","table","support"],"features":{"type":0,"size":1000,"indices":[25,189,372,605,681,695,710,769,837,841,918],"values":[1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"currently in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame  However  collect   currently can t collect data of nested types from a DataFrame because     The serializer in JVM backend does not support nested types     collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector","_c1":"Improve the implementation of collect   on DataFrame in SparkR","document":"currently in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame  However  collect   currently can t collect data of nested types from a DataFrame because     The serializer in JVM backend does not support nested types     collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector Improve the implementation of collect   on DataFrame in SparkR","words":["currently","in","sparkr","","collect","","","on","a","dataframe","collects","the","data","within","the","dataframe","into","a","local","data","frame","","r","users","are","used","to","using","data","frame","","however","","collect","","","currently","can","t","collect","data","of","nested","types","from","a","dataframe","because","","","","","the","serializer","in","jvm","backend","does","not","support","nested","types","","","","","collect","","","in","r","side","assumes","each","column","is","of","simple","atomic","type","that","can","be","combinded","into","a","atomic","vector","improve","the","implementation","of","collect","","","on","dataframe","in","sparkr"],"filtered":["currently","sparkr","","collect","","","dataframe","collects","data","within","dataframe","local","data","frame","","r","users","used","using","data","frame","","however","","collect","","","currently","collect","data","nested","types","dataframe","","","","","serializer","jvm","backend","support","nested","types","","","","","collect","","","r","side","assumes","column","simple","atomic","type","combinded","atomic","vector","improve","implementation","collect","","","dataframe","sparkr"],"features":{"type":0,"size":1000,"indices":[5,18,82,101,138,161,170,281,300,343,372,388,412,421,445,465,522,526,570,593,594,601,605,608,624,656,673,695,698,710,742,755,760,763,767,777,781,789,833,852,885,891,921,953,980],"values":[1.0,1.0,4.0,1.0,1.0,4.0,4.0,1.0,1.0,3.0,20.0,1.0,1.0,1.0,4.0,2.0,1.0,2.0,2.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,2.0,4.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0]},"cluster_label":11}
{"_c0":"distinct   and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns","_c1":"Implement dropDuplicates   method of DataFrame in SparkR","document":"distinct   and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns Implement dropDuplicates   method of DataFrame in SparkR","words":["distinct","","","and","unique","","","drop","duplicated","rows","on","all","columns","","while","dropduplicates","","","can","drop","duplicated","rows","on","selected","columns","implement","dropduplicates","","","method","of","dataframe","in","sparkr"],"filtered":["distinct","","","unique","","","drop","duplicated","rows","columns","","dropduplicates","","","drop","duplicated","rows","selected","columns","implement","dropduplicates","","","method","dataframe","sparkr"],"features":{"type":0,"size":1000,"indices":[43,82,161,333,343,372,373,438,445,472,511,577,654,670,707,767,833,968,969],"values":[1.0,2.0,1.0,3.0,1.0,9.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":2}
{"_c0":"h   Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store  ADL  from within Hadoop  This would enable existing Hadoop applications such has MR  HIVE  Hbase etc    to use ADL store as input or output  ADL is ultra high capacity  Optimized for massive throughput with rich management and security features  More details available at https   azure microsoft com en us services data lake store","_c1":"Support Microsoft Azure Data Lake   as a file system in Hadoop","document":"h   Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store  ADL  from within Hadoop  This would enable existing Hadoop applications such has MR  HIVE  Hbase etc    to use ADL store as input or output  ADL is ultra high capacity  Optimized for massive throughput with rich management and security features  More details available at https   azure microsoft com en us services data lake store Support Microsoft Azure Data Lake   as a file system in Hadoop","words":["h","","","description","this","jira","describes","a","new","file","system","implementation","for","accessing","microsoft","azure","data","lake","store","","adl","","from","within","hadoop","","this","would","enable","existing","hadoop","applications","such","has","mr","","hive","","hbase","etc","","","","to","use","adl","store","as","input","or","output","","adl","is","ultra","high","capacity","","optimized","for","massive","throughput","with","rich","management","and","security","features","","more","details","available","at","https","","","azure","microsoft","com","en","us","services","data","lake","store","support","microsoft","azure","data","lake","","","as","a","file","system","in","hadoop"],"filtered":["h","","","description","jira","describes","new","file","system","implementation","accessing","microsoft","azure","data","lake","store","","adl","","within","hadoop","","enable","existing","hadoop","applications","mr","","hive","","hbase","etc","","","","use","adl","store","input","output","","adl","ultra","high","capacity","","optimized","massive","throughput","rich","management","security","features","","details","available","https","","","azure","microsoft","com","en","us","services","data","lake","store","support","microsoft","azure","data","lake","","","file","system","hadoop"],"features":{"type":0,"size":1000,"indices":[0,3,25,29,36,64,101,108,116,122,128,163,170,181,187,208,212,221,272,280,281,282,333,344,356,371,372,373,388,409,412,439,445,489,572,580,589,599,629,634,639,650,695,698,723,730,742,743,755,756,809,810,821,853,921,952,998],"values":[1.0,1.0,2.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,2.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,17.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,4.0,1.0,3.0,1.0,1.0,3.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":17}
{"_c0":"hadoop common src main bin hadoop config sh needs to be updated post MR   eg the layout of mapred home has changed","_c1":"hadoop config sh needs to be updated post MR","document":"hadoop common src main bin hadoop config sh needs to be updated post MR   eg the layout of mapred home has changed hadoop config sh needs to be updated post MR","words":["hadoop","common","src","main","bin","hadoop","config","sh","needs","to","be","updated","post","mr","","","eg","the","layout","of","mapred","home","has","changed","hadoop","config","sh","needs","to","be","updated","post","mr"],"filtered":["hadoop","common","src","main","bin","hadoop","config","sh","needs","updated","post","mr","","","eg","layout","mapred","home","changed","hadoop","config","sh","needs","updated","post","mr"],"features":{"type":0,"size":1000,"indices":[9,181,237,272,343,352,365,372,388,392,490,580,637,656,666,710,846,954,973,982,988],"values":[1.0,3.0,1.0,2.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,2.0,1.0,2.0]},"cluster_label":13}
{"_c0":"hash scala was getting pretty long and it s not obvious that hash expressions belong there  Creating a hash scala to put all the hash expressions","_c1":"Move hash expressions from misc scala into hash scala","document":"hash scala was getting pretty long and it s not obvious that hash expressions belong there  Creating a hash scala to put all the hash expressions Move hash expressions from misc scala into hash scala","words":["hash","scala","was","getting","pretty","long","and","it","s","not","obvious","that","hash","expressions","belong","there","","creating","a","hash","scala","to","put","all","the","hash","expressions","move","hash","expressions","from","misc","scala","into","hash","scala"],"filtered":["hash","scala","getting","pretty","long","obvious","hash","expressions","belong","","creating","hash","scala","put","hash","expressions","move","hash","expressions","misc","scala","hash","scala"],"features":{"type":0,"size":1000,"indices":[18,25,130,132,170,197,234,282,333,343,372,374,388,418,490,495,710,760,767,831,863,891,904,921,968,993],"values":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"is missing in pom xml  That way   mvn versions set   does not work for the project","_c1":"Missing hadoop cloud storage project module in pom xml","document":"is missing in pom xml  That way   mvn versions set   does not work for the project Missing hadoop cloud storage project module in pom xml","words":["is","missing","in","pom","xml","","that","way","","","mvn","versions","set","","","does","not","work","for","the","project","missing","hadoop","cloud","storage","project","module","in","pom","xml"],"filtered":["missing","pom","xml","","way","","","mvn","versions","set","","","work","project","missing","hadoop","cloud","storage","project","module","pom","xml"],"features":{"type":0,"size":1000,"indices":[18,36,91,92,159,181,240,281,299,360,372,394,445,525,527,671,698,710,760,813],"values":[1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,2.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"kms dt currently does not have its own token identifier class to de","_c1":"Support decoding KMS Delegation Token with its own Identifier","document":"kms dt currently does not have its own token identifier class to de Support decoding KMS Delegation Token with its own Identifier","words":["kms","dt","currently","does","not","have","its","own","token","identifier","class","to","de","support","decoding","kms","delegation","token","with","its","own","identifier"],"filtered":["kms","dt","currently","token","identifier","class","de","support","decoding","kms","delegation","token","identifier"],"features":{"type":0,"size":1000,"indices":[18,228,281,296,299,360,388,494,528,534,615,650,695,698,763,782,906],"values":[1.0,2.0,1.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"kms was not rewritten to use the new shell framework  It should be reworked to take advantage of it","_c1":"Rewrite kms to use new shell framework","document":"kms was not rewritten to use the new shell framework  It should be reworked to take advantage of it Rewrite kms to use new shell framework","words":["kms","was","not","rewritten","to","use","the","new","shell","framework","","it","should","be","reworked","to","take","advantage","of","it","rewrite","kms","to","use","new","shell","framework"],"filtered":["kms","rewritten","use","new","shell","framework","","reworked","take","advantage","rewrite","kms","use","new","shell","framework"],"features":{"type":0,"size":1000,"indices":[3,18,25,123,234,343,372,388,489,495,534,599,615,656,665,710,738,855,928],"values":[1.0,1.0,2.0,2.0,1.0,1.0,1.0,3.0,2.0,2.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":13}
{"_c0":"make MultilayerPerceptronClassifier layers and weights public","_c1":"make MultilayerPerceptronClassifier layers and weights public","document":"make MultilayerPerceptronClassifier layers and weights public make MultilayerPerceptronClassifier layers and weights public","words":["make","multilayerperceptronclassifier","layers","and","weights","public","make","multilayerperceptronclassifier","layers","and","weights","public"],"filtered":["make","multilayerperceptronclassifier","layers","weights","public","make","multilayerperceptronclassifier","layers","weights","public"],"features":{"type":0,"size":1000,"indices":[333,359,415,498,504,525],"values":[2.0,2.0,2.0,2.0,2.0,2.0]},"cluster_label":13}
{"_c0":"mqtt client       was removed from the Eclipse Paho repository  and hence is breaking Spark build","_c1":"Upgrade MQTT dependency to use mqtt client","document":"mqtt client       was removed from the Eclipse Paho repository  and hence is breaking Spark build Upgrade MQTT dependency to use mqtt client","words":["mqtt","client","","","","","","","was","removed","from","the","eclipse","paho","repository","","and","hence","is","breaking","spark","build","upgrade","mqtt","dependency","to","use","mqtt","client"],"filtered":["mqtt","client","","","","","","","removed","eclipse","paho","repository","","hence","breaking","spark","build","upgrade","mqtt","dependency","use","mqtt","client"],"features":{"type":0,"size":1000,"indices":[3,66,105,135,234,242,281,333,344,372,388,489,512,536,588,710,796,805,921],"values":[1.0,3.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"mutate   in the dplyr package supports adding new columns and replacing existing columns  But currently the implementation of mutate   in SparkR supports adding new columns only  Also make the behavior of mutate more consistent with that in dplyr     Throw error message when there are duplicated column names in the DataFrame being mutated     when there are duplicated column names in specified columns by arguments  the last column of the same name takes effect","_c1":"Enhance mutate   to support replace existing columns","document":"mutate   in the dplyr package supports adding new columns and replacing existing columns  But currently the implementation of mutate   in SparkR supports adding new columns only  Also make the behavior of mutate more consistent with that in dplyr     Throw error message when there are duplicated column names in the DataFrame being mutated     when there are duplicated column names in specified columns by arguments  the last column of the same name takes effect Enhance mutate   to support replace existing columns","words":["mutate","","","in","the","dplyr","package","supports","adding","new","columns","and","replacing","existing","columns","","but","currently","the","implementation","of","mutate","","","in","sparkr","supports","adding","new","columns","only","","also","make","the","behavior","of","mutate","more","consistent","with","that","in","dplyr","","","","","throw","error","message","when","there","are","duplicated","column","names","in","the","dataframe","being","mutated","","","","","when","there","are","duplicated","column","names","in","specified","columns","by","arguments","","the","last","column","of","the","same","name","takes","effect","enhance","mutate","","","to","support","replace","existing","columns"],"filtered":["mutate","","","dplyr","package","supports","adding","new","columns","replacing","existing","columns","","currently","implementation","mutate","","","sparkr","supports","adding","new","columns","","also","make","behavior","mutate","consistent","dplyr","","","","","throw","error","message","duplicated","column","names","dataframe","mutated","","","","","duplicated","column","names","specified","columns","arguments","","last","column","name","takes","effect","enhance","mutate","","","support","replace","existing","columns"],"features":{"type":0,"size":1000,"indices":[15,25,76,83,96,138,161,175,181,189,223,231,254,266,267,276,333,343,371,372,374,388,445,460,525,601,629,650,656,695,698,710,735,760,763,767,773,787,792,793,800,831,899,954,969,981,989,994],"values":[1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,4.0,3.0,2.0,17.0,1.0,1.0,5.0,2.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,6.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,2.0]},"cluster_label":11}
{"_c0":"projectList is useless  Remove it from the class Window  It simplifies the codes in Analyzer and Optimizer","_c1":"Remove projectList from Windows","document":"projectList is useless  Remove it from the class Window  It simplifies the codes in Analyzer and Optimizer Remove projectList from Windows","words":["projectlist","is","useless","","remove","it","from","the","class","window","","it","simplifies","the","codes","in","analyzer","and","optimizer","remove","projectlist","from","windows"],"filtered":["projectlist","useless","","remove","class","window","","simplifies","codes","analyzer","optimizer","remove","projectlist","windows"],"features":{"type":0,"size":1000,"indices":[188,281,288,295,333,372,445,495,511,534,599,629,710,798,830,906,921],"values":[2.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0]},"cluster_label":13}
{"_c0":"repartition  Returns a new   Dataset   that has exactly  numPartitions  partitions  coalesce  Returns a new   Dataset   that has exactly  numPartitions  partitions  Similar to coalesce defined on an   RDD    this operation results in a narrow dependency  e g  if you go from      partitions to     partitions  there will not be a shuffle  instead each of the     new partitions will claim    of the current partitions","_c1":"SQL  Support coalesce and repartition in Dataset APIs","document":"repartition  Returns a new   Dataset   that has exactly  numPartitions  partitions  coalesce  Returns a new   Dataset   that has exactly  numPartitions  partitions  Similar to coalesce defined on an   RDD    this operation results in a narrow dependency  e g  if you go from      partitions to     partitions  there will not be a shuffle  instead each of the     new partitions will claim    of the current partitions SQL  Support coalesce and repartition in Dataset APIs","words":["repartition","","returns","a","new","","","dataset","","","that","has","exactly","","numpartitions","","partitions","","coalesce","","returns","a","new","","","dataset","","","that","has","exactly","","numpartitions","","partitions","","similar","to","coalesce","defined","on","an","","","rdd","","","","this","operation","results","in","a","narrow","dependency","","e","g","","if","you","go","from","","","","","","partitions","to","","","","","partitions","","there","will","not","be","a","shuffle","","instead","each","of","the","","","","","new","partitions","will","claim","","","","of","the","current","partitions","sql","","support","coalesce","and","repartition","in","dataset","apis"],"filtered":["repartition","","returns","new","","","dataset","","","exactly","","numpartitions","","partitions","","coalesce","","returns","new","","","dataset","","","exactly","","numpartitions","","partitions","","similar","coalesce","defined","","","rdd","","","","operation","results","narrow","dependency","","e","g","","go","","","","","","partitions","","","","","partitions","","shuffle","","instead","","","","","new","partitions","claim","","","","current","partitions","sql","","support","coalesce","repartition","dataset","apis"],"features":{"type":0,"size":1000,"indices":[18,25,64,77,82,126,170,193,208,333,343,363,372,373,388,417,420,425,427,445,457,493,534,568,580,588,641,656,686,695,710,752,760,801,831,842,863,870,878,885,910,921],"values":[1.0,3.0,2.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,2.0,1.0,42.0,4.0,2.0,1.0,2.0,1.0,2.0,2.0,6.0,3.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":5}
{"_c0":"see discussion here  https   github com apache spark pull      issuecomment","_c1":"Improve performance of Decimal times   and casting from integral","document":"see discussion here  https   github com apache spark pull      issuecomment Improve performance of Decimal times   and casting from integral","words":["see","discussion","here","","https","","","github","com","apache","spark","pull","","","","","","issuecomment","improve","performance","of","decimal","times","","","and","casting","from","integral"],"filtered":["see","discussion","","https","","","github","com","apache","spark","pull","","","","","","issuecomment","improve","performance","decimal","times","","","casting","integral"],"features":{"type":0,"size":1000,"indices":[105,135,221,333,343,372,382,495,510,515,522,526,597,674,695,759,844,921,923,998],"values":[1.0,1.0,1.0,1.0,1.0,10.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]},"cluster_label":2}
{"_c0":"slaves sh and the slaves file should get replace with workers sh and a workers file","_c1":"replace slaves with workers","document":"slaves sh and the slaves file should get replace with workers sh and a workers file replace slaves with workers","words":["slaves","sh","and","the","slaves","file","should","get","replace","with","workers","sh","and","a","workers","file","replace","slaves","with","workers"],"filtered":["slaves","sh","slaves","file","get","replace","workers","sh","workers","file","replace","slaves","workers"],"features":{"type":0,"size":1000,"indices":[64,108,170,333,383,650,665,710,787,846,959],"values":[3.0,2.0,1.0,2.0,3.0,2.0,1.0,1.0,2.0,2.0,1.0]},"cluster_label":13}
{"_c0":"to increase productivity in our current project  which makes a heavy use of Hadoop   we wrote a small Eclipse based GUI application which basically consists in   views    a HDFS explorer adapted from Eclipse filesystem explorer example  For now  it includes the following features  o classical tree based browsing interface  with directory content being detailed in a   columns table  file name  file size  file type  o refresh button o delete file or directory  with confirm dialog   select files in the tree or table and click the  Delete  button o rename file or directory  simple click on the file in the table  type the new name and validate o open file with system editor  select the file in the table and click  Open  button  works on Windows  not on Linux  o internal drag   drop o external drag   drop from the local filesystem to the HDFS  the opposite doesn t work    a MapReduce  very  simple job launcher  o select the job XML configuration file o run the job o kill the job o visualize map and reduce progress with progress bars o open a browser on the Hadoop job tracker web interface INSTALLATION NOTES    Eclipse       JDK       import the archive in Eclipse   copy your hadoop conf file  hadoop default xml in  src  folder     this step should be moved in the GUI later   right click on the project and Run As    Eclipse Application   enjoy","_c1":"Eclipse based GUI  DFS explorer and basic Map Reduce job launcher","document":"to increase productivity in our current project  which makes a heavy use of Hadoop   we wrote a small Eclipse based GUI application which basically consists in   views    a HDFS explorer adapted from Eclipse filesystem explorer example  For now  it includes the following features  o classical tree based browsing interface  with directory content being detailed in a   columns table  file name  file size  file type  o refresh button o delete file or directory  with confirm dialog   select files in the tree or table and click the  Delete  button o rename file or directory  simple click on the file in the table  type the new name and validate o open file with system editor  select the file in the table and click  Open  button  works on Windows  not on Linux  o internal drag   drop o external drag   drop from the local filesystem to the HDFS  the opposite doesn t work    a MapReduce  very  simple job launcher  o select the job XML configuration file o run the job o kill the job o visualize map and reduce progress with progress bars o open a browser on the Hadoop job tracker web interface INSTALLATION NOTES    Eclipse       JDK       import the archive in Eclipse   copy your hadoop conf file  hadoop default xml in  src  folder     this step should be moved in the GUI later   right click on the project and Run As    Eclipse Application   enjoy Eclipse based GUI  DFS explorer and basic Map Reduce job launcher","words":["to","increase","productivity","in","our","current","project","","which","makes","a","heavy","use","of","hadoop","","","we","wrote","a","small","eclipse","based","gui","application","which","basically","consists","in","","","views","","","","a","hdfs","explorer","adapted","from","eclipse","filesystem","explorer","example","","for","now","","it","includes","the","following","features","","o","classical","tree","based","browsing","interface","","with","directory","content","being","detailed","in","a","","","columns","table","","file","name","","file","size","","file","type","","o","refresh","button","o","delete","file","or","directory","","with","confirm","dialog","","","select","files","in","the","tree","or","table","and","click","the","","delete","","button","o","rename","file","or","directory","","simple","click","on","the","file","in","the","table","","type","the","new","name","and","validate","o","open","file","with","system","editor","","select","the","file","in","the","table","and","click","","open","","button","","works","on","windows","","not","on","linux","","o","internal","drag","","","drop","o","external","drag","","","drop","from","the","local","filesystem","to","the","hdfs","","the","opposite","doesn","t","work","","","","a","mapreduce","","very","","simple","job","launcher","","o","select","the","job","xml","configuration","file","o","run","the","job","o","kill","the","job","o","visualize","map","and","reduce","progress","with","progress","bars","o","open","a","browser","on","the","hadoop","job","tracker","web","interface","installation","notes","","","","eclipse","","","","","","","jdk","","","","","","","import","the","archive","in","eclipse","","","copy","your","hadoop","conf","file","","hadoop","default","xml","in","","src","","folder","","","","","this","step","should","be","moved","in","the","gui","later","","","right","click","on","the","project","and","run","as","","","","eclipse","application","","","enjoy","eclipse","based","gui","","dfs","explorer","and","basic","map","reduce","job","launcher"],"filtered":["increase","productivity","current","project","","makes","heavy","use","hadoop","","","wrote","small","eclipse","based","gui","application","basically","consists","","","views","","","","hdfs","explorer","adapted","eclipse","filesystem","explorer","example","","","includes","following","features","","o","classical","tree","based","browsing","interface","","directory","content","detailed","","","columns","table","","file","name","","file","size","","file","type","","o","refresh","button","o","delete","file","directory","","confirm","dialog","","","select","files","tree","table","click","","delete","","button","o","rename","file","directory","","simple","click","file","table","","type","new","name","validate","o","open","file","system","editor","","select","file","table","click","","open","","button","","works","windows","","linux","","o","internal","drag","","","drop","o","external","drag","","","drop","local","filesystem","hdfs","","opposite","doesn","work","","","","mapreduce","","","simple","job","launcher","","o","select","job","xml","configuration","file","o","run","job","o","kill","job","o","visualize","map","reduce","progress","progress","bars","o","open","browser","hadoop","job","tracker","web","interface","installation","notes","","","","eclipse","","","","","","","jdk","","","","","","","import","archive","eclipse","","","copy","hadoop","conf","file","","hadoop","default","xml","","src","","folder","","","","","step","moved","gui","later","","","right","click","project","run","","","","eclipse","application","","","enjoy","eclipse","based","gui","","dfs","explorer","basic","map","reduce","job","launcher"],"features":{"type":0,"size":1000,"indices":[11,15,17,18,24,25,36,60,82,91,92,98,108,138,140,148,165,169,170,181,187,192,199,202,213,216,243,263,275,280,281,295,333,343,353,364,372,373,374,380,381,388,401,411,445,466,470,486,489,495,500,502,526,527,547,551,556,558,572,574,577,582,586,597,598,602,605,608,610,625,639,645,650,656,665,671,688,691,697,704,709,710,714,719,721,732,742,755,764,777,783,784,786,798,818,837,852,855,856,867,871,880,920,921,933,943,944,948,953,963,967,969,980,988,992,993],"values":[1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,2.0,1.0,10.0,1.0,1.0,1.0,1.0,2.0,6.0,4.0,3.0,1.0,2.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,6.0,2.0,6.0,1.0,1.0,4.0,74.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,9.0,1.0,6.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,2.0,3.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,4.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,19.0,1.0,4.0,3.0,3.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,2.0,12.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,2.0,1.0,2.0,1.0,3.0,1.0]},"cluster_label":3}
{"_c0":"toLocalIterator is available in Java and Scala  If we add this functionality to Python  then we can also be able to use PySpark to iterate over a dataset partition by partition","_c1":"Add toLocalIterator to pyspark rdd","document":"toLocalIterator is available in Java and Scala  If we add this functionality to Python  then we can also be able to use PySpark to iterate over a dataset partition by partition Add toLocalIterator to pyspark rdd","words":["tolocaliterator","is","available","in","java","and","scala","","if","we","add","this","functionality","to","python","","then","we","can","also","be","able","to","use","pyspark","to","iterate","over","a","dataset","partition","by","partition","add","tolocaliterator","to","pyspark","rdd"],"filtered":["tolocaliterator","available","java","scala","","add","functionality","python","","also","able","use","pyspark","iterate","dataset","partition","partition","add","tolocaliterator","pyspark","rdd"],"features":{"type":0,"size":1000,"indices":[50,170,223,281,312,333,352,365,371,372,373,381,388,432,445,489,490,493,496,501,509,589,656,792,833,870,967,993],"values":[2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,4.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]},"cluster_label":0}
{"_c0":"typeId is not needed in columnar cache  it s confusing to having them","_c1":"Remove typeId in columnar cache","document":"typeId is not needed in columnar cache  it s confusing to having them Remove typeId in columnar cache","words":["typeid","is","not","needed","in","columnar","cache","","it","s","confusing","to","having","them","remove","typeid","in","columnar","cache"],"filtered":["typeid","needed","columnar","cache","","confusing","remove","typeid","columnar","cache"],"features":{"type":0,"size":1000,"indices":[18,163,197,244,250,281,288,372,388,445,495,659,675,728,924],"values":[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning","_c1":"PySpark TrainValidationSplitModel should support validationMetrics","document":"validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning PySpark TrainValidationSplitModel should support validationMetrics","words":["validationmetrics","in","trainvalidationsplitmodel","should","also","be","supported","in","pyspark","ml","tuning","pyspark","trainvalidationsplitmodel","should","support","validationmetrics"],"filtered":["validationmetrics","trainvalidationsplitmodel","also","supported","pyspark","ml","tuning","pyspark","trainvalidationsplitmodel","support","validationmetrics"],"features":{"type":0,"size":1000,"indices":[324,445,509,559,593,596,656,665,695,783,792],"values":[1.0,2.0,2.0,2.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
{"_c0":"we have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation","_c1":"test patch s issue matching regex should be configurable","document":"we have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation test patch s issue matching regex should be configurable","words":["we","have","the","issue","matching","regex","configurable","via","altering","the","default","","add","in","a","cli","arg","to","update","it","on","invocation","test","patch","s","issue","matching","regex","should","be","configurable"],"filtered":["issue","matching","regex","configurable","via","altering","default","","add","cli","arg","update","invocation","test","patch","issue","matching","regex","configurable"],"features":{"type":0,"size":1000,"indices":[82,94,137,170,197,241,299,343,372,381,388,432,445,495,500,510,586,595,656,665,710,734,748,897,964,993],"values":[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0]},"cluster_label":13}
