Implement  struct    encode   decode  in SparkR as documented at http   spark apache org docs latest api scala index html org apache spark sql functions Implement  struct    encode   decode  in SparkR
repartition  Returns a new   Dataset   that has exactly  numPartitions  partitions  coalesce  Returns a new   Dataset   that has exactly  numPartitions  partitions  Similar to coalesce defined on an   RDD    this operation results in a narrow dependency  e g  if you go from      partitions to     partitions  there will not be a shuffle  instead each of the     new partitions will claim    of the current partitions SQL  Support coalesce and repartition in Dataset APIs
 unify GetStructField and GetInternalRowField
Spark SQL aggregate function stddev stddev pop stddev samp variance var pop var samp skewness kurtosis collect list collect set should support columnName as arguments like other aggregate function max min count sum Stddev Variance etc should support columnName as arguments
Change cumeDist    cume dist  denseRank    dense rank  percentRank    percent rank  rowNumber    row number  There are two reasons that we should make this change    We should follow the naming convention rule of R  http   www inside r org node           Spark DataFrame has deprecated the old convention  such as cumeDist  and will remove it in Spark      It s better to fix this issue before     release  otherwise we will make breaking API change Rename some window rank function names for SparkR
Add  isNaN  to  Column  for SparkR  Column should has three related variable functions  isNaN  isNull  isNotNull     Replace  DataFrame isNaN  with  DataFrame isnan  at SparkR side  Because DataFrame isNaN has been deprecated and will be removed at Spark          Add  isnull  to  DataFrame  for SparkR  DataFrame should has two related functions  isnan  isnull Fix usage of isnan  isNaN
I think that we should upgrade from Tachyon       to       in order to get the fix for https   tachyon atlassian net browse TACHYON Upgrade Tachyon dependency to
 Change numPartitions   in RDD to be  getNumPartitions  to be consistent with pyspark scala
 Replace shuffleManagerClass with shortShuffleMgrNames in ExternalShuffleBlockResolver
Currently we support read df  write df  jsonFile  parquetFile in SQLContext  we should support more external data source API such as read json  read parquet  read orc  read jdbc  read csv and so on  Some of the exist API is deprecated and will remove at Spark      we should also deprecate them at SparkR  Note  we should refer the DataFrameReader and DataFrameWriter API at Spark SQL but defined with R like style  DataFrameReader API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameReader DataFrameWriter API  http   spark apache org docs latest api scala index html org apache spark sql DataFrameWriter Support more external data source API in SparkR
Kafka     already released and it introduce new consumer API that not compatible with old one  So  I added new consumer api  I made separate classes in package org apache spark streaming kafka v   with changed API  I didn t remove old classes for more backward compatibility  User will not need to change his old spark applications when he uprgade to new Spark version  Please rewiew my changes Update KafkaDStreams to new Kafka      Consumer API
Created a new private variable  boundTEncoder  that can be shared by multiple functions   RDD    select  and  collect     Replaced all the  queryExecution analyzed  by the function call  logicalPlan    A few API comments are using wrong class names  e g    DataFrame   or parameter names  e g    n     A few API descriptions are wrong   e g    mapPartitions SQL  Code refactoring and comment correction in Dataset APIs
 Implement drop method for DataFrame in SparkR
Starting from Hive       the derby metastore can use a in memory backend  Since our execution hive is a fake metastore  if we use in memory mode  we can reduce the time that is used on creating the execution hive Use in memory for execution hive s derby metastore
mutate   in the dplyr package supports adding new columns and replacing existing columns  But currently the implementation of mutate   in SparkR supports adding new columns only  Also make the behavior of mutate more consistent with that in dplyr     Throw error message when there are duplicated column names in the DataFrame being mutated     when there are duplicated column names in specified columns by arguments  the last column of the same name takes effect Enhance mutate   to support replace existing columns
Similar to Dataset transform DataFrame transform function
There are still some SparkPlan does not support UnsafeRow  or does not support well Support UnsafeRow in all SparkPlan  if possible
 Support UnsafeRow in MapPartitions MapGroups CoGroup
 Support UnsafeRow in Coalesce Except Intersect
 Support UnsafeRow in LocalTableScan
There have been continuing requests  e g    SPARK        for allowing users to extend and modify MLlib models and algorithms  If you are a user who needs these changes  please comment here about what specifically needs to be modified for your use case Remove final from classes in spark ml trees and ensembles where possible
Use sqlContext from MLlibTestSparkContext rather than creating new one for spark ml test cases Use sqlContext from MLlibTestSparkContext for spark ml test suites
IsNotNull   filter is not being pushed down for JDBC datasource  It looks it is SQL standard according to SQL     SQL       SQL      and SQL    x and I believe most databases support this isnotnull operator not pushed down for JDBC datasource
Like shuffle file encryption in SPARK       spills data should also be encrypted Support shuffle spill encryption in Spark
distinct   and unique   drop duplicated rows on all columns  While dropDuplicates   can drop duplicated rows on selected columns Implement dropDuplicates   method of DataFrame in SparkR
As discussed here https   github com apache spark pull        it might need to implement   unhandledFilter   to remove duplicated Spark side filtering Implement unhandledFilter interface
Creating an actual logical physical operator for range for matching the performance of RDD Range APIs  Compared with the old Range API  the new version is   times faster than the old version Improve performance of Range APIs via adding logical physical operators
CSV is the most common data format in the  small data  world  It is often the first format people want to try when they see Spark on a single node  Making this built in for the most common source can provide a better experience for first time users  We should consider inlining https   github com databricks spark csv Have a built in CSV data source implementation
We should add SQLUserDefinedType support for encoder Add SQLUserDefinedType support for encoder
Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode configuration for cluster mode Remove spark deploy mesos recoveryMode and use spark deploy recoveryMode
Remove spark deploy mesos zookeeper url and use existing configuration spark deploy zookeeper url for Mesos cluster mode Remove spark deploy mesos zookeeper url and use spark deploy zookeeper url
Remove spark deploy mesos zookeeper dir and use existing configuration spark deploy zookeeper dir for Mesos cluster mode Remove spark deploy mesos zookeeper dir and use spark deploy zookeeper dir
We should update to the latest version of Zinc in order to match our SBT version Upgrade Zinc from         to
Input  SELECT   FROM jdbcTable WHERE col     xxx  Current plan Implement JdbcRelation unhandledFilters for removing unnecessary Spark Filter
Right now the Java users cannot use ActorHelper because it uses special Scala syntax  This patch just refactored the codes to provide Java API and add an example Refactor ActorReceiver to support Java
We can provides the option to choose JSON parser can be enabled to accept quoting of all character or not  For example  if JSON file that includes not listed by JSON backslash quoting specification  it returns corrupt record    This issue similar to HIVE        HIVE Add option to accept quoting of all character backslash quoting mechanism
cc   nongli    please attach the design doc bucketed table support
 Support intersect except in Hive SQL
 Support window functions in SQLContext
Right now  numFields will be passed in by pointTo    then bitSetWidthInBytes is calculated  making pointTo   a little bit heavy  It should be part of constructor of UnsafeRow The numFields of UnsafeRow should not changed by pointTo
MLlib s Transformer uses the deprecated callUDF API Remove the use of the deprecated callUDF in MLlib
Before        https   github com apache spark pull        submitJob would create a separate thread to wait for the job result   submitJobThreadPool  was a workaround in  ReceiverTracker  to run these waiting job result threads  Now        https   github com apache spark pull       has been merged to master and resolved this blocking issue   submitJobThreadPool  can be removed now Remove submitJobThreadPool since submitJob doesn t create a separate thread to wait for the job result
This is beneficial just from a code testability point of view to be able to exercise individual components  Also makes it easy to benchmark it  It would be able to read data without need to create al the associate hadoop input split  etc components Expose API on UnsafeRowRecordReader to just run on files
Add hash function for SparkR SparkR support hash function
Our current Intersect physical operator simply delegates to RDD intersect  We should remove the Intersect physical operator and simply transform a logical intersect into a semi join  This way  we can take advantage of all the benefits of join implementations  e g  managed memory  code generation  broadcast joins Rewrite Intersect phyiscal plan using semi join
Right now  We use a Filter follow SortMergeJoin or BroadcastHashJoin for conditions  the result projection of join could be very expensive if they generate lots of rows  could be reduce mostly by condition SortMergeJoin and BroadcastHashJoin should support condition
It would be easier to fix bugs and maintain the ec  script separately from Spark releases  For more information  see https   issues apache org jira browse SPARK Move spark ec  scripts to AMPLab
As a pre requisite to off heap caching of blocks  we need a mechanism to prevent pages   blocks from being evicted while they are being read  With on heap objects  evicting a block while it is being read merely leads to memory accounting problems  because we assume that an evicted block is a candidate for garbage collection  which will not be true during a read   but with off heap memory this will lead to either data corruption or segmentation faults  To address this  we should add a reference counting mechanism to track which blocks pages are being read in order to prevent them from being evicted prematurely  I propose to do this in two phases  first  add a safe  conservative approach in which all BlockManager get    calls implicitly increment the reference count of blocks and where tasks  references are automatically freed upon task completion  This will be correct but may have adverse performance impacts because it will prevent legitimate block evictions  In phase two  we should incrementally add release   calls in order to fix the eviction of unreferenced blocks  The latter change may need to touch many different components  which is why I propose to do it separately in order to make the changes easier to reason about and review Use reference counting to prevent blocks from being evicted during reads
In Spark      MLlib provides logistic regression and linear regression with L  L  elastic net regularization  We want to expand the support of generalized linear models  GLMs  in      e g   Poisson Gamma families and more link functions  SPARK      implements a GLM solver for the case when the number of features is small  We also need to design an interface for GLMs  In SparkR  we can simply follow glm or glmnet  On the Python Scala Java side  the interface should be consistent with LinearRegression and LogisticRegression  e g     from GeneralizedLinearModel Estimator interface for generalized linear models  GLMs
CacheManager directly calls MemoryStore unrollSafely   and has its own logic for handling graceful fallback to disk when cached data does not fit in memory  However  this logic also exists inside of the MemoryStore itself  so this appears to be unnecessary duplication  Thanks to the addition of block level read write locks  we can refactor the code to remove the CacheManager and replace it with an atomic getOrElseUpdate BlockManager method Remove CacheManager and replace it with new BlockManager getOrElseUpdate method
I was investingating progress in SPARK       and I noticed that there is no TrainValidationSplit class in pyspark ml tuning module  Java Scala s examples SPARK       use org apache spark ml tuning TrainValidationSplit that is not available from Python and this blocks SPARK        Does the class have different name in PySpark  maybe  Also  I couldn t find any JIRA task to saying it need to be implemented  Is it by design that the TrainValidationSplit estimator is not ported to PySpark  If not  that is if the estimator needs porting then I would like to contribute TrainValidationSplit is missing in pyspark ml tuning
As benchmarked and discussed here  https   github com apache spark pull       files r          Benefits from codegen  the declarative aggregate function could be much faster than imperative one  we should re implement all the builtin aggregate functions as declarative one  For skewness and kurtosis  we need to benchmark it to make sure that the declarative one is actually faster than imperative one Reimplement stat functions as declarative function
Parquet files benefit from vectorized decoding  ColumnarBatches have been designed to support this  This means that a single encoded parquet column is decoded to a single ColumnVector Vectorize parquet decoding using ColumnarBatch
Implement a simple wrapper of AFTSurvivalRegression in SparkR to support survival analysis Survival analysis in SparkR
Implement a simple wrapper in SparkR to support k means K means wrapper in SparkR
The StateDStream currently does not provide the batch time as input to the state update function  This is required in cases where the behavior depends on the batch start time  We  Conviva  have been patching it manually for the past several Spark versions but we thought it might be useful for others as well Add API for updateStateByKey to provide batch time as input
As of Spark      Spark SQL internally has only a limited catalog  and does not support any of the DDLs  This is an umbrella ticket to introduce an internal API for a system catalog  and the associated DDL implementations using this API Native database table system catalog
 remove GenericInternalRowWithSchema
When you define a class inside of a package object  the name ends up being something like   org mycompany project package MyClass    However  when reflect on this we try and load   org mycompany project MyClass Support for classes defined in package objects
We currently delegate most DDLs directly to Hive  through NativePlaceholder in HiveQl scala  In Spark      we want to provide native implementations for DDLs for both SQLContext and HiveContext  The first step is to properly parse these DDLs  and then create logical commands that encapsulate them  The actual implementation can still delegate to HiveNativeCommand  As an example  we should define a command for RenameTable with the proper fields  and just delegate the implementation to HiveNativeCommand  we might need to track the original sql query in order to run HiveNativeCommand  but we can remove the sql query in the future once we do the next step   Once we flush out the internal persistent catalog API  we can then switch the implementation of these newly added commands to use the catalog API Create native DDL commands
Spark SQL should collapse adjacent   Repartition   operators and only keep the last one Collapse adjacent Repartition operations
in newMutableProjection  it will fallback to InterpretedMutableProjection if failed to compile  Since we remove the configuration for codegen  we are heavily reply on codegen  also TungstenAggregate require the generated MutableProjection to update UnsafeRow   should remove the fallback  which could make user confusing  see the discussion in SPARK Remove fallback in codegen
For lots of SQL operators  we have metrics for both of input and output  the number of input rows should be exactly the number of output rows of child  we could only have metrics for output rows  After we improve the performance using whole stage codegen  the overhead of SQL metrics are not trivial anymore  we should avoid that if it s not necessary  Some of the operator does not have SQL metrics  we should add that for them  For those operators that have the same number of rows from input and output  for example  Projection  we may don t need that Remove duplicated SQL metrics
Union Distinct has two Distinct that generate two Aggregation in the plan Remove an Extra Distinct in Union
This bug is reported by Stuti Awasthi  https   www mail archive com user spark apache org msg      html The lossSum has possibility of infinity because we do not standardize the feature before fitting model  we should support feature standardization  Another benefit is that standardization will improve the convergence rate AFTSurvivalRegression should support feature standardization
The current implementation of statistics of UnaryNode does not considering output  for example  Project   we should considering it to have a better guess Considering output for statistics of logical plan
An broadcasted table could be used multiple times in a query  we should cache them Avoid duplicated broadcasts
Currently the sbin  start stop  mesos dispatcher scripts only assume there is one mesos dispatcher launched  but potentially users that like to run multi tenant dispatcher might want to launch multiples  It also helps local development to have the ability to launch multiple ones Add support for launching multiple Mesos dispatchers
Support queries that JOIN tables with USING clause  SELECT   from table  JOIN table  USING Support USING clause in JOIN
I think model summary interface which is available in Spark s scala  Java and R interfaces should also be available in the python interface  Similar to  SPARK       https   issues apache org jira browse SPARK Expose ml summary function in PySpark for classification and regression models
Following SPARK        we can add a wrapper for naive Bayes in SparkR  R s naive Bayes implementation is from package e     with signature     It should be easy for us to match the parameters Naive Bayes wrapper in SparkR
TaskContext supports task completion callback  which gets called regardless of task failures  However  there is no way for the listener to know if there is an error  This ticket proposes adding a new listener that gets called when a task fails Add a task failure listener to TaskContext
With column pruning rule in optimizer  we will introduce redundant project for some cases  We should prevent it Remove redundant project in colum pruning rule
As part of Spark      we want to create a stable API foundation for Dataset to become the main user facing API in Spark  This ticket tracks various tasks related to that  The main high level changes are     Merge Dataset DataFrame    Create a more natural entry point for Dataset  SQLContext HiveContext are not ideal because of the name  SQL   Hive   and  SparkContext  is not ideal because of its heavy dependency on RDDs     First class support for sessions    First class support for some system catalog See the design doc for more details Dataset oriented  API evolution in Spark
After SPARK        we should add Python API for MaxAbsScaler Python API for MaxAbsScaler
This is just a clean up task  Today there are all these fields in SQLContext that are not organized in any particular way  However  since each SQLContext is a session  many of these fields are actually isolated per session  To minimize the size of these context files and provide a logical grouping that makes more sense  I propose that we move these fields into its own class  called SessionState Refactor  Move SQLContext HiveContext per session state to separate class
Remove all the deterministic conditions in a   Filter   that are contained in the Child Prune Filters based on Constraints
We already have internal APIs for Hive to do this  We should do it for SQLContext too so we can merge these code paths one day Track current database in SQL HiveContext
After SPARK        we should add Python API for generalized linear regression Python API for GeneralizedLinearRegression
Broadcast left semi join without joining keys is already supported in BroadcastNestedLoopJoin  it has the same implementation as LeftSemiJoinBNL  we should remove that Remove LeftSemiJoinBNL
We need to submit another PR against Spark to call the task failure callbacks before Spark calls the close function on various output streams  For example  we need to intercept an exception and call TaskContext markTaskFailed before calling close in the following   Changes to Spark should include unit tests to make sure this always work in the future Invoke task failure callbacks before calling outputstream close
In preparation for larger refactorings  I think that we should remove the confusing returnValues   option from the BlockStore put   APIs  returning the value is only useful in one place  caching  and in other situations  such as block replication  it s simpler to put   and then get Remove returnValues from BlockStore APIs
A majority of Spark SQL queries likely run though   HadoopFSRelation    however there are currently several complexity and performance problems with this path Simplify and Speedup HadoopFSRelation
Right now  we use PhysicalRDD for both existing RDD and data sources  they are becoming much different  we should use different physical plans for them Use different physical plan for existing RDD and data sources
 QueryPlan expressions should always include all expressions
When a cached block is spilled to disk and read back in serialized form  i e  as bytes   the current BlockManager implementation will attempt to re insert the serialized block into the MemoryStore even if the block s storage level requests deserialized caching  This behavior adds some complexity to the MemoryStore but I don t think it offers many performance benefits and I d like to remove it in order to simplify a larger refactoring patch  Therefore  I propose to change the behavior such that disk store reads will only cache bytes in the memory store for blocks with serialized storage levels  There are two places where we request serialized bytes from the BlockStore     getLocalBytes    which is only called when reading local copies of TorrentBroadcast pieces  Broadcast pieces are always cached using a serialized storage level  so this won t lead to a mismatch in serialization forms if spilled bytes read from disk are cached as bytes in the memory store     the non shuffle block branch in getBlockData    which is only called by the NettyBlockRpcServer when responding to requests to read remote blocks  Caching the serialized bytes in memory will only benefit us if those cached bytes are read before they re evicted and the likelihood of that happening seems low since the frequency of remote reads of non broadcast cached blocks seems very low  Caching these bytes when they have a low probability of being read is bad if it risks the eviction of blocks which are cached in their expected serialized deserialized forms  since those blocks seem more likely to be read in local computation  Therefore  I think this is a safe change Don t cache MEMORY AND DISK blocks as bytes in memory store when reading spills
Today  both the MemoryStore and DiskStore implement a common BlockStore API  but I feel that this API is inappropriate because it abstracts away important distinctions between the behavior of these two stores  For instance  the disk store doesn t have a notion of storing deserialized objects  so it s confusing for it to expose object based APIs like putIterator   and getValues   instead of only exposing binary APIs and pushing the responsibilities of serialization and deserialization to the client  As part of a larger BlockManager interface cleanup  I d like to remove the BlockStore API and refine the MemoryStore and DiskStore interfaces to reflect more narrow sets of responsibilities for those components Remove BlockStore interface to more cleanly reflect different memory and disk store responsibilities
projectList is useless  Remove it from the class Window  It simplifies the codes in Analyzer and Optimizer Remove projectList from Windows
Push down the predicate through the Window operator  In this JIRA  predicates are pushed through Window if and only if the following conditions are satisfied    Predicate involves one and only one column that is part of window partitioning key   Window partitioning key is just a sequence of attributeReferences   i e   none of them is an expression    Predicate must be deterministic Predicate Push Down Through Window Operator
RandomSampler sample currently accepts iterator as input and output another iterator  This makes it inappropriate to use in wholestage codegen of Sampler operator  We should add non iterator interface to RandomSampler Add non iterator interface to RandomSampler
We are using  SELECT    as a dummy table  when the table is used for SQL statements in which a table reference is required  but the contents of the table are not important  For example     In this case  we will see a useless Project whose projectList is empty after executing ColumnPruning rule Remove Project when its projectList is Empty
If the Window does not have any window expression  it is useless  It might happen after column pruning Eliminate Unnecessary Window
When generated code accesses a   ColumnarBatch   object  it is possible to get values of each column from   ColumnVector   instead of calling   getRow Direct consume ColumnVector in generated code when ColumnarBatch is used
When reading data from the DiskStore and attempting to cache it back into the memory store  we should guard against race conditions where multiple readers are attempting to re cache the same block in memory Guard against race condition when re caching spilled bytes in memory
When we generate code for join  we copy the output row  because there could be multiple output row from single input row  We could avoid this copy when there is no join  or the join will not generate multiple output rows from single input row Avoid the copy in whole stage codegen when there is no joins
We introduced some local operators in org apache spark sql execution local package but never fully wired the engine to actually use these  We still plan to implement a full local mode  but it s probably going to be fairly different from what the current iterator based local mode would look like  Let s just remove them for now  and we can always re introduced them in the future by looking at branch Remove org apache spark sql execution local
DescribeCommand should just take a TableIdentifier  and ask the metadata catalog for table s information Remove DescribeCommand s dependency on LogicalPlan
In general it is better for internal classes to not depend on the external class  in this case SQLContext  to reduce coupling between user facing APIs and the internal implementations Remove some internal classes  dependency on SQLContext
Our code can go through SessionState catalog  This brings two small benefits     Reduces internal dependency on SQLContext     Removes another public method in Java  Java does not obey package private visibility   More importantly  according to the design in SPARK        we d need to claim this catalog function for the user facing public functions  rather than having an internal field Remove SQLContext catalog  internal method
Currently Spark allows only a few cluster managers viz Yarn  Mesos and Standalone  But  as Spark is now being used in newer and different use cases  there is a need for allowing other cluster managers to manage spark components  One such use case is   embedding spark components like executor and driver inside another process which may be a datastore  This allows colocation of data and processing  Another requirement that stems from such a use case is that the executors driver should not take the parent process down when they go down and the components can be relaunched inside the same process again  So  this JIRA requests two functionalities     Support for external cluster managers    Allow a cluster manager to clean up the tasks without taking the parent process down Add support for pluggable cluster manager
Instead of storing serialized blocks in individual ByteBuffers  the BlockManager should be capable of storing a serialized block in multiple chunks  each occupying a separate ByteBuffer  This change will help to improve the efficiency of memory allocation and the accuracy of memory accounting when serializing blocks  Our current serialization code uses a   ByteBufferOutputStream    which doubles and re allocates its backing byte array  this increases the peak memory requirements during serialization  since we need to hold extra memory while expanding the array   In addition  we currently don t account for the extra wasted space at the end of the ByteBuffer s backing array  so a     megabyte serialized block may actually consume     megabytes of memory  After switching to storing blocks in multiple chunks  we ll be able to efficiently trim the backing buffers so that no space is wasted  This change is also a prerequisite to being able to cache blocks which are larger than  GB  although full support for that depends on several other changes which have not bee implemented yet Store serialized blocks as multiple chunks in MemoryStore
This continues the work of SPARK        SPARK       and SPARK       to expose R like model summary in more family and link functions Expose R like summary statistics in SparkR  glm for more family and link functions
Because ClassTags are available when constructing ShuffledRDD we can use them to automatically use Kryo for shuffle serialization when the RDD s types are guaranteed to be compatible with Kryo  e g  RDDs whose key  value  and or combiner types are primitives  arrays of primitives  or strings   This is likely to result in a large performance gain for many RDD API workloads Automatically use Kryo serializer when shuffling RDDs with simple types
Logging was made private in Spark      If we move it  then users would be able to create a Logging trait themselves to avoid changing their own code  Alternatively  we can also provide in a compatibility package that adds logging Move org apache spark Logging into org apache spark internal Logging
Recently the fast serialization has been introduced to collecting DataFrame Dataset  The same technology can be used on collect limit operator too Apply fast serialization on collect limit
Separate out linear algebra as a standalone module without Spark dependency to simplify production deployment  We can call the new module mllib local  which might contain local models in the future  The major issue is to remove dependencies on user defined types  The package name will be changed from mllib to ml  For example  Vector will be changed from  org apache spark mllib linalg Vector  to  org apache spark ml linalg Vector   The return vector type in the new ML pipeline will be the one in ML package  however  the existing mllib code will not be touched  As a result  this will potentially break the API  Also  when the vector is loaded from mllib vector by Spark SQL  the vector will automatically converted into the one in ml package Separate out local linear algebra as a standalone module without Spark dependency
This is to support order by position in SQL  e g     This should be controlled by config option spark sql groupByOrdinal Support group by ordinal in SQL
When a block is persisted in the MemoryStore at a serialized storage level  the current MemoryStore putIterator   code will unroll the entire iterator as Java objects in memory  then will turn around and serialize an iterator obtained from the unrolled array  This is inefficient and doubles our peak memory requirements  Instead  I think that we should incrementally serialize blocks while unrolling them  A downside to incremental serialization is the fact that we will need to deserialize the partially unrolled data in case there is not enough space to unroll the block and the block cannot be dropped to disk  However  I m hoping that the memory efficiency improvements will outweigh any performance losses as a result of extra serialization in that hopefully rare case Incrementally serialize blocks while unrolling them in MemoryStore
We should add support for caching serialized data off heap within the same process  i e  using direct buffers or sun misc unsafe   I ll expand this JIRA later with more detail  filing now as a placeholder Add support for off heap caching
Per   nongli  s suggestions  We should do these things     Remove the non vectorized parquet reader code     Support the remaining types  just big decimals      Move the logic to determine if our parquet reader can be used to planning  Only complex types should fall back to the parquet mr reader Cleanup Extend the Vectorized Parquet Reader
When SortOrder does not contain any reference  it has no effect on the sorting  Remove the noop SortOrder in Optimizer Remove noop SortOrder in Sort
 make SubqueryHolder an inner class
Per our discussion on the mailing list  please see  here http   mail archives apache org mod mbox  spark dev        mbox   CCA g  F aVRBH WyyK nvBSLCMPtSdUuL Ge  WW DnmnvY SXg mail gmail com  E   it would be nice to specify a custom coalescing policy as the current   coalesce     method only allows the user to specify the number of partitions and we cannot really control much  The need for this feature popped up when I wanted to merge small files by coalescing them by size Add support for custom coalescers
Currently  spark history server REST API provides functionality to query applications by application start time range based on minDate and maxDate query parameters  but it lacks support to query applications by their end time  In this Jira we are proposing optional minEndDate and maxEndDate query parameters and filtering capability based on these parameters to spark history server REST API  This functionality can be used for following queries     Applications finished in last  x  minutes    Applications finished before  y  time    Applications finished between  x  time to  y  time    Applications started from  x  time and finished before  y  time  For backward compatibility  we can keep existing minDate and maxDate query parameters as they are and they can continue support filtering based on start time range Add functionality in spark history sever API to query applications by end time
Currently  for the key that can not fit within a long  we build a hash map for UnsafeHashedRelation  it s converted to BytesToBytesMap after serialization and deserialization  We should build a BytesToBytesMap directly to have better memory efficiency Build BytesToBytesMap in HashedRelation
They were kept in SQLContext implicits object for binary backward compatibility  in the Spark   x series  It makes more sense for this API to be in SQLImplicits since that s the single class that defines all the SQL implicits Move StringToColumn implicit class into SQLImplicits
 Improve SparkStatusTracker to also track executor information
It would be nice to refactor the MemoryStore so that it can be unit tested without constructing a full BlockManager or needing to mock tons of things Refactor MemoryStore to be testable independent of BlockManager
In naive Bayes  we expect inputs to be individual observations  In practice  people may have the frequency table instead  It is useful for us to support instance weights to handle this case Support weighted instances in naive Bayes
We only parse create function command  In order to support native drop function command  we need to parse it too Parse Drop Function DDL command
remove consumeChild    always create code for UnsafeRow and variables Simplify whole stage codegen interface
 remove trait Queryable
 Execute multiple Python UDFs in single batch
Currently a method   submitStage     for waiting stages is called on every iteration of the event loop in   DAGScheduler   to submit all waiting stages  but most of them are not necessary because they are not related to Stage status  The case we should try to submit waiting stages is only when their parent stages are successfully completed  This elimination can improve   DAGScheduler   performance Eliminate unnecessary submitStage   call
 Reimplement TypedAggregateExpression to DeclarativeAggregate
While running a Spark job which is spilling a lot of data in reduce phase  we see that significant amount of CPU is being consumed in native Snappy ArrayCopy method  Please see the stack trace below   Stack trace   org xerial snappy SnappyNative   YJP  arrayCopy Native Method  org xerial snappy SnappyNative arrayCopy SnappyNative java  org xerial snappy Snappy arrayCopy Snappy java     org xerial snappy SnappyInputStream rawRead SnappyInputStream java      org xerial snappy SnappyInputStream read SnappyInputStream java      java io DataInputStream readFully DataInputStream java      java io DataInputStream readLong DataInputStream java      org apache spark util collection unsafe sort UnsafeSorterSpillReader loadNext UnsafeSorterSpillReader java     org apache spark util collection unsafe sort UnsafeSorterSpillMerger   loadNext UnsafeSorterSpillMerger java     org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      org apache spark sql execution UnsafeExternalRowSorter   next UnsafeExternalRowSorter java      The reason for that is the SpillReader does a lot of small reads from the underlying snappy compressed stream and SnappyInputStream invokes native jni ArrayCopy method to copy the data  which is expensive  We should fix Snappy  java to use with non JNI based System arrayCopy method in this case Significant amount of CPU is being consumed in SnappyNative arrayCopy method
We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r Refactor k means code in SparkRWrappers
We use a single object  SparkRWrappers   https   github com apache spark blob master mllib src main scala org apache spark ml r SparkRWrappers scala  to wrap method calls to glm and kmeans in SparkR  This is quite hard to maintain  We should refactor them into separate wrappers  like  AFTSurvivalRegressionWrapper  and  NaiveBayesWrapper   The package name should be  spakr ml r  instead of  spark ml api r Refactor GLMs code in SparkRWrappers
The toLocalIterator of RDD is super slow  we should have a optimized implementation for Dataset DataFrame Add toLocalIterator for Dataset
 Decouple deserializer expression resolution from ObjectOperator
The time windowing function  window  was added to Datasets  This JIRA is to track the status for the R  Python and SQL API Dateset Time Windowing API for Python  R  and SQL
There is no unit test for KMeansSummary in spark ml  Other items which could be fixed here    Add Since version to KMeansSummary class   Modify clusterSizes method to match GMM method  to be robust to empty clusters  in case we support that sometime   See PR for  SPARK Unit test for spark ml KMeansSummary
Right now  operations for an existing functions in SessionCatalog do not really check if the function exists  We should add this check and avoid of doing the check in command SessionCatalog needs to check function existence
Currently  many functions do now show usages like the followings     The only exceptions are  cube    grouping    grouping id    rollup    window All functions should show usages by command  DESC FUNCTION
We have ParserUtils and ParseUtils which are both utility collections for use during the parsing process  Those name and what they are used for is very similar so I think we can merge them  Also  the original unescapeSQLString method may have a fault  When   u      style character literals are passed to the method  it s not unescaped successfully Merge ParserUtils and ParseUtils
In order to upgrade to Kryo    we need to shade Kryo in our custom Hive       fork Shade Kryo in our custom Hive       fork
 refactor object operator framework to make it easy to eliminate serializations
In Spark       DataFrame  is an alias of  Dataset Row    MLlib API actually works for other types of  Dataset   so we should accept  Dataset     instead  It maps to  Dataset  in Java  This is a source compatible change Accept Dataset    instead of DataFrame in MLlib APIs
It will be great to have these SQL functions  IFNULL  NULLIF  NVL  NVL  The meaning of these functions could be found in oracle docs SQL function  IFNULL  NULLIF  NVL and NVL
Currently  Union only takes intersect of the constraints from it s children  all others are dropped  we should try to merge them together Improve constraints propagation in Union
We currently disable codegen for CaseWhen if the number of branches is greater than     in CaseWhen MAX NUM CASES FOR CODEGEN   It would be better if this value is a non public config defined in SQLConf spark sql codegen maxCaseBranches config option
Right now  filter push down only works with Project  Aggregate  Generate and Join  they can t be pushed through many other plans Improve filter push down
When you have an error in your R code using the RDD API  you always get as error message  Error in if  returnStatus           argument is of length zero This is not very useful and I think it might be better to catch the R exception and show it instead Improve error messages for RDD API
This issue aims to add  bound  function  aka Banker s round  by extending current  round  implementation  Hive supports  bround  since         Language Manual https   cwiki apache org confluence display Hive LanguageManual UDF Add  bround  function
The goal of the ticket is to simplify both the external interface and the internal implementation for accumulators and metrics  They are unnecessarily convoluted and we should be able to simplify them quite a bit  This is an umbrella ticket and I will iteratively create new tasks as my investigation goes on  At a high level  I d would like to create better abstractions for internal implementations  as well as creating a simplified accumulator v  external interface that doesn t involve a complex type hierarchy Simplify accumulators and task metrics
This issue aims to expose Scala  bround  function in Python R API   bround  function is implemented in SPARK       by extending current  round  function  We used the following semantics from  Hive https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic RoundUtils java Add  bround  function in Python R
This issue aims to implement  assert true  function  It s  Hive Generic UDF Function https   github com apache hive blob master ql src java org apache hadoop hive ql udf generic GenericUDFAssertTrue java  since      The following is function description of Hive         Please note that Hive s false values are  false         null   and       empty string    But  in Spark  we intentionally designed to use implicit typecasting to BooleanType Add  assert true  function
Provide API for SVM algorithm for DataFrames  I would recommend using OWL QN  rather than wrapping spark mllib s SGD based implementation  The API should mimic existing spark ml classification APIs spark ml API for linear SVM
 Rename upstreams      inputRDDs   in WholeStageCodegen
The current benchmark framework runs a code block for several iterations and reports statistics  However there is no way to exclude per iteration setup time from the overall results Allow custom timing control in microbenchmarks
In Spark     we shouldn t have two parsers anymore  There should be only a single one Merge HiveSqlAstBuilder and SparkSqlAstBuilder
This is an umbrella ticket to reduce the difference between sql core and sql hive  Ultimately the only difference should just be the ability to run Hive serdes as well as UDFs Merge functionality in Hive module into SQL core module
In current master  we have   ML methods in SparkR     If we make this change  we might want to avoid name collisions because they have different signature  We can use  ml kmeans    ml glm   etc  Sorry for discussing API changes in the last minute  But I think it would be better to have consistent signatures in SparkR  cc    shivaram    josephkb    yanboliang Make ML APIs in SparkR consistent
In SPARK       we switched to use GenericArrayData to store indices and values in vector matrix UDTs  However  GenericArrayData is not specialized for primitive types  This might hurt MLlib performance badly  We should consider either specialize GenericArrayData or use a different container  cc    cloud fan    yhuai VectorUDT MatrixUDT should take primitive arrays without boxing
 Break SQLQuerySuite out into smaller test suites
It looks like the head master branch of Spark uses quite an old version of Jetty         v         There have been some announcement of security vulnerabilities  notably in      and there are versions of both   and   that address those  We recently left a web ui port open and had the server compromised within days  Albeit  this upgrade shouldn t be the only security improvement made  the current version is clearly vulnerable  as is Upgrade Jetty to latest version of
Since  SPARK        breaks behavior of HashingTF  we should try to enforce good practice by removing the  native  hashingAlg option in spark ml and pyspark ml  We can leave spark mllib and pyspark mllib alone Remove spark ml HashingTF hashingAlg option
We currently have no way for users to propagate options to the underlying library that rely in Hadoop configurations to work  For example  there are various options in parquet mr that users might want to set  but the data source API does not expose a per job way to set it  This patch propagates the user specified options also into Hadoop Configuration Propagate data source options to Hadoop configurations
This issues aims to add new FoldablePropagation optimizer that propagates foldable expressions by replacing all attributes with the aliases of original foldable expression  Other optimizations will take advantage of the propagated foldable expressions  e g  EliminateSorts optimizer now can handle the following Case   and     Case   is the previous implementation      Literals and foldable expression  e g   ORDER BY       abc   Now       Foldable ordinals  e g   SELECT       abc   Now   ORDER BY             Foldable aliases  e g   SELECT     x   abc  y  Now   z ORDER BY x  y  z    Before Add FoldablePropagation optimizer
Currently wholestage codegen version of TungstenAggregate does not support subexpression elimination  We should support it Subexpression elimination in wholestage codegen version of TungstenAggregate
The YarnShuffleService  currently just picks a directly in the yarn local dirs to store the leveldb file  YARN added an interface in hadoop     getRecoverPath   to get the location where it should be storing this  We should change to use getRecoveryPath    This does mean we will have to use reflection or similar to check for its existence though since it doesn t exist before hadoop YarnShuffleService should use YARN getRecoveryPath   for leveldb location
JSON schema inference spends a lot of time in inferField and there are a number of techniques to speed it up  including eliminating unnecessary sorting and the use of inefficient collections Improve performance of JSON schema inference s inferField step
validationMetrics in TrainValidationSplitModel should also be supported in pyspark ml tuning PySpark TrainValidationSplitModel should support validationMetrics
In SparkR  spark kmeans take a DataFrame with double columns  This is different from other ML methods we implemented  which support R model formula  We should add support for that as well Support formula in spark kmeans in SparkR
Implement repartitionByColumn on DataFrame  This will allow us to run R functions on each partition identified by column groups with dapply   method SparkR   Implement repartitionByColumn on DataFrame
Our current dataset registerTempTable does not actually materialize data  So  it should be considered as creating a temp view  We can deprecate it and create a new method called dataset createTempView replaceIfExists  Boolean   The default value of replaceIfExists should be false  For registerTempTable  it will call dataset createTempView replaceIfExists   true Deprecate registerTempTable and add dataset createTempView
LazyFileRegion was created so we didn t create a file descriptor before having to send the file  see https   issues apache org jira browse SPARK       The change has been pushed back into Netty to support the same things under the DefaultFileRegion  https   github com netty netty issues      https   github com netty netty commit a    b  d c   f b f  e      b    a    be It looks like that went into        Final  I believe at the time we created LazyFileRegion we were on        Final and we are now using        Final so we should be able to use the netty class directly Remove LazyFileRegion
We should add an interface to the GLR summaries in Python for feature parity Python API for Generalized Linear Regression Summary
Generate   currently does not support code generation  Lets add support for CG and for it and its most important generators    explode   and   json tuple Implement code generation for Generate
SPARK       makes KMeansModel store the clusters one per row  KMeansModel load   method needs to be updated in order to load models saved with Spark Make spark ml KMeansModel load backwards compatible
Now picklers for both new and old vectors are implemented under PythonMLlibAPI  To separate spark mllib from spark ml  we should implement them under  spark ml python  instead  I set the target to     since those are private APIs Implement Python picklers for ml Vector and ml Matrix under spark ml python
Since ml evaluation has supported save load at Scala side  supporting it at Python side is very straightforward and easy PySpark ml evaluation should support save load
This is the parent JIRA to track all the work for the building a Kafka source for Structured Streaming  Here is the design doc for an initial version of the Kafka Source  https   docs google com document d   t rWe  x tq e AOfrsM qb  m BRuv fel i PqR  edit usp sharing                    Old description                           Structured streaming doesn t have support for kafka yet  I personally feel like time based indexing would make for a much better interface  but it s been pushed back to kafka        https   cwiki apache org confluence display KAFKA KIP      Add a time based log index Structured streaming support for consuming from Kafka
We re using  asML  to convert the mllib vector matrix to ml vector matrix now  Using  as  is more correct given that this conversion actually shares the same underline data structure  As a result  in this PR   toBreeze  will be changed to  asBreeze   This is a private API  as a result  it will not affect any user s application Change  toBreeze  to  asBreeze  in Vector and Matrix
We should open up the APIs for converting between new  old linear algebra types  in spark mllib linalg     Vector asML   Vectors fromML   same for Sparse Dense and for Matrices I made these private originally  but they will be useful for users transitioning workloads Make the mllib ml linalg type conversion APIs public
We removed some classes in Spark      If the user uses an incompatible library  he may see ClassNotFoundException  It s better to give an instruction to ask people using a correct version Display a better message for not finding classes removed in Spark
This is an umbrella ticket to list issues I found with APIs for the     release Spark     SQL API audit
Several classes and methods have been deprecated and are creating lots of build warnings in branch      This issue is to identify and fix those items     WithSGD classes  Change to make class not deprecated  object deprecated  and public class constructor deprecated  Any public use will require a deprecated API  We need to keep a non deprecated private API since we cannot eliminate certain uses  Python API  streaming algs  and examples     Use in PythonMLlibAPI  Change to using private constructors    Streaming algs  No warnings after we un deprecate the classes    Examples  Deprecate or change ones which use deprecated APIs   MulticlassMetrics fields  precision  etc Eliminate MLlib     build warnings from deprecations
Support for partitioned  parquet  format in FileStreamSink was added in Spark        now let s add support for partitioned  csv    json    text  format Add support for writing partitioned  csv    json    text  formats in Structured Streaming
See containing JIRA for details   SPARK ML     QA  Scala APIs audit for evaluation  tuning
See containing JIRA for details   SPARK ML     QA  Scala APIs audit for feature
This issue replaces all deprecated  SQLContext  occurrences with  SparkSession  in  ML MLLib  module except the following two classes  These two classes use  SQLContext  as their function arguments    ReadWrite scala   TreeModels scala Replace SQLContext with SparkSession in ML MLLib
See parent task SPARK          bryanc  did this component python coverage ml feature
See parent task SPARK python converage ml recommendation module
See parent task SPARK python converage ml classification module
See parent task SPARK python converage ml regression module
See parent task SPARK python converage pyspark ml linalg
Use the latest Sparksession to replace the existing SQLContext in MLlib Replace SQLContext with SparkSession in MLlib
Making DefaultParamsWritable and DefaultParamsReadable public will help users who have their own transformers save and load Pipelines  Making them public should be safe  even if we change internal formats Make DefaultParamsReadable Writable public APIs
DataFrameWriter insertInto includes some Analyzer stuff  We should move it to Analyzer Move some Analyzer stuff to Analyzer from DataFrameWriter
There s no corresponding python api for KMeansSummary  it would be nice to have it Add KMeanSummary in KMeans of PySpark
Spark s SBT build currently uses a fork of the sbt pom reader plugin but depends on that fork via a SBT subproject which is cloned from https   github com scrapcodes sbt pom reader tree ignore artifact id  This unnecessarily slows down the initial build on fresh machines and is also risky because it risks a build breakage in case that GitHub repository ever changes or is deleted  In order to address these issues  I propose to publish a pre built binary of our forked sbt pom reader plugin to Maven Central under the org spark project namespace Publish Spark s forked sbt pom reader to Maven Central
We should expose codahale metrics for the codegen source text size and how long it takes to compile  The size is particularly interesting  since the JVM does have hard limits on how large methods can get Metrics for codegen size and perf
Sometimes it doesn t make sense to specify partitioning parameters  e g  when we write data out from Datasets DataFrames into jdbc tables or streaming   ForeachWriters    We probably should add checks against this in   DataFrameWriter Add assertNotPartitioned check in DataFrameWriter
Right now  Spark     does not load hive site xml  Based on users  feedback  it seems make sense to still load this conf file  Originally  this file was loaded when we load HiveConf class and all settings can be retrieved after we create a HiveConf instances  Let s avoid of using this way to load hive site xml  Instead  since hive site xml is a normal hadoop conf file  we can first find its url using the classloader and then use Hadoop Configuration s addResource  or add hive site xml as a default resource through Configuration addDefaultResource  to load confs  Please note that hive site xml needs to be loaded into the hadoop conf used to create metadataHive Bring back the hive site xml support for Spark
This is for API parity of Scala API  Refer to https   issues apache org jira browse SPARK Add varargs type dropDuplicates   function in SparkR
It would be good to have an additional implementation  which uses dense format  for   UnsafeArrayData   to reduce memory footprint  Current   UnsafeArrayData   implementation uses only a sparse format  It is useful for an   UnsafeArrayData   that is created by a method   fromPrimitiveArray    which have no   null   value Introduce additonal implementation with a dense format for UnsafeArrayData
Interface method   FileFormat prepareRead     was added in  PR        https   github com apache spark pull        to handle a special case in the LibSVM data source  However  the semantics of this interface method isn t intuitive  it returns a modified version of the data source options map  Considering that the LibSVM case can be easily handled using schema metadata inside   inferSchema    we can remove this interface method to keep the   FileFormat   interface clean Remove FileFormat prepareRead
Add a new API method called gapplyCollect   for SparkDataFrame  It does gapply on a SparkDataFrame and collect the result back to R  Compared to gapply     collect    gapplyCollect   offers performance optimization as well as programming convenience  as no schema is needed to be provided  This is similar to dapplyCollect add gapplyCollect   for SparkDataFrame
This is a debug only version of SPARK        for tutorials and debugging of streaming apps  it would be nice to have a text based socket source similar to the one in Spark Streaming  It will clearly be marked as debug only so that users don t try to run it in production applications  because this type of source cannot provide HA without storing a lot of state in Spark Add debug only socket source in Structured Streaming
There is a ToDo of GenericArrayData class  which is to eliminate boxing unboxing for a primitive array  described  here https   github com apache spark blob master sql catalyst src main scala org apache spark sql catalyst util GenericArrayData scala L     It would be good to prepare GenericArrayData implementation specialized for a primitive array to eliminate boxing unboxing from the view of runtime memory footprint and performance Prepare GenericArrayData implementation specialized for a primitive array
This issue adds  read orc write orc  to SparkR for API parity Add  read orc write orc  to SparkR
Both VectorUDT and MatrixUDT are private APIs  because UserDefinedType itself is private in Spark  However  in order to let developers implement their own transformers and estimators  we should expose both types in a public API to simply the implementation of transformSchema  transform  etc  Otherwise  they need to get the data types using reflection  Note that this doesn t mean to expose VectorUDT MatrixUDT classes  We can just have a method or a static value that returns VectorUDT MatrixUDT instance with DataType as the return type  There are two ways to implement this     following DataTypes java in SQL  so Java users doesn t need the extra          Define DataTypes in Scala Expose VectorUDT MatrixUDT in a public API
We need hashCode and euqals in UnsafeMapData because of the      behaivour of UnsafeMapData is different from that of ArrayBasedMapData Remove hashCode and euqals in ArrayBasedMapData
In     and earlier releases  we have package grouping in the generated Java API docs  See http   spark apache org docs       api java index html  However  this disappeared in        http   spark apache org docs       api java index html  Rather than fixing it  I d suggest removing grouping  Because it might take some time to fix and it is a manual process to update the grouping in SparkBuild scala  No one complained about missing groups since Remove package grouping in genjavadoc
We embed partitioning logic in FileSourceStrategy apply  making the function very long  This is a small refactoring to move it into its own functions  Eventually we would be able to move the partitioning functions into a physical operator  rather than doing it in physical planning Move RDD creation logic from FileSourceStrategy apply
CollectSet  cannot have map typed data because MapTypeData does not implement  equals   So  if we find map type in  CollectSet   queries fail Improve the type check of CollectSet in CheckAnalysis
This patch removes the blind fallback into Hive for functions  Instead  it creates a whitelist and adds only a small number of functions to the whitelist  i e  the ones we intend to support in the long run in Spark Whitelist the list of Hive fallback functions
There are some duplicated code for options  we should simplify them Cleanup options for DataFrame reader API in Python
Spark SQL currently falls back to Hive for xpath related functions Implement xpath user defined functions
Currently our Optimizer may reorder the predicates to run them more efficient  but in non deterministic condition  change the order between deterministic parts and non deterministic parts may change the number of input rows  For example     may call rand   for different times and therefore the output rows differ Improve the PushDownPredicate rule to pushdown predicates currectly in non deterministic condition
LogicalPlan  InsertIntoHiveTable  is useless  Thus  we can remove it from the code base Remove InsertIntoHiveTable From Logical Plan
Currently  there are a few reports about Spark     query performance regression for large queries  This issue speeds up SQL query processing performance by removing redundant consecutive  executePlan  call in  Dataset ofRows  function and  Dataset  instantiation  Specifically  this issue aims to reduce the overhead of SQL query execution plan generation  not real query execution  So  we can not see the result in the Spark Web UI  Please use the following query script    Before Speed up SQL query performance by removing redundant  executePlan  call in  Dataset
Different from the other leaf nodes   MetastoreRelation  and  SimpleCatalogRelation  have a pre defined  alias   which is used to change the qualifier of the node  However  based on the existing alias handling  alias should be put in  SubqueryAlias   This PR is to separate alias handling from  MetastoreRelation  and  SimpleCatalogRelation  to make it consistent with the other nodes  For example  below is an example query for  MetastoreRelation   which is converted to  LogicalRelation        Note  the optimized plans are the same    For  SimpleCatalogRelation   the existing code always generates two Subqueries  Thus  no change is needed Remove Alias from MetastoreRelation and SimpleCatalogRelation
When users try to implement a data source API with extending only RelationProvider and CreatableRelationProvider  they will hit an error when resolving the relation Data Source APIs  Extending RelationProvider and CreatableRelationProvider Without SchemaRelationProvider
ExternalShuffleService is essential for spark  In order to better monitor shuffle service  we added various metrics in shuffle service and ExternalShuffleServiceSource for metric system Add metrics and source for external shuffle service
It seems  EqualNullSafe  filter was missed for batch pruneing partitions in cached tables  Supporting this improve the performance roughly       it will vary   Running the codes below Support partition batch pruning with     EqualNullSafe  predicate in InMemoryTableScanExec
 move hive hack for data source table into HiveExternalCatalog
Currently filters for   TimestampType   and   DecimalType   are not being pushed down in ORC data source although ORC filters support both Support for pushing down filters for decimal and timestamp types in ORC
Dataframe drop supported multi columns in spark api and should make python api also support it Dataframe drop supported multi columns in spark api and should make python api also support it
 Move regexp unit tests to RegexpExpressionsSuite
Elt function doesn t support codegen execution  It is better to provide the support Add codegen for Elt function
This is similar with https   issues apache org jira browse SPARK        Currently    JdbcUtils savePartition   is doing type based dispatch for each row to write appropriate values  So  appropriate writers can be created first according to the schema  and then apply them to each row  This approach is similar with   CatalystWriteSupport Avoid per record type dispatch in JDBC when writing
 move BucketSpec to catalyst module and use it in CatalogTable
 use StructType in CatalogTable and remove CatalogColumn
Some codes in subexpressionEliminationForWholeStageCodegen are never used actually  Remove them using this jira Remove unused codes in subexpressionEliminationForWholeStageCodegen
This JIRA is to upgrade the derby version from           to           Sean and I figured that we only use derby for tests and so the initial pull request was to not include it in the jars folder for Spark  I now believe it is required based on comments for the pull request and so this is only a dependency upgrade  The upgrade is due to an already disclosed vulnerability  CVE            in derby            We used https   www versioneye com search and will be checking for any other problems in a variety of libraries too  investigating if we can set up a Jenkins job to check our pom on a regular basis so we can stay ahead of the game for matters like this  This was raised on the mailing list at http   apache spark developers list         n  nabble com VOTE Release Apache Spark       RC  tp     p      html by Stephen Hellberg and replied to by Sean Owen  I ve checked the impact to previous Spark releases and this particular version of derby is the only relatively recent and without vulnerabilities version  I checked up to the     branch  so ideally we d backport this for all impacted Spark releases  I ve marked this as critical and ticked the important checkbox as it s going to impact every user  there isn t a security component  should we add one   and hence the build tag Upgrade derby to           from
SparkILoop getAddedJars is a useful method to use so we can programmatically get the list of jars added Open up SparkILoop getAddedJars
The catalyst package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql  and private spark  from catalyst package
 remove MaxOf and MinOf
Remove TestHiveSharedState  Otherwise  we are not really testing the reflection logic based on the setting of we are not really testing the reflection logic based on the setting of CATALOG IMPLEMENTATION Removal of TestHiveSharedState
Update LogisticCostAggregator serialization code to make it consistent with LinearRegression Update LogisticCostAggregator serialization code to make it consistent with LinearRegression
Function related  HiveExternalCatalog  APIs do not have enough verification logics  After the PR   HiveExternalCatalog  and  InMemoryCatalog  become consistent in the error handling  For example  below is the exception we got when calling  renameFunction Verification of Function related ExternalCatalog APIs
There could be same subquery within a single query  we could reuse the result without running it multiple times Reuse subqueries within single query
The execution package is meant to be internal  and as a result it does not make sense to mark things as private sql  or private spark   It simply makes debugging harder when Spark developers need to inspect the plans at runtime Remove private sql  and private spark  from sql execution package
CC   mgummelt    tnachen    skonto  I think this is fairly easy and would be beneficial as more work goes into Mesos  It should separate into a module like YARN does  just on principle really  but because it also means anyone that doesn t need Mesos support can build without it  I m entirely willing to take a shot at this Collect Mesos support code into a module profile
Sometimes we simply need to add a property in Spark Config for the Mesos Dispatcher  The only option right now is to created a property file Add   conf to mesos dispatcher process
After acquiring allocations from YARN and launching containers  Spark currently waits for   seconds for executors to connect to the driver  On Spark standalone  nothing like this happens  I m wondering whether we can just remove this sleep entirely  Is there a reason I m missing why YARN is different than standalone in this regard  At the least we could do something smarter like wait until all executors have registered Remove   second sleep before starting app on YARN
 Take advantage of AMRMClient APIs to simplify logic in YarnAllocationHandler
CreateHiveTableAsSelectLogicalPlan   is a dead code after refactoring Removal of useless CreateHiveTableAsSelectLogicalPlan
Inline tables currently do not support SQL generation  and as a result a view that depends on inline tables would fail Support SQL generation for inline tables
 Refactor R mllib for easier ml implementations
 remove catalog table type INDEX
Since  HiveClient  is used to interact with the Hive metastore  it should be hidden in  HiveExternalCatalog   After moving  HiveClient  into  HiveExternalCatalog    HiveSharedState  becomes a wrapper of  HiveExternalCatalog   Thus  removal of  HiveSharedState  becomes straightforward  After removal of  HiveSharedState   the reflection logic is directly applied on the choice of  ExternalCatalog  types  based on the configuration of  CATALOG IMPLEMENTATION    HiveClient  is also used invoked by the other entities besides HiveExternalCatalog  we defines the following two APIs Removal of HiveSharedState
Method  SQLContext parseDataType dataTypeString  String   could be removed  we should use  SparkSession parseDataType dataTypeString  String   instead  This require updating PySpark Method  SQLContext parseDataType dataTypeString  String   could be removed
Spark has configurable L  regularization parameter for generalized linear regression  It is very important to have them in SparkR so that users can run ridge regression SparkR spark glm should have configurable regularization parameter
 move CreateTables to HiveStrategies
Kolmogorov Smirnov Test is a popular nonparametric test of equality of distributions  There is implementation in MLlib  It will be nice if we can expose that in SparkR Add Kolmogorov Smirnov Test to SparkR
In publishing SparkR to CRAN  it would be nice to have a vignette as a user guide that   describes the big picture   introduces the use of various methods This is important for new users because they may not even know which method to look up Add package vignette to SparkR
This is another step to get rid of HiveClient from  HiveSessionState   All the metastore interactions should be through  ExternalCatalog  interface  However  the existing implementation of  InsertIntoHiveTable   still requires Hive clients  Thus  we can remove HiveClient by moving the metastore interactions into  ExternalCatalog Remove Direct Usage of HiveClient in InsertIntoHiveTable
It would be useful if more of JDBCRDD s JDBC    Spark SQL functionality was usable from outside of JDBCRDD  this would make it easier to write test harnesses comparing Spark output against other JDBC databases Refactor JDBCRDD to expose JDBC    SparkSQL conversion functionality
To regulate pending and running executors we determine the executors which are eligible to kill and kill them iteratively rather than a loop  This does an RPC call and is synchronized leading to lock contention for SparkListenerBus  Side effect   listener bus is blocked while we iteratively remove executors Kill multiple executors together to reduce lock contention
For building SparkR vignettes on Jenkins machines  we need the rmarkdown R  The package is available at https   cran r project org web packages rmarkdown index html   I think running something like Rscript  e  install packages  rmarkdown   repos  http   cran stat ucla edu     should work Install rmarkdown R package on Jenkins machines
Many users have requirements to use third party R packages in executors workers  but SparkR can not satisfy this requirements elegantly  For example  you should to mess with the IT administrators of the cluster to deploy these R packages on each executors workers node which is very inflexible  I think we should support third party R packages for SparkR users as what we do for jar packages in the following two scenarios     Users can install R packages from CRAN or custom CRAN like repository for each executors     Users can load their local R packages and install them on each executors  To achieve this goal  the first thing is to make SparkR executors support virtualenv like Python conda  I have investigated and found packrat http   rstudio github io packrat   is one of the candidates to support virtualenv for R  Packrat is a dependency management system for R and can isolate the dependent R packages in its own private package space  Then SparkR users can install third party packages in the application scope destroy after the application exit  and don t need to bother IT administrators to install these packages manually  I would like to know whether it make sense SparkR executors workers support virtualenv
The   TaskMetricsUIData updatedBlockStatuses   field is assigned to but never read  increasing the memory consumption of the web UI  We should remove this field Remove unused TaskMetricsUIData updatedBlockStatuses field
In logical plan    SerializeFromObject   for an array always use   GenericArrayData   as a destination    UnsafeArrayData   could be used for an primitive array  This is a simple approach to solve issues that are addressed by SPARK        Here is a motivating example Optimize SerializeFromObject for primitive array
Profiling a job  we saw that patten matching in wrap function of HiveInspector is consuming around     of the time which can be avoided  A similar change in the unwrap function was made in SPARK When wrapping catalyst datatype to Hive data type avoid pattern matching
We use multiple DStreams coming from different Kafka topics in a Streaming application  Some settings like maxrate and backpressure enabled disabled would be better passed as config to KafkaUtils createStream and KafkaUtils createDirectStream  instead of setting them in SparkConf  Being able to set a different maxrate for different streams is an important requirement for us  we currently work around the problem by using one receiver based stream and one direct stream  We would like to be able to turn on backpressure for only one of the streams as well Set Streaming MaxRate Independently For Multiple Streams
Query     Only one distinct should be necessary  This makes a bunch of unions slower than a bunch of union alls followed by a distinct Optimizer should remove unnecessary distincts  in multiple unions
There are   listLeafFiles related functions in Spark    ListingFileCatalog listLeafFiles  which calls HadoopFsRelation listLeafFilesInParallel if the number of paths passed in is greater than a threshold  if it is lower  then it has its own serial version implemented    HadoopFsRelation listLeafFiles  called only by HadoopFsRelation listLeafFilesInParallel    HadoopFsRelation listLeafFilesInParallel  called only by ListingFileCatalog listLeafFiles  It is actually very confusing and error prone because there are effectively two distinct implementations for the serial version of listing leaf files  This  into ListingFileCatalog  since it is the only class that needs this    Keep only one function for listing files in serial Consolidate various listLeafFiles implementations
Dataset always does eager analysis now  Thus  spark sql eagerAnalysis is not used any more  Thus  we need to remove it Remove spark sql eagerAnalysis
The current InternalRow hierarchy makes a difference between immutable and mutable rows  In practice we cannot guarantee that an immutable internal row is immutable  you can always pass a mutable object as an one of its elements   Lets make all internal rows mutable  and reduce the complexity Simplify InternalRow hierarchy
 Remove redundant Experimental annotations in sql streaming package
When I was creating the example code for SPARK        I realized it was pretty convoluted to define the frame boundaries for window functions when there is no partition column or ordering column  The reason is that we don t provide a way to create a WindowSpec directly with the frame boundaries  We can trivially improve this by adding rowsBetween and rangeBetween to Window object DataFrame API should simplify defining frame boundaries without partitioning ordering
ANSI SQL uses the following to specify the frame boundaries for window functions Improve window function frame boundary API in DataFrame
Classifier getNumClasses  can not support Non Double types  and classification algos relying on it do not support non double labelCol  like  NavieBayes   As suggested by   sethah   it is not a reasonable way to do datatype cast everywhere  And we can make cast only happen in  Predictor     yanboliang   josephkb   srowen Move LabelCol datatype cast into Predictor fit
We generate bitmasks for grouping sets during the parsing process  and use these during analysis  These bitmasks are difficult to work with in practice and have lead to numerous bugs  I suggest that we remove these and use actual sets instead  however we would need to generate these offsets for the grouping id Do not use bitmasks during parsing and analysis of CUBE ROLLUP GROUPING SETS
Code generation to get data from   ColumnVector   and   ColumnarBatch   is becoming pervasive  The  generation part as a trait for ease of reuse Refactor code generation to get data for ColumnVector ColumnarBatch
In the existing code  there are three layers of serialization involved in sending a task from the scheduler to an executor    A Task object is serialized   The Task object is copied to a byte buffer that also contains serialized information about any additional JARs  files  and Properties needed for the task to execute  This byte buffer is stored as the member variable serializedTask in the TaskDescription class    The TaskDescription is serialized  in addition to the serialized task   JARs  the TaskDescription class contains the task ID and other metadata  and sent in a LaunchTask message  While it is necessary to have two layers of serialization  so that the JAR  file  and Property info can be deserialized prior to deserializing the Task object  the third layer of deserialization is unnecessary  this is as a result of SPARK        We should eliminate a layer of serialization by moving the JARs  files  and Properties into the TaskDescription class taskScheduler has some unneeded serialization
The new Tungsten execution engine has very robust memory management and speed for simple data types  It does  however  suffer from the following    For user defined aggregates  Hive UDAFs  Dataset typed operators   it is fairly expensive to fit into the Tungsten internal format    For aggregate functions that require complex intermediate data structures  Unsafe  on raw bytes  is not a good programming abstraction due to the lack of structs  The idea here is to introduce a JVM object based hash aggregate operator that can support the aforementioned use cases  This operator  however  should limit its memory usage to avoid putting too much pressure on GC  e g  falling back to sort based aggregate as soon the number of objects exceeds a very low threshold  Internally at Databricks we prototyped a version of this for a customer POC and have observed substantial speed ups over existing Spark Introduce a JVM object based aggregate operator
There are known complaints cribs about History Server s Application List not updating quickly enough when the event log files that need replay are huge  Currently  the FsHistoryProvider design causes the entire event log file to be replayed when building the initial application listing  refer the method mergeApplicationListing fileStatus  FileStatus     The process of replay involves    each line in the event log being read as a string    parsing the string to a Json structure   converting the Json to the corresponding Scala classes with nested structures Particularly the part involving parsing string to Json and then to Scala classes is expensive  Tests show that majority of time spent in replay is in doing this work  When the replay is performed for building the application listing  the only two events that the code really cares for are  SparkListenerApplicationStart  and  SparkListenerApplicationEnd    since the only listener attached to the ReplayListenerBus at that point is the ApplicationEventListener  This means that when processing an event log file with a huge number  hundreds of thousands  can be more  of events  the work done to deserialize all of these event  and then replay them is not needed  Only two events are what we re interested in  and this can be used to ensure that when replay is performed for the purpose of building the application list  we only make the effort to replay these two events and not others  My tests show that this drastically improves application list load time  For a    MB event log from a user  with over         events  the load time  local on my mac  comes down from about    secs to under   second using this approach  For customers that typically execute applications with large event logs  and thus have multiple large event logs present  this can speed up how soon the history server UI lists the apps considerably  I will be updating a pull request with take at fixing this Remove unneeded heavy work performed by FsHistoryProvider for building up the application listing UI page
We should upgrade to the latest release of MiMa          in order to include my fix for a bug which led to flakiness in the MiMa checks  https   github com typesafehub migration manager issues Upgrade to MiMa
Whenever we aggregate data by event time  we want to consider data is late and out of order in terms of its event time  Since we keep aggregate keyed by the time as state  the state will grow unbounded if we keep around all old aggregates in an attempt consider arbitrarily late data  Since the state is a store in memory  we have to prevent building up of this unbounded state  Hence  we need a watermarking mechanism by which we will mark data that is older beyond a threshold as  too late   and stop updating the aggregates with them  This would allow us to remove old aggregates that are never going to be updated  thus bounding the size of the state  Here is the design doc   https   docs google com document d  z Pazs v rA  azvmYhu I xwqaNQl ZLIS  xhkfCQ edit usp sharing Observed delay based event time watermarks
Plan    Mark it very explicit in Spark       that support for the aforementioned environments are deprecated    Remove support it Spark       Also see mailing list discussion  http   apache spark developers list         n  nabble com Straw poll dropping support for things like Scala      tp     p      html More officially deprecate support for Python      Java    and Scala
Column expr is private sql   but it s an actually really useful field to have for debugging  We should open it up  similar to how we use QueryExecution Make Column expr public
hash scala was getting pretty long and it s not obvious that hash expressions belong there  Creating a hash scala to put all the hash expressions Move hash expressions from misc scala into hash scala
I think we have an undocumented naming convention to call expression unit tests ExpressionsSuite  and the end to end tests FunctionsSuite  It d be great to make all test suites consistent with this naming convention Use consistent naming for expression test suites
As of today  I could not access rdd localCheckpoint   in pyspark  This is an important issue for machine learning people  as we often have to iterate algorithms and perform operations like joins in each iteration  If the lineage is not truncated  the memory usage  the lineage  and computation time explode  rdd localCheckpoint   seems like the most straightforward way of truncating the lineage  but the python API does not expose it Expose RDD localCheckpoint in PySpark
Spark s CSVFileFormat data source uses inefficient methods for reading files during schema inference and does not benefit from file listing   IO performance improvements made in Spark      In order to fix this performance problem  we should re implement those read paths in terms of TextFileFormat Use TextFileFormat in implementation of CSVFileFormat
Refactor   StaticInvoke      Invoke   and   NewInstance   as    Introduce   InvokeLike   to extract common logic from   StaticInvoke      Invoke   and   NewInstance   to prepare arguments    Remove unneeded null checking and fix nullability of   NewInstance      Modify to short circuit if arguments have   null   when   propageteNull    true Refactor StaticInvoke  Invoke and NewInstance
These two methods were added to Scala Datasets  but are not available in Python yet Add withWatermark and checkpoint to python dataframe
trying to monitoring our streaming application using Spark REST interface and found out that there is no api for streaming  it let us no choice but to implement one for ourself  this api should cover exceptly the same amount of information as you can get from the web interface the implementation is base on the current REST implementation of spark core and will be available for running applications only here is how you can use it  endpoint root   streaming api v     Endpoint    Meaning      statistics Statistics information of stream    receivers A list of all receiver streams    receivers   stream id   Details of the given receiver stream    batches A list of all retained batches    batches   batch id   Details of the given batch    batches   batch id   operations A list of all output operations of the given batch    batches   batch id   operations   operation id   Details of the given operation  given batch Add a REST api to spark streaming
 remove OverwriteOptions
We should include in Spark distribution the built source package for SparkR  This will enable help and vignettes when the package is used  Also this source package is what we would release to CRAN R   Include package vignettes and help pages  build source package in Spark distribution
Move DT RF GBT Param setter methods to subclasses and deprecate these methods in the Model classes to make them more Java friendly  See discussion at https   github com apache spark pull       discussion r Move DT RF GBT Param setter methods to subclasses
AggregateFunction   currently implements   ImplicitCastInputTypes    which enables implicit input type casting   This can lead to unexpected results  and should only be enabled when it is suitable for the function at hand AggregateFunction should not ImplicitCastInputTypes
We currently have function input file name to get the path of the input file  but don t have functions to get the block start offset and length  This patch introduces two functions     input file block start  returns the file block start offset  or    if not available     input file block length  returns the file block length  or    if not available input file block start and input file block length function
Many Spark developers often want to test the runtime of some function in interactive debugging and testing  It d be really useful to have a simple spark time method that can test the runtime SparkSession time   a simple timer function
Otherwise  other threads cannot query the content in MemorySink when  DataFrame collect  takes long time to finish MemorySink should not call DataFrame collect when holding a lock
An informal poll of a bunch of users found this name to be more clear Rename recentProgresses to recentProgress
Currently  when users use Python UDF in Filter    BatchEvalPython   is always generated below   FilterExec    However  not all the predicates need to be evaluated after Python UDF execution  Thus  we can push down the predicates through   BatchEvalPython Push Down Filter Through BatchEvalPython
spark logit is added in      We need to update spark vignettes to reflect the changes  This is part of SparkR QA work Update spark logit in sparkr vignettes
Implement a wrapper in SparkR to support bisecting k means Bisecting k means wrapper in SparkR
When starting a stream with a lot of backfill and maxFilesPerTrigger  the user could often want to start with most recent files first  This would let you keep low latency for recent data and slowly backfill historical data  It s better to add an option to control this behavior Make FileStream be able to start with most recent files
 Expose event time time stats through StreamingQueryProgress
Make StreamExecution and progress classes serializable because it is too easy for it to get captured with normal usage Make StreamExecution and progress classes serializable
SparkR mllib R is getting bigger as we add more ML wrappers  I d like to split it into multiple files to make us easy to maintain    mllibClassification R   mllibRegression R   mllibClustering R   mllibFeature R or    mllib classification R   mllib regression R   mllib clustering R   mllib features R For R convention  it s more prefer the first way  And I m not sure whether R supports the second organized way  will check later   Please let me know your preference  I think the start of a new release cycle is a good opportunity to do this  since it will involves less conflicts  If this proposal was approved  I can work on it  cc   felixcheung    josephkb    mengxr Split SparkR mllib R into multiple files
Currently  we only have a SQL interface for recovering all the partitions in the directory of a table and update the catalog   MSCK REPAIR TABLE  or  ALTER TABLE table RECOVER PARTITIONS    Actually  very hard for me to remember  MSCK  and have no clue what it means  After the new  Scalable Partition Handling   the table repair becomes much more important for making visible the data in the created data source partitioned table  It is desriable to add it into the Catalog interface so that users can repair the table by Add recoverPartitions API to Catalog
I recently hit a bug of com thoughtworks paranamer paranamer  which causes jackson fail to handle byte array defined in a case class  Then I find https   github com FasterXML jackson module scala issues     which suggests that it is caused by a bug in paranamer  Let s upgrade paranamer  Since we are using jackson       and jackson module paranamer       use com thoughtworks paranamer paranamer      I suggests that we upgrade paranamer to Upgrade com thoughtworks paranamer paranamer to
SortPartitions and RedistributeData logical operators are not actually used and can be removed  Note that we do have a Sort operator  with global flag false  that subsumed SortPartitions Remove SortPartitions and RedistributeData
Right now  ContextCleaner referenceBuffer  is ConcurrentLinkedQueue and the time complexity of the  remove  action is O   n    It can be changed to use ConcurrentHashMap whose  remove  is O Change ContextCleaner referenceBuffer to ConcurrentHashMap to make it faster
Since   spark sql hive thriftServer singleSession   is a configuration of SQL component  this conf can be moved from   SparkConf   to   StaticSQLConf    When we introduced   spark sql hive thriftServer singleSession    all the SQL configuration can be modified in different sessions  Later  static SQL configuration is added  It is a perfect fit for   spark sql hive thriftServer singleSession    Previously  we did the same move for   spark sql warehouse dir   from   SparkConf   to   StaticSQLConf Move spark sql hive thriftServer singleSession to SQLConf
Remove useless  databaseName   from  SimpleCatalogRelation Remove databaseName from SimpleCatalogRelation
 remove the supportsPartial flag in AggregateFunction
In SPARK        support for AES encryption was added to the Spark network library  But the authentication of different Spark processes is still performed using SASL s DIGEST MD  mechanism  That means the authentication part is the weakest link  since the AES keys are currently encrypted using  des  strongest cipher supported by SASL   Spark can t really claim to provide the full benefits of using AES for encryption  We should add a new auth protocol that doesn t need these disclaimers AES based authentication mechanism for Spark
Currently in SQL we implement overwrites by calling fs delete   directly on the original data  This is not ideal since we the original files end up deleted even if the job aborts  We should extend the commit protocol to allow file overwrites to be managed as well Add deleteWithJob hook to internal commit protocol API
When we use the   jdbc   in pyspark  if we check the lowerBound and upperBound  we can give a more friendly suggestion Check the lowerBound and upperBound whether equal None in jdbc API
To implement DDL commands  we added several analyzer rules in sql hive module to analyze DDL related plans  However  our Analyzer currently only have one extending interface  extendedResolutionRules  which defines extra rules that will be run together with other rules in the resolution batch  and doesn t fit DDL rules well  because     DDL rules may do some checking and normalization  but we may do it many times as the resolution batch will run rules again and again  until fixed point  and it s hard to tell if a DDL rule has already done its checking and normalization  It s fine because DDL rules are idempotent  but it s bad for analysis performance    some DDL rules may depend on others  and it s pretty hard to write if conditions to guarantee the dependencies  It will be good if we have a batch which run rules in one pass  so that we can guarantee the dependencies by rules order add a new extending interface in Analyzer for post hoc resolution
Create a Python wrapper for spark ml classification LinearSVC LinearSVC Python API
Apache Parquet       is released officially last week on    Jan  This issue aims to bump Parquet version to       since it includes many fixes  https   lists apache org thread html af c   f          a   d  ec  b bbeecaea  aa ef  f   c      Cdev parquet apache org  E Upgrade Parquet to
There is a metadata introduced before to mark the optional columns in merged Parquet schema for filter predicate pushdown  As we upgrade to Parquet       which includes the fix for the pushdown of optional columns  we don t need this metadata now Remove the metadata used to mark optional columns in merged Parquet schema for filter predicate pushdown
The removed codes are not reachable  because  InConversion  already resolve the type coercion issues Remove IN type coercion from PromoteStrings
According to http   doc akka io docs akka       intro getting started html Akka is now published to Maven Central  so our documentation and POM files don t need to use the old Akka repo  It will be one less step for users to worry about Remove use of special Maven repo for Akka
HiveLocalContext is nearly completely redundant with HiveContext  We should consider deprecating it and removing all uses Get rid of LocalHiveContext
This will depend a bit on both user demand and the commitment level of maintainers  but I d like to propose the following timeline for yarn alpha support  Spark      Deprecate YARN alpha Spark      Remove YARN alpha  i e  require YARN stable  Since YARN alpha is clearly identified as an alpha API  it seems reasonable to drop support for it in a minor release  However  it does depend a bit whether anyone uses this outside of Yahoo   and that I m not sure of  In the past this API has been used and maintained by Yahoo  but they ll be migrating soon to the stable API s Deprecate and later remove YARN alpha support
After Pyrolite release a new version with PR https   github com irmen Pyrolite pull     we should remove the workaround introduced in PR https   github com apache spark pull remove workaround to pickle array of float for Pyrolite
We should upgrade snappy java to         across all of our maintenance branches  This release improves error messages when attempting to deserialize empty inputs using SnappyInputStream  this operation is always an error  but the old error messages made it hard to distinguish failures due to empty streams from ones due to reading invalid   corrupted streams   see https   github com xerial snappy java issues    for more context  This should be a major help in the Snappy debugging work that I ve been doing Upgrade snappy java to
Due to vertex attribute caching  EdgeRDD previously took two type parameters  ED and VD  However  this is an implementation detail that should not be exposed in the interface  so this PR drops the VD type parameter  This requires removing the filter method from the EdgeRDD interface  because it depends on vertex attribute caching Drop VD type parameter from EdgeRDD
For example  YarnRMClient and YarnRMClientImpl can be merged YarnAllocator and YarnAllocationHandler can be merged Remove layers of abstraction in YARN code no longer needed after dropping yarn alpha
In RDDSampler  it try use numpy to gain better performance for possion    but the number of call of random   is only    faction    N in the pure python implementation of possion    so there is no much performance gain from numpy  numpy is not a dependent of pyspark  so it maybe introduce some problem  such as there is no numpy installed in slaves  but only installed master  as reported in xxxx  It also complicate the code a lot  so we may should remove numpy from RDDSampler remove numpy from RDDSampler of PySpark
In this refactoring  the performance is slightly increased by removing the overhead from breeze vector  The bottleneck is still in breeze norm which is implemented by activeIterator  This inefficiency of breeze norm will be addressed in next PR  At least  this PR makes the base Refactorize Normalizer to make code cleaner
mqtt client       was removed from the Eclipse Paho repository  and hence is breaking Spark build Upgrade MQTT dependency to use mqtt client
 Remove unneeded staging repositories from build
Fix a todo in spark sql  remove    Command    and use    RunnableCommand    instead Refactory command in spark sql
toLocalIterator is available in Java and Scala  If we add this functionality to Python  then we can also be able to use PySpark to iterate over a dataset partition by partition Add toLocalIterator to pyspark rdd
This method survived the code review and it has been there since v       It exposes jblas types  Let s remove it from the public API  I expect that no one calls it directly Hide ALS solveLeastSquares
Continue the discussion from the LDA PR  CheckpoingDir is a global Spark configuration  which should not be altered by an ML algorithm  We could check whether checkpointDir is set if checkpointInterval is positive Remove setCheckpointDir from LDA and tree Strategy
Deprecated configs are currently all strewn across the code base  It would be good to simplify the handling of the deprecated configs in a central location to avoid duplicating the deprecation logic everywhere Centralize deprecated configs in SparkConf
 Upgrade Tachyon dependency to
Would be great to create APIs for external block stores  rather than doing a bunch of if statements everywhere Create external block store API
This depends on some internal interface of Spark SQL  should be done after merging into Spark DataFrame UDFs in R
This is not always possible  but whenever possible we should remove or reduce the differences between Pandas and Spark DataFrames in Python Improve DataFrame API compatibility with Pandas
This maven repository is blocked in China  We should get rid of that dependency so people in China can compile Spark Remove dependency on Twitter J repository
We want to change and improve the spark ml API for trees and ensembles  but we cannot change the old API in spark mllib  To support the changes we want to make  we should move the implementation from spark mllib to spark ml  We will generalize and modify it  but will also ensure that we do not change the behavior of the old API  There are several steps to this     Copy the implementation over to spark ml and change the spark ml classes to use that implementation  rather than calling the spark mllib implementation  The current spark ml tests will ensure that the   implementations learn exactly the same models  Note  This should include performance testing to make sure the updated code does not have any regressions       UPDATE   I have run tests using spark perf  and there were no regressions     Remove the spark mllib implementation  and make the spark mllib APIs wrappers around the spark ml implementation  The spark ml tests will again ensure that we do not change any behavior     Move the unit tests to spark ml  and change the spark mllib unit tests to verify model equivalence  This JIRA is now for step   only  Steps   and   will be in separate JIRAs  After these updates  we can more safely generalize and improve the spark ml implementation Move tree forest implementation from spark mllib to spark ml
 Removed diffSum which is theoretical zero in LinearRegression and coding formating
We can just rewrite distinct using groupby  i e  aggregate operator Remove physical Distinct operator in favor of Aggregate
 Removed calling size  length in while condition to avoid extra JVM call
Learnt a lesson from SPARK       Spark should avoid to use  scala concurrent ExecutionContext Implicits global  because the user may submit blocking actions to  scala concurrent ExecutionContext Implicits global  and exhaust all threads in it  This could crash Spark  So Spark should always use its own thread pools for safety Remove  import scala concurrent ExecutionContext Implicits global
ExternalSorter contains a bunch of  to move this functionality out of ExternalSorter and into a separate class which shares a common interface  insertAll   writePartitionedFile     This is a stepping stone towards eventually removing this bypass path  see SPARK Move hash style shuffle code out of ExternalSorter and into own file
It s not a very useful type to use  We can just remove it to simplify expressions slightly Remove EvaluatedType from SQL Expression
From my perspective as a code reviewer  I find them more confusing than using String directly Remove Term Code type aliases in code generation
We should remove the existing ExpressionOptimizationSuite  and update checkEvaluation to also run the optimizer version ExpressionEvalHelper checkEvaluation should also run the optimizer version
This will make it convenient for R users to use SparkR from their browsers Install and configure RStudio server on Spark EC
Based on discussion offline with   marmbrus   we should remove GenerateProjection Remove GenerateProjection
The type alias was there because initially when I moved Row around  I didn t want to do massive changes to the expression code  But now it should be pretty easy to just remove it  One less concept to worry about Remove InternalRow type alias in expressions package
Right now InternalRow is megamorphic because it has many different implementations  We should work towards having only one or at most two InternalRow implementations Remove EmptyRow class
It is unnecessary and makes the type hierarchy slightly more complicated than needed Remove ExtractValueWithOrdinal abstract class
The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark      Also  the feature in SPARK      is strictly better than a correct implementation of that feature  We should remove any documentation references to that feature and print a warning when it is used saying it doesn t work Remove references to preferredNodeLocalityData in javadoc and print warning when used
They are not very useful  and cause problems with toString due to the order they are mixed in Remove LeafNode  UnaryNode  BinaryNode from TreeNode
As the new Parquet external data source matures  we should remove the old Parquet support now Removes old Parquet support code
A small change  based on code review and offline discussion with   dragos Removing unnecessary self types in Catalyst
Spark has an option called   spark localExecution enabled    according to the docs   quote  Enables Spark to run certain jobs  such as first   or take   on the driver  without sending tasks to the cluster  This can make certain jobs execute very quickly  but may require shipping a whole partition of data to the driver   quote  This feature ends up adding quite a bit of complexity to DAGScheduler  especially in the   runLocallyWithinThread   method  but as far as I know nobody uses this feature  I searched the mailing list and haven t seen any recent mentions of the configuration nor stacktraces including the runLocally method   As a step towards scheduler complexity reduction  I propose that we remove this feature and all code related to it for Spark Remove DAGScheduler runLocallyWithinThread and spark localExecution enabled
See  SPARK        We added varargs  again   Though it is technically correct  it often requires that developers do clean assembly  rather than  not clean  assembly  which is a nuisance during development  This JIRA will remove it for now  pending a fix to the Scala compiler Params setDefault should not keep varargs annotation
It is a big change  but it lets us use the type information to prevent accidentally passing internal types to external types Remove InternalRow s inheritance from Row
We should consolidate LocalScheduler and ClusterScheduler  given most of the functionalities are duplicated in both  This can be done by removing the LocalScheduler  and create a LocalSchedulerBackend that connects directly to an Executor Consolidate local scheduler and cluster scheduler
 Remove UnsafeRowConverter in favor of UnsafeProjection
They were added to improve performance  so JIT can inline the JoinedRow calls   However  we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators Remove all extra JoinedRows
While reviewing   yhuai  s patch for SPARK       I noticed that Exchange s   compatible   check may be incorrectly returning   false   in many cases  As far as I know  this is not actually a problem because the   compatible      meetsRequirements    and   needsAnySort   checks are serving only as short circuit performance optimizations that are not necessary for correctness  In order to reduce code complexity  I think that we should remove these checks and unconditionally rewrite the operator s children  This should be safe because we rewrite the tree in a single bottom up pass Remove compatibleWith  meetsRequirements  and needsAnySort checks from Exchange
 remove the createCode and createStructCode  and replace the usage of them by createStructCode
See http   apache spark developers list         n  nabble com Re Should spark ec  get its own repo td      html for more details Move spark ec  from mesos to amplab
 Add StreamingContext getActiveOrCreate   to python API
Spark s style checker should ban the use of Scala s JavaConversions  which provides implicit conversions between Java and Scala collections types  Instead  we should be performing these conversions explicitly using JavaConverters  or forgoing the conversions altogether if they re occurring inside of performance critical code Ban use of JavaConversions and migrate all existing uses to JavaConverters
 update InternalRow toSeq to make it accept data type info
In many modeling application  data points are not necessarily sampled with equal probabilities  Linear regression should support weighting which account the over or under sampling LinearRegression should supported weighted data
Currently  we don t support using DecimalType with precision      in new unsafe aggregation  it s good to support it Support update DecimalType with precision      in UnsafeRow
GenerateUnsafeProjection can be used directly as a code generated serializer  We no longer need SparkSqlSerializer Remove SparkSqlSerializer  in favor of Unsafe exchange
It is subsumed by the new aggregate implementation Remove GeneratedAggregate
A small performance optimization    we don t need to generate a Tuple  and then immediately discard the key  We also don t need an extra wrapper Remove SqlNewHadoopRDD s generated Tuple  and InterruptibleIterator
E g  currently we can do up to   sorts within a task      During the aggregation     During a sort on the same key     During the shuffle In environments with tight memory restrictions  the first operator may acquire so much memory such that the subsequent ones in the same task are starved  A simple fix is to reserve at least a page in advance in each of these places  The reserved page size need not be the same as the normal page size  This is a sister problem to SPARK      in Spark Core Reserve a page in all unsafe operators to avoid starving an operator
Add feature interaction as a transformer  which takes a list of vector double columns  and generate a single vector column that contains the interactions  multiplication  among them with proper handling of feature names Add feature interaction as a transformer
Previously  we use   MB as the default page size  which was way too big for a lot of Spark applications  especially for single node   This patch changes it so that the default page size  if unset by the user  is determined by the number of cores available and the total execution memory available Pick default page size more intelligently
Consider SortMergeJoin  which requires a sorted  clustered distribution of its input rows  Say that both of SMJ s children produce unsorted output but are both single partition  In this case  we will need to inject sort operators but should not need to inject exchanges  Unfortunately  it looks like the Exchange unnecessarily repartitions using a hash partitioning  We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied  I d like to fix this for Spark     since it makes certain types of unit tests easier to write EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied
This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages  Issue brought up by   eronwright   Descriptions below copied from  http   apache spark developers list         n  nabble com Make ML Developer APIs public post     td      html   We plan to make these APIs public in Spark      However  they will be marked DeveloperApi and are  very likely  to be broken in the future    VectorUDT  To define a relation with a vector field  VectorUDT must be instantiated    Identifiable trait  The trait generates a unique identifier for the associated pipeline component  Nice to have a consistent format by reusing the trait    ProbabilisticClassifier  Third party components should leverage the complex logic around computing only selected columns  We will not yet make these public    SchemaUtils  Third party pipeline components have a need for checking column types and appending columns     This will probably be moved into Spark SQL  Users can copy the methods into their own  as needed Make some ML APIs public  VectorUDT  Identifiable  ProbabilisticClassifier
JoinedRow anyNull currently loops through every field to check for null  which is inefficient if the underlying rows are UnsafeRows  It should just delegate to the underlying implementation JoinedRow anyNull should delegate to the underlying rows
In https   github com apache spark pull      we added  FromUnsafe  to convert nexted unsafe data like array map struct to safe versions  It s a quick solution and we already have  GenerateSafe  to do the conversion which is codegened  So we should remove  FromUnsafe  and implement its codegen version in  GenerateSafe remove FromUnsafe and add its codegen version to GenerateSafe
TypeCheck no longer applies in the new Tungsten world Remove TypeCheck in debug package
see discussion here  https   github com apache spark pull      issuecomment Improve performance of Decimal times   and casting from integral
We introduced the Netty network module for shuffle in Spark      and has turned it on by default for   releases  The old ConnectionManager is difficult to maintain  It s time to remove it Remove ConnectionManager
Add Python API  user guide and example for ml feature CountVectorizerModel Add Python API for ml feature CountVectorizer
Add Python API for MultilayerPerceptronClassifier Add Python API for MultilayerPerceptronClassifier
Add Python API  user guide and example for ml regression IsotonicRegression Add Python API for ml regression IsotonicRegression
I took a look at the commit messages in git log    it looks like the individual commit messages are not that useful to include  but do make the commit messages more verbose  They are usually just a bunch of extremely concise descriptions of  bug fixes    merges   etc     See mailing list discussions  http   apache spark developers list         n  nabble com discuss Removing individual commit messages from the squash commit message td      html Remove individual commit messages from the squash commit message
PlatformDependent UNSAFE is way too verbose Rename PlatformDependent UNSAFE    Platform
The utilities such as Substring substringBinarySQL and BinaryPrefixComparator computePrefix for binary data are put together in ByteArray for easy to read Move utilities for binary data into ByteArray
parquet mr         fixed several issues that affect Spark  For example PARQUET      SPARK Upgrade parquet mr to
There exists a chance that the prefixes keep growing to the maximum pattern length  Then the final local processing step becomes unnecessary Skip local processing in PrefixSpan if there are no small prefixes
This requires some discussion  I m not sure whether  runs  is a useful parameter  It certainly complicates the implementation  We might want to optimize the k means implementation with block matrix operations  In this case  having  runs  may not be worth the trade offs Remove runs from KMeans under the pipeline API
What StringIndexerInverse does is not strictly associated with StringIndexer  and the name is not super clear Rename StringIndexerInverse to IndexToString
We should deprecate ConnectionManager in     before removing it in Deprecate NIO ConnectionManager
