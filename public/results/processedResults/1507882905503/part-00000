Description,Summary
This is necessary for s3 reads and writes to work correctly with some hadoop versions.,Add jets3t dependency to Spark Build
Our current dataset.registerTempTable does not actually materialize data. So, it should be considered as creating a temp view. We can deprecate it and create a new method called dataset.createTempView(replaceIfExists: Boolean). The default value of replaceIfExists should be false. For registerTempTable, it will call dataset.createTempView(replaceIfExists = true).,Deprecate registerTempTable and add dataset.createTempView
Currently TestFilterFileSystem only checks for FileSystem methods that must be implemented in FilterFileSystem with a list of methods that are exception to this rule  This jira wants to make this check stricter by adding a test for ensuring the methods in exception rule list must not be implemented by the FilterFileSystem  This also cleans up the current class that has methods from exception rule list to interface to avoid having to provide dummy implementation of the methods,Cleanup TestFilterFileSystem
As noted in MAPREDUCE      and HADOOP       LocalDirAllocator can be a bottleneck for multithreaded setups like the ShuffleHandler  We should consider moving to a lockless design or minimizing the critical sections to a very small amount of time that does not involve I O operations,LocalDirAllocator should avoid holding locks while accessing the filesystem
Jetty  is no longer maintained  Update the dependency to jetty,Update jetty dependency to version
The classes in o a h record have been deprecated for more than a year and a half  They should be removed  As the first step  the jira moves all these classes into the hadoop streaming project  which is the only user of these classes,Move o a h record to hadoop streaming
Hadoop streaming no longer requires many classes in o a h record  This jira removes the dead code,Remove dead classes in hadoop streaming
Storm would like to be able to fetch delegation tokens and forward them on to running topologies so that they can access HDFS  STORM       But to do so we need to open up access to some of APIs  Most notably FileSystem addDelegationTokens    Token renew  Credentials getAllTokens  and UserGroupInformation but there may be others  At a minimum adding in storm to the list of allowed API users  But ideally making them public  Restricting access to such important functionality to just MR really makes secure HDFS inaccessible to anything except MR  or tools that reuse MR input formats,Open up already widely used APIs for delegation token fetching   renewal to ecosystem projects
We should make an effort to clean up the shell env var name space by removing unsafe variables  See comments for list,Rename remove non HADOOP    etc from the shell scripts
kms was not rewritten to use the new shell framework  It should be reworked to take advantage of it,Rewrite kms to use new shell framework
KMS and HttpFS are using Tomcat         we should move it to        to get bug fixes and security fixes  We should add a property with the tomcat version in the hadoop project POM and use that property from KMS and HttpFS,Update Tomcat version used by HttpFS and KMS to latest   x version
Write a metrics  sink plugin for Hadoop to send metrics directly to Apache Kafka in addition to the current  Graphite   Hadoop      https   issues apache org jira browse HADOOP         Ganglia and File sinks,metrics  sink plugin for Apache Kafka
Post HADOOP       we need to rework how heap is configured for small footprint machines  deprecate some options  introduce new ones for greater flexibility,rework heap management vars
As part of HADOOP       java execution across many different shell bits were consolidated down to  effectively  two routines  Prior to calling those two routines  the CLASSPATH is exported  This export should really be getting handled in the exec function and not in the individual shell bits  Additionally  it would be good if there was     so that bash  x would show the content of the classpath or even a    debug classpath  option that would echo the classpath to the screen prior to java exec to help with debugging,CLASSPATH handling should be consolidated  debuggable
There is little to no reason for it to call hadoop daemon sh anymore,hadoop daemons sh should just call hdfs directly
Java   is coming quickly to various clusters  Making sure Hadoop seamlessly works with Java   is important for the Apache community  This JIRA is to track the issues experiences encountered during Java   migration  If you find a potential bug   please create a separate JIRA either as a sub task or linked into this JIRA  If you find a Hadoop or JVM configuration tuning  you can create a JIRA as well  Or you can add a comment here,Umbrella  Support Java   in Hadoop
Hadoop currently supports one JVM defined through JAVA HOME  Since multiple JVMs  Java          are active  it will be helpful if there is an user configuration to choose the custom but supported JVM for her job  In other words  user will be able to choose her expected JVM only for her container execution while Hadoop services may be running on different JVM,Allow user to choose JVM for container execution
DistCp uses ThrottleInputStream  which provides a bandwidth throttling on a specified stream  Currently  Distcp allows the max bandwidth value in Mega Bytes  which does not accept fractional values  It would be better if it accepts the Max Bandwitdh in fractional MegaBytes  Due to this we are not able to throttle the bandwidth in KBs in our prod setup,Allow ditscp to accept bandwitdh in fraction MegaBytes
During heavy shuffle  packet loss for IPC packets was observed from a machine  Avoid packet loss and speed up transfer by using  x   QOS bits for the packets,Add a configuration to set ipc Client s traffic class with IPTOS LOWDELAY IPTOS RELIABILITY
As hadoop     will drop the support of Java    the jenkins slaves should be compiling code using Java,Move jenkins to Java
,Rewrite sls rumen to use new shell framework
The system should be able to read in user defined env vars from    hadooprc,Add support for  hadooprc
The   o a h fs permission AccessControlException   has been deprecated for last major releases and it should be removed,Removed deprecated o a h fs permission AccessControlException
It is a very common shell pattern in   x to effectively replace sub project specific vars with generics  We should have a function that does this replacement and provides a warning to the end user that the old shell var is deprecated  Additionally  we should use this shell function to deprecate the shell vars that are holdovers already,Deprecate shell vars
Along the same vein as HADOOP        there are now several remaining usages of guava APIs that are now incompatible with a more recent version  e g       This JIRA proposes eliminating those usages  With this  the hadoop base compatible with guava,Remove some uses of obsolete guava APIs from the hadoop codebase
It would be useful to provide a way for core and non core Hadoop components to plug into the shell infrastructure  This would allow us to pull the HDFS  MapReduce  and YARN shell functions out of hadoop functions sh  Additionally  it should let  rd parties such as HBase influence things like classpaths at runtime,Pluggable shell integration
This should be hdfs dfsadmin and yarn rmadmin,ServiceLevelAuth still references hadoop dfsadmin mradmin
We had an application sitting on top of Hadoop and got problems using jsch once we switched to java    Got this exception     Upgrading to jsch        from jsch        fixed the issue for us  but then it got in conflict with hadoop s jsch version  we fixed this for us by jarjar ing our jsch version   So i think jsch got introduce by namenode HA  HDFS        So you guys should check if the ssh part is properly working for java  or preventively upgrade the jsch lib to jsch         Some references to problems reported    http   sourceforge net p jsch mailman jsch users thread loom         T           post gmane org    https   issues apache org bugzilla show bug cgi id,Upgrade jsch lib to jsch        to avoid problems running on java
An RPC server handler thread is tied up for each incoming RPC request  This isn t ideal  since this essentially implies that RPC operations should be short lived  and most operations which could take time end up falling back to a polling mechanism  Some use cases where this is useful    YARN submitApplication   which currently submits  followed by a poll to check if the application is accepted while the submit operation is written out to storage  This can be collapsed into a single call    YARN allocate   requests and allocations use the same protocol  New allocations are received via polling  The allocate protocol could be split into a request heartbeat along with a  awaitResponse   The request heartbeat is sent only when there s a request or on a much longer heartbeat interval  awaitResponse is always left active with the RM   and returns the moment something is available  MapReduce Tez task to AM communication is another example of this pattern  The same pattern of splitting calls can be used for other protocols as well  This should serve to improve latency  as well as reduce network traffic since the keep alive heartbeat can be sent less frequently  I believe there s some cases in HDFS as well  where the DN gets told to perform some operations when they heartbeat into the NN,Allow handoff on the server side for RPC requests
HadoopKerberosName has been around as a  secret hack  for quite a while  We should clean up the output and make it official by exposing it via the hadoop command,Expose HadoopKerberosName as a hadoop subcommand
Add a   slaves shell option to hadoop config sh to trigger the given command on slave nodes  This is required to deprecate hadoop daemons sh and yarn daemons sh,Add   slaves shell option
With HADOOP       now committed  we need to remove usages of yarn daemons sh and hadoop daemons sh from the start and stop scripts  converting them to use the new   slaves option  Additionally  the documentation should be updated to reflect these new command options,Update sbin commands and documentation to use new   slaves option
When a new file is added  the source is  dev null  rather than the root of the tree  which would mean a a b prefix   Allow for this,Allow smart apply patch sh to add new files in binary git patches
According to the discussion in HADOOP       we should remove   io native lib available   from trunk  and always use native libraries if they exist,Remove io native lib available
FileUtil copyMerge is currently unused in the Hadoop source tree  In branch    it had been part of the implementation of the hadoop fs  getmerge shell command  In branch    the code for that shell command was rewritten in a way that no longer requires this method  Please check more details here   https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment,Deprecate FileUtil copyMerge
The EMC ViPR ECS object storage platform uses proprietary headers starting by x emc    like Amazon does with x amz     Headers starting by x emc   should be included in the signature computation  but it s not done by the Amazon S  Java SDK  it s done by the EMC S  SDK   When s a copy an object it copies all the headers  but when the object includes x emc   headers  it generates a signature mismatch  Removing the x emc   headers from the copy would allow s a to be compatible with the EMC ViPR ECS object storage platform  Removing the x   which aren t x amz   headers from the copy would allow s a to be compatible with any object storage platform which is using proprietary headers,Ignore x   and response headers when copying an Amazon S  object
distcpv  is pretty much unsupported  we should just remove it,Remove DistCpV  and Logalyzer
Currently    ViewFileSystem   does not dispatch snapshot methods through the mount table  All snapshot methods throw   UnsupportedOperationException    even though the underlying mount points could be HDFS instances that support snapshots  We need to update   ViewFileSystem   to implement the snapshot methods,ViewFileSystem should support snapshot methods
Extend AltKerberosAuthenticationHandler to provide WebSSO flow for UIs  The actual authentication is done by some external service that the handler will redirect to when there is no hadoop auth cookie and no JWT token found in the incoming request  Using JWT provides a number of benefits    It is not tied to any specific authentication mechanism   so buys us many SSO integrations   It is cryptographically verifiable for determining whether it can be trusted   Checking for expiration allows for a limited lifetime and window for compromised use This will introduce the use of nimbus jose jwt library for processing  validating and parsing JWT tokens,Add Redirecting WebSSO behavior with JWT Token in Hadoop Auth
The current way we generate these build artifacts is awful  Plus they are ugly and  in the case of release notes  very hard to pick out what is important,Rework the changelog and releasenotes
With the commit of HADOOP        the CHANGES txt files are now EOLed  We should remove them,Remove all of the CHANGES txt files
For very large source trees on s  distcp is taking long time to build file listing  client code  before starting mappers   For a dataset I used     M files    K dirs  it was taking    minutes before my fix in HADOOP       and    minutes after the fix,Speed up distcp buildListing   using threadpool
As discussed with   aw    In AVRO      a docker based solution was created to setup all the tools for doing a full build  This enables much easier reproduction of any issues and getting up and running for new developers  This issue is to  copy port  that setup into the hadoop project in preparation for the bug squash,Make setting up the build environment easier
Set minimum version of trunk to JDK,JDK   Set minimum version of Hadoop   to JDK
deprecate DistCpV  and Logalyzer  which are no longer used,Deprecate DistCpV  and Logalyzer
we have the issue matching regex configurable via altering the default  add in a cli arg to update it on invocation,test patch s issue matching regex should be configurable
AzureStorageExceptions currently are logged as part of the WAB code which often is not too informative  AzureStorage SDK supports client side logging that can be enabled that logs relevant information w r t request made from the Storage client  This JIRA is created to enable Azure Storage Client Side logging at the Job submission level  User should be able to configure Client Side logging on a Per Job bases,Enable Azure Storage Client Side logging
ThreadLocalRandom should be used when available in place of ThreadLocal  For JDK  the difference is minimal  but JDK  starts including optimizations for ThreadLocalRandom,Replace uses of ThreadLocal with JDK  ThreadLocalRandom
During http authentication  a cookie which contains the authentication token is dropped  The expiry time of the authentication token can be configured via hadoop http authentication token validity  The default value is    hours  For clusters which require enhanced security  it is desirable to have a configurable MaxInActiveInterval for the authentication token  If there is no activity during MaxInActiveInterval  the authentication token will be invalidated  The MaxInActiveInterval will be less than hadoop http authentication token validity  The default value will be    minutes,Enable MaxInactiveInterval for hadoop http auth token
Since our min version is now JDK   there s hardlink support via   Files    This means we can deprecate the JNI implementation and discontinue usage,Deprecate usage of NativeIO link
Right now S Credentials only works with cleartext passwords in configs  as a secret access key or the URI   The non URI version should use credential providers with a fallback to the clear text option,S Credentials should support use of CredentialProvider
guice     doesn t work with lambda statement  https   github com google guice issues     We should upgrade it to     which includes the fix,JDK   Update guice version to
The requirement is to support LDAP based authentication scheme via Hadoop AuthenticationFilter  HADOOP      added a support to plug in custom authentication scheme  in addition to Kerberos  via AltKerberosAuthenticationHandler class  But it is based on selecting the authentication mechanism based on User Agent HTTP header which does not conform to HTTP protocol semantics  As per  RFC      http   www w  org Protocols rfc     rfc     html    HTTP protocol provides a simple challenge response authentication mechanism that can be used by a server to challenge a client request and by a client to provide the necessary authentication information    This mechanism is initiated by server sending the      Authenticate  response with  WWW Authenticate  header which includes at least one challenge that indicates the authentication scheme s  and parameters applicable to the Request URI    In case server supports multiple authentication schemes  it may return multiple challenges with a      Authenticate  response  and each challenge may use a different auth scheme    A user agent MUST choose to use the strongest auth scheme it understands and request credentials from the user based upon that challenge  The existing Hadoop authentication filter implementation supports Kerberos authentication scheme and uses  Negotiate  as the challenge as part of  WWW Authenticate  response header  As per the following documentation   Negotiate  challenge scheme is only applicable to Kerberos  and Windows NTLM  authentication schemes   SPNEGO based Kerberos and NTLM HTTP Authentication http   tools ietf org html rfc       Understanding HTTP Authentication https   msdn microsoft com en us library ms         v vs        aspx  On the other hand for LDAP authentication  typically  Basic  authentication scheme is used  Note TLS is mandatory with Basic authentication scheme   http   httpd apache org docs trunk mod mod authnz ldap html Hence for this feature  the idea would be to provide a custom implementation of Hadoop AuthenticationHandler and Authenticator interfaces which would support both schemes   Kerberos  via Negotiate auth challenge  and LDAP  via Basic auth challenge   During the authentication phase  it would send both the challenges and let client pick the appropriate one  If client responds with an  Authorization  header tagged with  Negotiate    it will use Kerberos authentication  If client responds with an  Authorization  header tagged with  Basic    it will use LDAP authentication  Note   some HTTP clients  e g  curl or Apache Http Java client  need to be configured to use one scheme over the other e g    curl tool supports option to use either Kerberos  via   negotiate flag  or username password based authentication  via   basic and  u flags     Apache HttpClient library can be configured to use specific authentication scheme  http   hc apache org httpcomponents client ga tutorial html authentication html Typically web browsers automatically choose an authentication scheme based on a notion of  strength  of security  e g  take a look at the  design of Chrome browser for HTTP authentication https   www chromium org developers design documents http authentication,Support multiple authentication schemes via AuthenticationFilter
Given test patch s tendency to get forked into a variety of different projects  it makes a lot of sense to make an Apache TLP so that everyone can benefit from a common code base,Umbrella  Split test patch off into its own TLP
Some of the monitoring functions could be moved from YARN to Common for easier sharing,Move ResourceCalculatorPlugin from YARN to Common
NetworkToplogy uses nodes with a list of children  The access to these children is slow as it s a linear search,NetworkTopology is not efficient adding getting removing nodes
The protoc maven plugin currently generates new Java classes every time  which means Maven always picks up changed files in the build  It would be better if the protoc plugin only generated new Java classes when the source protoc files change,Support for incremental generation in the protoc plugin
In order to enable significantly better unit testing as well as enhanced functionality  large portions of   config sh should be pulled into functions  See first comment for more,pull argument parsing into a function
When using   LdapGroupsMapping   with Hadoop  nested groups are not supported  So for example if user   jdoe   is part of group A which is a member of group B  the group mapping currently returns only group A  Currently this facility is available with   ShellBasedUnixGroupsMapping   and SSSD  or similar tools  but would be good to have this feature as part of   LdapGroupsMapping   directly,Add support for nested groups in LdapGroupsMapping
The new JVM pause monitor has been written with its own start stop lifecycle which has already proven brittle to both ordering of operations and  even after HADOOP        is not thread safe  both start and stop are potentially re entrant   It also requires every class which supports the monitor to add another field and perform the lifecycle operations in its own lifecycle  which  for all Yarn services  is the YARN app lifecycle  as implemented in Hadoop common  Making the monitor a subclass of   AbstractService   and moving the init start   stop operations in   serviceInit        serviceStart         serviceStop     methods will fix the concurrency and state model issues  and make it trivial to add as a child to any YARN service which subclasses   CompositeService    most the NM and RM apps  will be able to hook up the monitor simply by creating one in the ctor and adding it as a child,Make JvmPauseMonitor an AbstractService
This JIRA proposes to add a counter called RpcSlowCalls and also a configuration setting that allows users to log really slow RPCs  Slow RPCs are RPCs that fall at   th percentile  This is useful to troubleshoot why certain services like name node freezes under heavy load,RPC Metrics   Add the ability track and log slow RPCs
HADOOP       mitigated the problem of HMaster aborting regionserver due to Azure Storage Throttling event during HBase WAL archival  The way this was achieved was by applying an intensive exponential retry when throttling occurred  As a second level of mitigation we will change the mode of copy operation if the operation fails even after all retries  i e  we will do a client side copy of the blob and then copy it back to destination  This operation will not be subject to throttling and hence should provide a stronger mitigation  However it is more expensive  hence we do it only in the case we fail after all retries,Change Mode Of Copy Operation of HBase WAL Archiving to bypass Azure Storage Throttling after retries
We have seen many cases with customers deleting data inadvertently with  skipTrash  The FSShell should prompt user if the size of the data or the number of files being deleted is bigger than a threshold even though  skipTrash is being used,Add  safely flag to rm to prompt when deleting many files
It would be useful for  rd party apps to know the locations of things when hadoop is running without explicit path env vars set,expose calculated paths
Because CLI is using CommandWithDestination java which add    COPYING   to the tail of file name when it does the copy  For blobstore like S  and Swift  to create    COPYING   file and rename it is expensive    direct  flag can allow user to avoiding the    COPYING   file,Add   direct  flag option for fs copy so that user can choose not to create    COPYING   file
FileSystem createNonRecursive   is deprecated  However  there is no DistributedFileSystem create   implementation which throws exception if parent directory doesn t exist  This limits clients  migration away from the deprecated method  For HBase  IO fencing relies on the behavior of FileSystem createNonRecursive    Variant of create   method should be added which throws exception if parent directory doesn t exist,Undeprecate createNonRecursive
kms dt currently does not have its own token identifier class to de,Support decoding KMS Delegation Token with its own Identifier
After HADOOP       we should remove metrics v  from trunk,Remove metrics v
The hadoop azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account  The configuration works by overwriting the src test resources azure test xml file  This can be an error prone process  The azure test xml file is checked into revision control to show an example  There is a risk that the tester could overwrite azure test xml containing the keys and then accidentally commit the keys to revision control  This would leak the keys to the world for potential use by an attacker  This issue proposes to use XInclude to isolate the keys into a separate file  ignored by git  which will never be committed to revision control  This is very similar to the setup already used by hadoop aws for integration testing,Use XInclude in hadoop azure test configuration to isolate Azure Storage account keys for service integration tests
Make the re j dependency consistent with other parts of Hadoop  Seeing some weird rare failures with older versions of maven that appear to be related to this,make re j dependency consistent
It would be good if we could read s  creds from a source other than via a java property Hadoop configuration option,Read s a creds from a Credential Provider
It would be better if Hadoop s dockerfile could be used by Yetus so that external dependencies are owned by the project,Make hadoop dockerfile usable by Yetus
hdfs fetchdt is missing some critical features and is geared almost exclusively towards HDFS operations  Additionally  the token files that are created use Java serializations which are hard impossible to deal with in other languages  It should be replaced with a better utility in common that can read write protobuf based token files  has enough flexibility to be used with other services  and offers key functionality such as append and rename  The old version file format should still be supported for backward compatibility  but will be effectively deprecated  A follow on JIRA will deprecrate fetchdt,Updated utility to create modify token files
The   WriteableRPCEninge   depends on Java s serialization mechanisms for RPC requests  Without proper checks  it has be shown that it can lead to security vulnerabilities such as remote code execution  e g   COLLECTIONS      HADOOP         The current implementation has migrated from   WriteableRPCEngine   to   ProtobufRPCEngine   now  This jira proposes to deprecate   WriteableRPCEngine   in branch   and to remove it in trunk,Deprecate WriteableRPCEngine
The   FileContext   class currently is annotated as   Evolving    However  at this point we really need to treat it as a   Stable   interface,FileContext and AbstractFileSystem should be annotated as a Stable interface
We should add a config to disable the  logs endpoint in HttpServer   Listing a directory like this can be dangerous from a security perspective  We can keep it enabled by default for compatibility though,Add a config to disable the  logs endpoints
HADOOP       added a compile and runtime dependence on the Intel ISA L library but didn t add it to the Dockerfile so that it could be part of the Docker based build environment  start build env sh   This needs to be fixed,Intel ISA L libraries should be added to the Dockerfile
Currently the WASB implementation of the HDFS interface does not support Append API  This JIRA is added to design and implement the Append API support to WASB  The intended support for Append would only support a single writer,Adding Append API support for WASB
Now that Yetus has had a release  we should rip out the components that make it up from dev support and replace them with wrappers  The wrappers should    default to a sane version   allow for version overrides via an env var   download into patchprocess   execute with the given parameters Marking this as an incompatible change  since we should also remove the filename extensions and move these into a bin directory for better maintenance towards the future,Replace dev support with wrappers to Yetus
h   Description This JIRA describes a new file system implementation for accessing Microsoft Azure Data Lake Store  ADL  from within Hadoop  This would enable existing Hadoop applications such has MR  HIVE  Hbase etc    to use ADL store as input or output  ADL is ultra high capacity  Optimized for massive throughput with rich management and security features  More details available at https   azure microsoft com en us services data lake store,Support Microsoft Azure Data Lake   as a file system in Hadoop
Currently Embeded jetty Server used across all hadoop services is configured through ssl server xml file from their respective configuration section  However  the SSL TLS protocol being used for this jetty servers can be downgraded to weak cipher suites  This   so it can exclude the ciphers supplied through this key,Support excluding weak Ciphers in HttpServer  through ssl server xml
Currently if the value of ipc client rpc timeout ms is greater than    the timeout overrides the ipc ping interval and client will throw exception instead of sending ping when the interval is passed  RPC timeout should work without effectively disabling IPC ping,RPC timeout should not override IPC ping interval
CSRF prevention for REST APIs can be provided through a common servlet filter  This filter would check for the existence of an expected  configurable  HTTP header   such as X XSRF Header  The fact that CSRF attacks are entirely browser based means that the above approach can ensure that requests are coming from either  applications served by the same origin as the REST API or that there is explicit policy configuration that allows the setting of a header on XmlHttpRequest from another origin,Add CSRF Filter for REST APIs to Hadoop Common
We need a metrics  sink that can write metrics to HDFS  The sink should accept as configuration a  directory prefix  and do the following in   putMetrics       Get yyyyMMddHH from current timestamp    If HDFS dir  dir prefix    yyyyMMddHH doesn t exist  create it  Close any currently open file and create a new file called  log in the new directory    Write metrics to the current log file,Add an HDFS metrics sink
Some of the checkstyle checks are not realistic  like the line length   leading to spurious    in precommit  Let s disable,Disable spurious checkstyle checks
There is a problem when a user job adds too many dependency jars in their command line  The HADOOP CLASSPATH part can be addressed  including using wildcards       But the same cannot be done with the  libjars argument  Today it takes only fully specified file paths  We may want to consider supporting wildcards as a way to help users in this situation  The idea is to handle it the same way the JVM does it     expands to the list of jars in that directory  It does not traverse into any child directory  Also  it probably would be a good idea to do it only for libjars  i e  don t do it for  files and  archives,support wildcard in libjars argument
Aliyun OSS is widely used among China s cloud users  but currently it is not easy to access data laid on OSS storage from user s Hadoop Spark application  because of no original support for OSS in Hadoop  This work aims to integrate Aliyun OSS with Hadoop  By simple configuration  Spark Hadoop applications can read write data from OSS without any code change  Narrowing the gap between user s APP and data storage  like what have been done for S  in Hadoop,Incorporate Aliyun OSS file system implementation
To protect against CSRF attacks  HADOOP       introduces a CSRF filter that will require a specific HTTP header to be sent with every REST API call  This will affect all API consumers from web apps to CLIs and curl  Since CSRF is primarily a browser based attack we can try and minimize the impact on non browser clients  This enhancement will provide additional configuration for identifying non browser useragents and skipping the enforcement of the header requirement for anything identified as a non browser  This will largely limit the impact to browser based PUT and POST calls when configured appropriately,Extend CSRF Filter with UserAgent Checks
The RollingFileSystemSink only rolls over to a new directory if a new metrics record comes in  The issue is that HDFS does not update the file size until it s closed  HDFS        and if no new metrics record comes in  then the file size will never be updated  This JIRA is to add a background thread to the sink that will eagerly close the file at the top of the hour,RollingFileSystemSink should eagerly rotate directories
Various SSL security fixes are needed  See  CVE            CVE            CVE            CVE,update apache httpclient version to        httpcore to
Remove getaclstatus call for non acl commands in getfacl,Remove getaclstatus call for non acl commands in getfacl
The typical LDAP group name resolution works well under typical scenarios  However  we have seen cases where a user is mapped to many groups  in an extreme case  a user is mapped to more than     groups   The way it s being implemented now makes this case super slow resolving groups from ActiveDirectory  The current LDAP group resolution implementation sends two queries to a ActiveDirectory server  The first query returns a user object  which contains DN  distinguished name   The second query looks for groups where the user DN is a member  If a user is mapped to many groups  the second query returns all group objects associated with the user  and is thus very slow  After studying a user object in ActiveDirectory  I found a user object actually contains a  memberOf  field  which is the DN of all group objects where the user belongs to  Assuming that an organization has no recursive group relation  that is  a user A is a member of group G   and group G  is a member of group G    we can use this properties to avoid the second query  which can potentially run very slow  I propose that we add a configuration to only enable this feature for users who want to reduce group resolution time and who does not have recursive groups  so that existing behavior will not be broken,Faster LDAP group name resolution with ActiveDirectory
The HBase s HMaster port number conflicts with Hadoop kms port number  Both uses        There might be use cases user need kms and HBase present on the same cluster  The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories  Users would have to manually override the default port of either application on their cluster  It would be nice to have different default ports so kms and HBase could naturally coexist,Change kms server port number which conflicts with HMaster port number
Java   supports TLSv    and TLSv     which are more secure than TLSv   which was all that was supported in Java     so we should add those to the default list for   hadoop ssl enabled protocols,Enable TLS v    and
Let s pull the shell code out of the hadoop dist pom xml,pull shell code out of hadoop dist
As hadoop tools grows bigger and bigger  it s becoming evident that having a single directory that gets sucked in is starting to become a big burden as the number of tools grows  Let s rework this to be smarter,Rework hadoop tools
As discussed in mailing list  this will disable style checks in class setters like the following,Disable hiding field style checks in class setters
When o a h record was moved  bin rcc was never updated to pull those classes from the streaming jar,Remove bin rcc script
gridmix shouldn t require a raw java command line to run,add a subcommand for gridmix
We should use ResourceManager and NodeManager instead of JobTracker and TaskTracker,Remove MRv  terms from HttpAuthentication md
Update Yetus to,Update Yetus to
In ipc Client  the underlying mechanism is already supporting asynchronous calls    the calls shares a connection  the call requests are sent using a thread pool and the responses can be out of order  Indeed  synchronous call is implemented by invoking wait   in the caller thread in order to wait for the server response  In this JIRA  we change ipc Client to support asynchronous mode  In asynchronous mode  it return once the request has been sent out but not wait for the response from the server,Change ipc Client to support asynchronous calls
As discussed in the mailing list  we d like to introduce Apache Kerby into Hadoop  Initially it s good to start with upgrading Hadoop MiniKDC with Kerby offerings  Apache Kerby  https   github com apache directory kerby   as an Apache Directory sub project  is a Java Kerberos binding  It provides a SimpleKDC server that borrowed ideas from MiniKDC and implemented all the facilities existing in MiniKDC  Currently MiniKDC depends on the old Kerberos implementation in Directory Server project  but the implementation is stopped being maintained  Directory community has a plan to replace the implementation using Kerby  MiniKDC can use Kerby SimpleKDC directly to avoid depending on the full of Directory project  Kerby also provides nice identity backends such as the lightweight memory based one and the very simple json one for easy development and test environments,Upgrade Hadoop MiniKDC with Kerby
Currently back off policy from HADOOP       is hard coded to base on whether call queue is full  This ticket is open to allow flexible back off policies such as moving average of response time in RPC calls of different priorities,Allow RPC scheduler callqueue backoff using response times
Umbrella for converting hadoop  hdfs  mapred  and yarn to allow for dynamic subcommands  See first comment for more details,Umbrella  Dynamic subcommands for hadoop shell scripts
Currently the dfs  test command only supports  d   e   f   s   z options  It would be helpful if we add  w   r to verify permission of r w before actual read or write  This will help script programming,Add  w  r options in dfs  test command
HADOOP      added a ShutdownHookManager to be used by different components instead of the JVM shutdownhook  For each of the shutdown hook registered  we currently don t have an upper bound for its execution time  We have seen namenode failed to shutdown completely  waiting for shutdown hook to finish after failover  for a long period of time  which breaks the namenode high availability scenarios  This ticket is opened to allow specifying a timeout value for the registered shutdown hook,ShutdownHookManager should have a timeout for each of the Registered shutdown hook
In async RPC  if the callers don t read replies fast enough  the buffer storing replies could be used up  This is to propose limiting the number of outstanding async calls to eliminate the issue,Limit the number of outstanding async calls
As per  comment https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           from   wheat   in HADOOP        Need to remove FileUtil copyMerge  CC  to   wheat,Remove FileUtil copyMerge
This allows metrics collector such as AMS to collect it with MetricsSink  The per user RPC call counts  schedule decisions and per priority response time will be useful to detect and trouble shoot Hadoop RPC server such as Namenode overload issues,Support MetricsSource interface for DecayRpcScheduler Metrics
Cross Frame Scripting  XFS  prevention for UIs can be provided through a common servlet filter  This filter will set the X Frame Options HTTP header to DENY unless configured to another valid setting  There are a number of UIs that could just add this to their filters as well as the Yarn webapp proxy which could add it for all it s proxied UIs   if appropriate,Add XFS Filter for UIs to Hadoop Common
The jira proposes an improvement over HADOOP       to remove webhdfs dependencies from the ADL file system client and build out a standalone client  At a high level  this approach would extend the Hadoop file system class to provide an implementation for accessing Azure Data Lake  The scheme used for accessing the file system will continue to be adl    azuredatalake net path to file  The Azure Data Lake Cloud Store will continue to provide a webHDFS rest interface  The client will access the ADLS store using WebHDFS Rest APIs provided by the ADLS store,Refactor Azure Data Lake Store as an independent FileSystem
Currently FileSystem Statistics exposes the following statistics  BytesRead BytesWritten ReadOps LargeReadOps WriteOps These are in turn exposed as job counters by MapReduce and other frameworks  There is logic within DfsClient to map operations to these counters that can be confusing  for instance  mkdirs counts as a writeOp  Proposed enhancement  Add a statistic for each DfsClient operation including create  append  createSymlink  delete  exists  mkdirs  rename and expose them as new properties on the Statistics object  The operation specific counters can be used for analyzing the load imposed by a particular job on HDFS  For example  we can use them to identify jobs that end up creating a large number of files  Once this information is available in the Statistics object  the app frameworks like MapReduce can expose them as additional counters to be aggregated and recorded as part of job summary,Add a new interface for retrieving FS and FC Statistics
LdapGroupsMapping   currently does not set timeouts on the LDAP queries  This can create a risk of a very long infinite wait on a connection,Support timeouts in LDAP queries in LdapGroupsMapping
We want to rename       to       alpha  for the first alpha release  However  the version number is also encoded outside of the pom xml s  so we need to update these too,Change project version from       to       alpha
After DistCp copies a file  it calls   getFileStatus   to get the   FileStatus   from the destination so that it can compare to the source and update metadata if necessary  If the DistCp command was run without the option to preserve metadata attributes  then this additional   getFileStatus   call is wasteful,In DistCp  prevent unnecessary getFileStatus call when not preserving metadata
We should slim down the Docker image by removing JDK  now that trunk no longer supports it,remove JDK  from Dockerfile
Currently  the Future returned by ipc async call only support Future get   but not Future get timeout  unit   We should support the latter as well,Support Future get with timeout in ipc async calls
The hadoop ant code is an ancient kludge unlikely to have any users  still  We can delete it from trunk as a  scream test  for   x,Remove hadoop ant from hadoop tools
Upgrade yetus wrapper to be       now that it has passed vote,Upgrade to Apache Yetus
slaves sh and the slaves file should get replace with workers sh and a workers file,replace slaves with workers
This is a follow up jira from HADOOP           Now with the findbug warning     As discussed in HADOOP        bq  Why was this committed with a findbugs errors rather than adding the necessary plumbing in pom xml to make it go away  we will add the findbugsExcludeFile xml and will get rid of this given kerby       rc  release     Add the kerby version hadoop project pom xml bq  hadoop project pom xml contains the dependencies of all libraries used in all modules of hadoop  under dependencyManagement  Only here version will be mentioned  All other Hadoop Modules will inherit hadoop project  so all submodules will use the same version  In submodule  version need not be mentioned in pom xml  This will make version management easier,Follow on fixups after upgraded mini kdc using Kerby
In current Async DFS implementation  file system calls are invoked and returns Future immediately to clients  Clients call Future get to retrieve final results  Future get internally invokes a chain of callbacks residing in ClientNamenodeProtocolTranslatorPB  ProtobufRpcEngine and ipc Client  The callback path bypasses the original retry layer logic designed for synchronous DFS  This proposes refactoring to make retry also works for Async DFS,Support async call retry and failover
This JIRA is to address  Jing s comments https   issues apache org jira browse HADOOP       focusedCommentId          page com atlassian jira plugin system issuetabpanels comment tabpanel comment           in HADOOP,AsyncCallHandler should use an event driven architecture to handle async calls
In HADOOP       the Guava cache was introduced to allow refreshes on the Namenode group cache to run in the background  avoiding many slow group lookups  Even with this change  I have seen quite a few clusters with issues due to slow group lookups  The problem is most prevalent in HA clusters  where a slow group lookup on the hdfs user can fail to return for over    seconds causing the Failover Controller to kill it  The way the current Guava cache implementation works is approximately     On initial load  the first thread to request groups for a given user blocks until it returns  Any subsequent threads requesting that user block until that first thread populates the cache     When the key expires  the first thread to hit the cache after expiry blocks  While it is blocked  other threads will return the old value  I feel it is this blocking thread that still gives the Namenode issues on slow group lookups  If the call from the FC is the one that blocks and lookups are slow  if can cause the NN to be killed  Guava has the ability to refresh expired keys completely in the background  where the first thread that hits an expired key schedules a background cache reload  but still returns the old value  Then the cache is eventually updated  This patch introduces this background reload feature  There are two new parameters     hadoop security groups cache background reload   default false to keep the current behaviour  Set to true to enable a small thread pool and background refresh for expired keys    hadoop security groups cache background reload threads   only relevant if the above is set to true  Controls how many threads are in the background refresh pool  Default is    which is likely to be enough,Reload cached groups in background after expiry
Current   ViewFileSystem   does not support storage policy related API  it will throw   UnsupportedOperationException,ViewFileSystem should support storage policy related API
,Update maven enforcer plugin version to
Big features like YARN      demonstrate that even senior level Hadoop developers forget that daemons need a custom  OPTS env var  We can replace all of the custom vars with generic handling just like we do for the username check  For example  with generic handling in place     Old Var    New Var      HADOOP NAMENODE OPTS   HDFS NAMENODE OPTS     YARN RESOURCEMANAGER OPTS   YARN RESOURCEMANAGER OPTS     n a   YARN TIMELINEREADER OPTS     n a   HADOOP DISTCP OPTS     n a   MAPRED DISTCP OPTS     HADOOP DN SECURE EXTRA OPTS   HDFS DATANODE SECURE EXTRA OPTS     HADOOP NFS  SECURE EXTRA OPTS   HDFS NFS  SECURE EXTRA OPTS     HADOOP JOB HISTORYSERVER OPTS   MAPRED HISTORYSERVER OPTS   This makes it  a  consistent across the entire project b  consistent for every subcommand c  eliminates almost all of the custom appending in the case statements It s worth pointing out that subcommands like distcp that sometimes need a higher than normal client side heapsize or custom options are a huge win  Combined with  hadooprc and or dynamic subcommands  it means users can easily do customizations based upon their needs without a lot of weirdo shell aliasing or one line shell scripts off to the side,Deprecate HADOOP SERVERNAME OPTS  replace with  command   subcommand  OPTS
Right now the git repo has a branch named  master  in addition to our  trunk  branch  Since  master  is the common place name of the  most recent  branch in git repositories  this is misleading to new folks  It looks like the branch is from     months ago  We should remove it,delete spurious  master  branch
Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients  We are currently using version       of the SDK  Version       brings some breaking changes  Need to fix code to resolve all these breaking changes and certify that everything works properly,Update WASB driver to use the latest version         of SDK for Microsoft Azure Storage Clients
In branch     and later  the patches for various child and related bugs listed in HADOOP        most recently including HADOOP        HADOOP        HADOOP        HADOOP        and HDFS        eliminate all use of  commons httpclient  from Hadoop and its sub projects  except for hadoop tools hadoop openstack  see HADOOP         However  after incorporating these patches   commons httpclient  is still listed as a dependency in these POM files    hadoop project pom xml   hadoop yarn project hadoop yarn hadoop yarn registry pom xml We wish to remove these  but since commons httpclient is still used in many files in hadoop tools hadoop openstack  we ll need to  add  the dependency to   hadoop tools hadoop openstack pom xml  We ll add a note to HADOOP       to undo this when commons httpclient is removed from hadoop openstack   In      this was mostly done by HADOOP        but the version info formerly inherited from hadoop project pom xml also needs to be added  so that is in the branch     version of the patch  Other projects with undeclared transitive dependencies on commons httpclient  previously provided via hadoop common or hadoop client  may find this to be an incompatible change  Of course that also means such project is exposed to the commons httpclient CVE  and needs to be fixed for that reason as well,remove unneeded commons httpclient dependencies from POM files in Hadoop and sub projects
Currently  KMS audit log is using log j  to write a text format log  We should refactor this  so that people can easily add new format audit logs  The current text format log should be the default  and all of its behavior should remain compatible,Allow pluggable audit loggers in KMS
Umbrella jira for y  optimizations to reduce object allocations  more efficiently use protobuf APIs  unified ipc and webhdfs callq to enable QoS  etc,IPC layer optimizations
We re cleaning up Hive and Spark s use of FileSystem exists  because it is often the case we see code of exists open  exists delete  when the exists probe is needless  Against object stores  expensive needless  Hadoop can set an example here by stripping them out  It will also show where there are opportunities to optimise things better and or improve reporting,Eliminate needless uses of FileSystem  exists    isFile    isDirectory
Shell java has a hardcoded path to  bin bash which is not correct on all platforms  Pointed out by   aw  while reviewing HADOOP,Remove hardcoded absolute path for shell executable
The RPC layer supports QoS but other protocols  ex  webhdfs  are completely unconstrained  Generalizing   Server Call   to be extensible with simple changes to the handlers will enable unifying the call queue for multiple protocols,Design Server Call to be extensible for unified call queue
Introduce an AutoCloseableLock class that is a thin wrapper over RentrantLock  It allows using RentrantLock with try with resources syntax  The wrapper functions perform no expensive operations in the lock acquire release path,Add an AutoCloseableLock class
This patch adds to fs shell Stat java the missing options of  a and  A  FileStatus already contains the getPermission   method required for returning symbolic permissions  FsPermission contains the method to return the binary short  but nothing to present in standard Octal format  Most UNIX admins base their work on such standard octal permissions  Hence  this patch also introduces one tiny method to translate the toShort   return into octal  Build has already passed unit tests and javadoc,Add  A and  a formats for fs  stat command to print permissions
Leveraging HADOOP       will allow non rpc calls to be added to the call queue  This is intended to support routing webhdfs calls through the call queue to provide a unified and protocol independent QoS,Support external calls in the RPC call queue
ZStandard  https   github com facebook zstd has been used in production for   months by facebook now  v    was recently released  Create a codec for this library,Add Codec for ZStandard Compression
Based on discussion at YETUS      this code can t go there  but it s still very useful for release managers  A similar variant of this script has been used for a while by Apache HBase and Apache Kudu  and IMO JACC output is easier to understand than JDiff,Incorporate checkcompatibility script which runs Java API Compliance Checker
The UGI has a background thread to renew the tgt  On exception  it  terminates itself https   github com apache hadoop blob bee f  f ca f   ade   c fd  b dad  a     hadoop common project hadoop common src main java org apache hadoop security UserGroupInformation java L     L      If something temporarily goes wrong that results in an IOE  even if it recovered no renewal will be done and client will eventually fail to authenticate  We should retry with our best effort  until tgt expires  in the hope that the error recovers before that,Retry until TGT expires even if the UGI renewal thread encountered exception
add a metadata file giving the FS impl of swift  remove the entry from core default xml,swift FS to add a service load metadata file
We re currently pulling in version       I think we should upgrade to the latest,Upgrade commons configuration version to
We re currently pulling in version       incubating   I think we should upgrade to the latest       incubating,Upgrade HTrace version
As work continues on HADOOP        it s become evident that we need better hooks to start daemons as specifically configured users  Via the  command   subcommand  USER environment variables in   x  we actually have a standardized way to do that  This in turn means we can make the sbin scripts super functional with a bit of updating    Consolidate start dfs sh and start secure dns sh into one script   Make start    sh and stop    sh know how to switch users when run as root   Undeprecate start stop all sh so that it could be used as root for production purposes and as a single user for non production users,Update scripts to be smarter when running with privilege
The newly added Kafka module defines the Kafka dependency as,Reduce Kafka dependencies in hadoop kafka module
Currently  downstream projects that want to integrate with different Hadoop compatible file systems like WASB and S A need to list dependencies on each one  This creates an ongoing maintenance burden for those projects  because they need to update their build whenever a new Hadoop compatible file system is introduced  This issue proposes adding a new artifact that transitively includes all Hadoop compatible file systems  Similar to hadoop client  this new artifact will consist of just a pom xml listing the individual dependencies  Downstream users can depend on this artifact to sweep in everything  and picking up a new file system in a future version will be just a matter of updating the Hadoop dependency version,Provide a unified dependency artifact that transitively includes the cloud storage modules shipped with Hadoop
We generate source code with line numbers for inclusion in the javadoc JARs  Given that there s github and other online viewers  this doesn t seem so useful these days  Disabling the  linkSource  option saves us   MB for the hadoop common javadoc jar,Stop bundling HTML source code in javadoc JARs
Add a new instrumented read write lock in hadoop common  so that the HDFS      can use this to improve the locking in FsDatasetImpl,Add a new instrumented read write lock
Per discussion on HADOOP        I d like to revert HADOOP        It removes a deprecated API  but the   x line does not have a release with the new replacement API  This places a burden on downstream applications,Revert HADOOP       Remove unused TrashPolicy getInstance and initialize code
The runCommand code in Shell java can get into a situation where it will ignore InterruptedExceptions and refuse to shutdown due to being in I O waiting for the return value of the subprocess that was spawned  We need to allow for the subprocess to be interrupted and killed when the shell process gets killed  Currently the JVM will shutdown and all of the subprocesses will be orphaned and not killed,Ability to clean up subprocesses spawned by Shell when the process exits
To make our tests robust against timing problems and eventual consistent stores  we need to do more spin   wait for state  We have some   we ve examples to follow  Some of that work has been reimplemented slightly in   S ATestUtils eventually   I propose adding a class in the test tree    Eventually   to be a successor replacement for these    has an eventually waitfor operation taking a predicate that throws an exception   has an  evaluate  exception which tries to evaluate an answer until the operation stops raising an exception   again  from scalatest    plugin backoff strategies  from Scalatest  lets you do exponential as well as linear    option of adding a special handler to generate the failure exception  e g  run more detailed diagnostics for the exception text  etc     be Java   lambda expression friendly   be testable and tested itself,Add LambdaTestUtils class for tests  fix eventual consistency problem in contract test setup
The DiskChecker class has a few unused public methods  We can remove them,Cleanup DiskChecker interface
DiskChecker can fail to detect total disk controller failures indefinitely  We have seen this in real clusters  DiskChecker performs simple permissions based checks on directories which do not guarantee that any disk IO will be attempted  A simple improvement is to write some data and flush it to the disk,DiskChecker should perform some disk IO
To track user level connections  How many connections for each user  in busy cluster where so many connections to server,Expose  NumOpenConnectionsPerUser  as a metric
Currently the   MutableRates   metrics class serializes all writes to metrics it contains because of its use of   MetricsRegistry add      i e   even two increments of unrelated metrics contained within the same   MutableRates   object will serialize w r t  each other   This class is used by   RpcDetailedMetrics    which may have many hundreds of threads contending to modify these metrics  Instead we should allow updates to unrelated metrics objects to happen concurrently  To do so we can let each thread locally collect metrics  and on a   snapshot    aggregate the metrics from all of the threads  I have collected some benchmark performance numbers in HADOOP        https   issues apache org jira secure attachment          benchmark results  which indicate that this can bring significantly higher performance in high contention situations,Make MutableRates metrics thread local write  aggregate on read
Shell java has a hardcoded path to  bin ls which is not correct on all platforms  eg  not on NixOS   see HADOOP       for a similar issue,Remove hardcoded absolute path for ls
Currently SequenceFiles put in sync blocks every      bytes  It would be much better if it was configurable with a much higher default   mb or so,The distance between sync blocks in SequenceFiles should be configurable
KMS and HttpFS currently uses Tomcat         propose to upgrade to the latest version is,Upgrade Tomcat to
This is the KMS part  Please refer to HDFS       for the design doc,Add reencryptEncryptedKey interface to KMS
Current implementation of WASB  only supports Azure storage keys and SAS key being provided via org apache hadoop conf Configuration  which results in these secrets residing in the same address space as the WASB process and providing complete access to the Azure storage account and its containers  Added to the fact that WASB does not inherently support ACL s  WASB is its current implementation cannot be securely used for environments like secure hadoop cluster  This JIRA is created to add a new mode in WASB  which operates on Azure Storage SAS keys  which can provide fine grained timed access to containers and blobs  providing a segway into supporting WASB for secure hadoop cluster  More details about the issue and the proposal are provided in the design proposal document,Azure  Add a new SAS key mode for WASB
ViewFileSystem doesn t override FileSystem getLinkTarget    So  when view filesystem is used to resolve the symbolic links  the default FileSystem implementation throws UnsupportedOperationException  The proposal is to define getLinkTarget   for ViewFileSystem and invoke the target FileSystem for resolving the symbolic links  Path thus returned is preferred to be a viewfs qualified path  so that it can be used again on the ViewFileSystem handle,Implement getLinkTarget for ViewFileSystem
Azure Data Lake Store File System dependent Azure Data Lake Store SDK is released and has not need for further snapshot version dependency  This JIRA removes the SDK snapshot dependency to released SDK candidate  There is not functional change in the SDK and no impact to live contract test,Remove snapshot version of SDK dependency from Azure Data Lake Store File System
Currently we have one command to get state of namenode     It will be good to have command which will give state of all the namenodes,Add haadmin  getAllServiceState option to get the HA state of all the NameNodes ResourceManagers
The FTP transfer mode used by FTPFileSystem is BLOCK TRANSFER MODE  FTP Data connection mode used by FTPFileSystem is ACTIVE LOCAL DATA CONNECTION MODE  This jira makes them configurable,Make FTPFileSystem s data connection mode and transfer mode configurable
Read ADLS credentials using Hadoop CredentialProvider API  See https   hadoop apache org docs current hadoop project dist hadoop common CredentialProviderAPI html,Read ADLS credentials from Credential Provider
ADLS has multiple upgrades since the version        we are using                and        Change list  https   github com Azure azure data lake store java blob master CHANGES md,Update ADLS SDK to
the share hadoop  component  template directories are from when RPM and such were built as part of the build system  that no longer happens and now those files cause more harm than good since they are in the classpath  let s remove them,Remove vestigal templates directories creation
Other people aren t seeing this  yet    but unless you explicitly exclude v     of commons lang  from the azure build  which HADOOP       does   then the dependency declaration of commons lang  v       is creating a resolution conflict  That s a dependency only needed for the local dynamodb   tests  I propose to fix this in s guard by explicitly declaring the version used in the tests to be that of the azure storage one  excluding that you get for free  It doesn t impact anything shipped in production  but puts the hadoop build in control of what versions of commons lang are coming in everywhere by way of the commons config version declared in hadoop common,explicitly declare the commons lang  dependency as
Doing some Tomcat performance tuning on a loaded cluster  we found that   acceptCount      acceptorThreadCount    and   protocol   can be useful  Let s make these configurable in the kms startup script  Since the KMS is Jetty in   x  this is targeted at just branch,Make additional KMS tomcat settings configurable
is missing in pom xml  That way   mvn versions set   does not work for the project,Missing hadoop cloud storage project module in pom xml
It would be nice to have a utility that looked at the first N bytes of a file and picked a decoder for it into Strings  It could then be hooked up to  bin hadoop fs cat  and the web ui to textify sequence and compressed files,Create a utility to convert binary  sequence and compressed  files to strings
Unlike gzip  the bzip file format supports splitting  Compression is by blocks     k by default  and blocks are separated by a synchronization marker  a    bit approximation of Pi   This would permit very large compressed files to be split into multiple map tasks  which is not currently possible unless using a Hadoop specific file format,want InputFormat for bzip  files
Hudson should kill long running tests   I believe it is supposed to but doesn t quite seem to do the job if the test is really hung up   It would be nice if  when the timer goes off  Hudson did a     See the section  Killing a hung test  at http   wiki apache org lucene hadoop HudsonBuildServer,Hudson should kill long running tests
Currently max queue size for IPC server is set to        handlers   Usually when RPC failures are observed  e g  HADOOP        we increase number of handlers and the problem goes away  I think a big part of such a fix is increase in max queue size  I think we should make maxQsize per handler configurable  with a bigger default than       There are other improvements also  HADOOP        Server keeps reading RPC requests from clients  When the number in flight RPCs is larger than maxQsize  the earliest RPCs are deleted  This is the main feedback Server has for the client  I have often heard from users that Hadoop doesn t handle bursty traffic  Say handler count is     default  and Server can handle      RPCs a sec  quite conservative low for a typical server   it implies that an RPC can wait for only for   sec before it is dropped  If there      clients and all of them send RPCs around the same time  not very rare  with heartbeats etc        will be dropped  In stead of dropping the earliest RPCs  if the server delays reading new RPCs  the feedback to clients would be much smoother  I will file another jira regd queue management  For this jira I propose to make queue size per handler configurable  with a larger default  may be,IPC server max queue size should be configurable
The metrics system in the JobTracker is defaulting to every   seconds computing all of the counters for all of the jobs  This work is a substantial amount of work showing up as running in     of the snapshots that I ve seen  I d like to lower the default interval to once every    seconds and make it a low priority thread,the metrics system in the job tracker is running too often
A user was moving from      to      and was invoking randomwriter with a config on the command line like  bin hadoop jar hadoop   examples jar randomwriter output conf xml which worked in       but in      it ignores the conf xml without complaining  The equivalent is bin hadoop jar hadoop   examples jar randomwriter  conf conf xml output,randomwriter should complain if there are too many arguments
Currently when a block is transfered to a data node the client interleaves data chunks with the respective checksums  This requires creating an extra copy of the original data in a new buffer interleaved with the crcs  We can avoid extra copying if the data and the crc are fed to the socket one after another,Non interleaved checksums would optimize block transfers
Move hbase out of hadoop core  Move its JIRA issues and move it in svn from https   svn apache org repos asf hadoop core trunk src contrib hbase to https   svn apache org repos asf hadoop hbase trunk,Move hbase out of hadoop core
This jira is intended to enhance IPC s scalability and robustness  Currently an IPC server can easily hung due to a disk failure or garbage collection  during which it cannot respond to the clients promptly  This has caused a lot of dropped calls and delayed responses thus many running applications fail on timeout  On the other side if busy clients send a lot of requests to the server in a short period of time or too many clients communicate with the server simultaneously  the server may be swarmed by requests and cannot work responsively  The proposed changes aim to   provide a better client server coordination    Server should be able to throttle client during burst of requests     A slow client should not affect server from serving other clients     A temporary hanging server should not cause catastrophic failures to clients    Client server should detect remote side failures  Examples of failures include      the remote host is crashed      the remote host is crashed and then rebooted      the remote process is crashed or shut down by an operator    Fairness  Each client should be able to make progress,Improve the Scalability and Robustness of IPC
this is something that we have implemented in the application layer   may be useful to have in hadoop itself   long term log storage systems often keep data sorted  by some sort key   future computations on such files can often benefit from this sort order  if the job requires grouping by the sort key   then it should be possible to do reduction in the map stage itself  this is not natively supported by hadoop  except in the degenerate case of   map file per task  since splits can span the sort key  however aligning the data read by the map task to sort key boundaries is straightforward   and this would be a useful capability to have in hadoop  the definition of the sort key should be left up to the application  it s not necessarily the key field in a Sequencefile  through a generic interface   but otherwise   the sequencefile and text file readers can use the extracted sort key to align map task data with key boundaries,align map splits on sorted files with key boundaries
Checkpoint to verify the fsimage each time it creates the new one,Keep two generations of fsimage
Currently  only way to find files with all replica being corrupt is when we read those files  Instead  can we have fsck report those   Using the corrupted blocks found by the periodic verification,fsck to show  checksum  corrupted files
Utility will collapse the contents of a directory into a small number of files,compaction utility for directories
Should we change the hash function for Text to something that handles non ascii characters better  http   bailey svn sourceforge net viewvc bailey trunk src java org apache bailey util Hash java view markup,Replace Text hashcode with a better hash function for non ascii strings
SequenceFile s block compression format is too complex and requires   codecs to compress or decompress  It would be good to have a file format that only needs,New binary file format
Besides the default JT scheduling algorithm  there is work going on with at least two more schedulers  HADOOP       HADOOP        HADOOP      makes it easier to plug in new schedulers into the JT  Where do we place the source files for various schedulers so that it s easy for users to choose their scheduler of choice during deployment  and easy for developers to add in more schedulers into the framework  without inundating it,Hadoop Core should support source filesfor multiple schedulers
Problem  We need to have Object  Serialization Deserialization  support for TFile,ObjectFile on top of TFile
simplify Parsers implementation   add map and reduce side to the demux   add dynamic link between RecordType and Parsers using configuration file and alias   encapsulate data files creation location and naming convention inside the core demux classes   sort all data by TimePartition Machine Timestamp by default,Update Chukwa Demux process
Update Chukwa parsers,Update Chukwa parsers
In order to reduce the number of file on HDFS we need to have a rolling mechanism for the demux output   avoid immediate merging if there s already file for the same time range  create a spill file instead   merge all raw files every hours   merge all hourly files every days,Rolling mechanism for demux output
Create some utility classes to dump both Archive and ChukwaRecords files,Utility classes for Archive and ChukwaRecord files
An external application  an Ops script  or some CLI based tool  can change the configuration of the Capacity Scheduler  change the capacities of various queues  for example  by updating its config file  This application then needs to tell the Capacity Scheduler that its config has changed  which causes the Scheduler to re read its configuration  It s possible that the Capacity Scheduler may need to interact with external applications in other similar ways,Capacity Scheduler needs to re read its configuration
Ability to test endToEnd chukwa pipeline   From log file to dataSink   From dataSink to demux output,Chukwa test framework
,Add Hadoop native library to java library path  compression
In the FileTailingAdaptor  if when trying to read a file  a  File does not exist     Permission denied  exception is throws then that file should be log and removed,When the FileTailingAdaptor is unable to read a file it should take action instead of trying     times
Amongst other things  JUnit   has better support for class wide set up and tear down  via  BeforeClass and  AfterClass annotations   and more flexible assertions  http   junit sourceforge net doc ReleaseNotes    html   It would be nice to be able to take advantage of these features in tests we write  JUnit   can run tests written for JUnit       without any changes,Upgrade to JUnit
It would be useful to implement a JNI based runtime for Hadoop to get access to the native OS runtime  This would allow us to stop relying on exec ing bash to get access to information such as user groups  process limits etc  and for features such as chown chgrp  org apache hadoop util Shell,Implement a native OS runtime for Hadoop
When data need to be reprocessed in the database  there is currently no manual method to reload the chukwa sequence files into database  A few minor tweaks to MetricsDataLoader should be possible to create a command line utility to do this,Add utilities to load chukwa sequence file to database
,Split build script for building core  hdfs and mapred separately
,to optimize hudsonBuildHadoopNightly sh script
Watchdog is watching for ChukwaAgent only once every   minutes  so there s no point in retrying more than once every   mins  In practice  if the watchdog is not able to automatically restart the agent  it will take more than    minutes to get Ops to restart it  Also Ops want us to limit the number of communications between Hadoop and Chukwa  that s why    minutes,ChukwaAgent controller should retry to register for a longer period but not as frequent as now
already has quotas for HDFS namespace  HADOOP        HADOOP      implements similar quotas for disk space on HDFS in       This jira proposes to port HADOOP      to,Port HDFS space quotas to
to increase productivity in our current project  which makes a heavy use of Hadoop   we wrote a small Eclipse based GUI application which basically consists in   views    a HDFS explorer adapted from Eclipse filesystem explorer example  For now  it includes the following features  o classical tree based browsing interface  with directory content being detailed in a   columns table  file name  file size  file type  o refresh button o delete file or directory  with confirm dialog   select files in the tree or table and click the  Delete  button o rename file or directory  simple click on the file in the table  type the new name and validate o open file with system editor  select the file in the table and click  Open  button  works on Windows  not on Linux  o internal drag   drop o external drag   drop from the local filesystem to the HDFS  the opposite doesn t work    a MapReduce  very  simple job launcher  o select the job XML configuration file o run the job o kill the job o visualize map and reduce progress with progress bars o open a browser on the Hadoop job tracker web interface INSTALLATION NOTES    Eclipse       JDK       import the archive in Eclipse   copy your hadoop conf file  hadoop default xml in  src  folder     this step should be moved in the GUI later   right click on the project and Run As    Eclipse Application   enjoy,Eclipse based GUI  DFS explorer and basic Map Reduce job launcher
A specific sub case of the general priority inversion problem noted in HADOOP      is when many lower priority jobs are submitted and are waiting for mappers to free up  Even though they haven t actually done any work  they will be assigned any free reducers  If a higher priority job is submitted  priority inversion results not just due to the lower priority tasks that are in the midst of completing  but also due to the ones that haven t yet started but have claimed all the free reducers  A simple workaround is to require a job to complete some useful work before assigning it a reducer  This can be done in a tunable and backwards compatible manner by adding a  minimum map progress percentage before assigning a reducer  option to the JobConf  Setting this to   would eliminate the common case above  and setting it to     would technically eliminate the inversion of HADOOP       though likely at an unacceptably high cost,JobConf option for minimum progress threshold before reducers are assigned
In an effort to simplify the  in the capacity scheduler  We would reintroduce this  possibly with some revisions to the original design  after a while  This will be an incompatible change  Any objections,Remove pre emption from the capacity scheduler code base
The following tests are under the org apache hadoop fs package but were moved to hdfs sub directory by HADOOP            Some of them are not related to hdfs  e g  TestFTPFileSystem  These files should be moved out from hdfs and should not use hdfs codes    Some of them are testing hdfs features  e g  TestStickyBit  They should be defined under org apache hadoop hdfs package,fs tests should not be placed in hdfs
Adding an option to   FsShell stat   to get a file s block location information will be very useful  we can print the block location information in this format  blockID XXXXX byte range YYYY ZZZZ location dn  dn   blockID XXXXX byte range YYYY ZZZZ location dn  dn,Add a command to   FsShell stat   to get a file s block location information
We need some mechanism for RPC calls that get exceptions to automatically retry the call under certain circumstances  In particular  we often end up with calls to rpcs being wrapped with retry loops for timeouts  We should be able to make a retrying proxy that will call the rpc and retry in some circumstances,we need some rpc retry framework
Currently with quota turned on  user cannot call   rmr  on large directory that causes over quota     Besides from error message being unfriendly  how should this be handled,Handling of Trash with quota
There s a possibility that contributors and commiters might be checking their patches against a version of FindBugs which differs from one installed on Hudson  test patch script has to verify if the version of FindBugs is correct,test patch should verify that FindBugs version used for verification is correct one
For HDFS     we need to use unix domain sockets  This JIRA is to include a library in common which adds a o a h net unix package based on the code from Android  apache   license,Add support for unix domain sockets to JNI libs
Configuration objects send a DEBUG level log message every time they re instantiated  which include a full stack trace  This is more appropriate for TRACE level logging  as it renders other debug logs very hard to read,Configuration sends too much data to log j
FileSystem should have mkdir and create file apis which do not create parent path  The usecase is illustrated at  this https   issues apache org jira browse HDFS    focusedCommentId          page com atlassian jira plugin system issuetabpanels  Acomment tabpanel action,FileSystem should have mkdir and create file apis which do not create parent path
Common tests are functional tests or end to end  It makes sense to have Mockito framework for the convenience of true unit tests development,Add unit tests framework  Mockito
The way metrics are currently exposed to the JMX in the NameNode is not helpful  since only the current counters in the record can be fetched and without any context those number mean little  For example the number of files created equal to     only means that in the last period there were     files created but when the new period will end is unknown so fetching     again will either mean another     files or we are fetching the same time period  One of the solutions for this problem will be to have a JMX context that will accumulate the data  being child class of AbstractMetricsContext  and expose different records to the JMX through custom MBeans  This way the information fetched from the JMX will represent the state of things in a more meaningful way,JMX Context for Metrics
We need a configurable mapping from full user names  eg  omalley APACHE ORG  to local user names  eg  omalley   For many organizations it is sufficient to just use the prefix  however  in the case of shared clusters there may be duplicated prefixes  A configurable mapping will let administrators resolve the issue,Need mapping from long principal names to local OS user names
The UserGroupInformation should contain authentication method in its subject  This will be used in HDFS to issue delegation tokens only to kerberos authenticated clients,UGI should contain authentication method
When token is used for authentication over RPC  information other than username may be needed for access authorization  This information is typically specified in TokenIdentifier  This is especially true for block tokens used for client to datanode accesses  where authorization is based on access permissions specified in TokenIdentifier  and not on username  Block tokens used to be called access tokens and one can think of them as capability tokens  See HADOOP      for more info,Add authenticated TokenIdentifiers to UGI so that they can be used for authorization
Now that we are adding the serialized form of delegation tokens into the http interfaces  we should include some version information,Should add version to the serialization of DelegationToken
Currently the Maven POM file is generated from a template file that includes the versions of all the libraries we depend on  The versions of these libraries are also present in ivy libraries properties  so that  when a library is updated  it must be updated in two places  which is error prone  We should instead only specify library versions in a single place,versions of dependencies should be specified in a single place
Per discussions with Arun  Chris  Hong and Rajiv  et al  we concluded that the current metrics framework needs an overhaul to    Allow multiple plugins for different monitoring systems simultaneously  see also  HADOOP          Refresh metrics plugin config without server restart     Including filtering of metrics per plugin    Support metrics schema for plugins  The jira will be resolved when core hadoop components  hdfs  mapreduce  are updated to use the new framework   Updates to external components that use the existing metrics framework will be tracked by different issues   The current design wiki http   wiki apache org hadoop HADOOP      MetricsV,Overhaul metrics framework
Whenever a new patch is submitted for verification   test patch   process has to make sure that none of Herriot bindings were broken,test patch needs to verify Herriot integrity
As Hadoop widespreads and matures the number of tools and utilities for users keeps growing  Some of them are bundled with Hadoop core  some with Hadoop contrib  some on their own  some are full fledged servers on their own  For example  just to name a few  distcp  streaming  pipes  har  pig  hive  oozie  Today there is no standard mechanism for making these tools available to users  Neither there is a standard mechanism for these tools to integrate and distributed them with each other  The lack of a common foundation creates issues for developers and users,Common foundation for Hadoop client tools
While working HADOOP       I noticed that our metrics naming style is all over the place    Capitalized camel case  e g    FilesCreated  in namenode metrics and some rpc metrics   uncapitalized camel case  e g   threadsBlocked  in jvm metrics and some rpc metrics   lowercased underscored  e g    bytes written  in datanode metrics and mapreduce metrics Let s make them consistent  How about uncapitalized camel case  My main reason for the camel case  some backends have limits on the name length and underscore is wasteful  Once we have a consistent naming style we can do   Metric  Number of INodes created   MutableCounterLong filesCreated  instead of the more redundant   Metric   FilesCreated    Number of INodes created    MutableCounterLong filesCreated,Make metrics naming consistent
We should allow users to use the more compact form of xml elements  For example  we could allow     The old format would also be supported,Allow compact property description in xml
New file system API  HADOOP       should implement security features currently provided by FileSystem APIs This is a critical requirement for MapReduce components to migrate and use new APIs for internal filesystem operations  MAPREDUCE,security implementation for new FileSystem  FileContext  API
DaemonFactory class is defined in hdfs util  common would be a better place for this class,DaemonFactory should be moved from HDFS to common
Our project  Pig  exposes FsShell functionality to our end users through a shell command  We want to use this command with no modifications to make sure that whether you work with HDFS through Hadoop or Pig you get identical semantics  The main concern that has been recently raised by our users is that there is no way to ignore certain failures that they consider to be benign  for instance  removing a non existent directory  We have   asks related to this issue      Meaningful error code returned from FsShell  we use java class  so that we can take different actions on different errors     Unix like ways to tell the command to ignore certain behavior  Here are the commands that we would like to be expanded implemented    rm  f   rmdir    ignore fail on non empty   mkdir  p,Extensions to FsShell
The FsShell has many chains if then else chains for instantiating and running commands  A dynamic mechanism is needed for registering commands such that FsShell requires no changes when adding new commands,Add command factory to FsShell
,Deprecate metrics v
We should create a script that Hudson uses to execute test patch that is in source control so modifications to test patch sh arguments can be done w o updating Hudson  The script would execute the following  and take just the password as an argument,Create a test patch script for Hudson
When setting up a compression codec in an MR job the full class name of the codec must be used  To ease usability  compression codecs should be resolved by their codec name  ie  gzip    deflate    zlib    bzip    instead their full codec class name  Besides easy of use for Hadoop users who would use the codec alias instead the full codec class name  it could simplify how HBase resolves loads the codecs,Add capability to resolve compression codec based on codec name
When you have a key value class that s non Writable and you forget to attach io serializers for the same  an NPE is thrown by the tasks with no information on why or what s missing and what led to it  I think a better exception can be thrown by SerializationFactory instead of an NPE when a class is not found accepted by any of the loaded ones,When a serializer class is missing  return null  not throw an NPE
When an older Hadoop version tries to contact a newer Hadoop version across an IPC protocol version bump  the client currently just gets a non useful error message like  EOFException   Instead  the IPC server code can speak just enough of prior IPC protocols to send back a  fatal  message indicating the version mismatch,Send back nicer error to clients using outdated IPC version
,IPC Wire Compatibility
In HADOOP      and HDFS     it was agreed that FileSystem  listStatus should throw FileNotFoundException instead of returning null  when the target directory did not exist  However  in LocalFileSystem implementation today  FileSystem  listStatus still may return null  when the target directory exists but does not grant read permission  This causes NPE in many callers  for all the reasons cited in HADOOP      and HDFS      See HADOOP      and its linked issues for examples,FileSystem listStatus should throw IOE upon access error
In a nutshell  ls needs the ability to list a directory but not its contents  W o  d  it is impossible to list the root directory s owner  permissions  etc  See the original hdfs bug for details,hadoop dfs  ls   Do not expand directories  was HDFS
hadoop common src main bin hadoop config sh needs to be updated post MR   eg the layout of mapred home has changed,hadoop config sh needs to be updated post MR
hadoop common src main bin hadoop config sh needs to be updated post mavenization  eg it still refers to build classes etc,hadoop config sh needs to be updated post mavenization
See this thread  http   markmail org thread cxtz i lvztfgfxn We need to get things up and running for a top level hadoop tools module  DistCpV  will be the first resident of this new home  Things we need    The module itself and a top level pom with appropriate dependencies   Integration with the patch builds for the new module   Integration with the post commit and nightly builds for the new module,Set things up for a top level hadoop tools module
,create a script to setup application in order to create root directories for application such hbase  hcat  hive etc
,RPC Layer improvements to support protocol compatibility
Currently the check done in the  hasSufficientTimeElapsed    method is hardcoded to    mins wait  The wait time should be driven by configuration and its default value  for clients should be   min,Kerberos relogin interval in UserGroupInformation should be configurable
HDFS     added a new public API SequenceFile syncFs  we need to forward port this for compatibility  Looks like it might have introduced other APIs that need forward porting as well  eg LocaltedBlocks setFileLength  and DataNode getBlockInfo,Forward port SequenceFile syncFs and friends from Hadoop   x
Given that we are locked down to using only XML for configuration and most of the administrators need to manage it by themselves  unless a tool that manages for you is used   it would be good to also validate the provided config XML    site xml  files with a tool like   xmllint   or maybe Xerces somehow  when running a command or  at least  when starting up daemons  We should use this only if a relevant tool is available  and optionally be silent if the env  requests,Validate XMLs if a relevant tool is available  when using scripts
We should be able to start a kdc server for unit tests  so that security could be turned on  This will greatly improve the coverage of unit tests,Add capability to turn on security in unit tests
,Access Control support for Non secure deployment of Hadoop on Windows
The Syncable sync   was deprecated in       We should remove it,Remove the deprecated Syncable sync   method
One thing that the original patch for HADOOP      didn t address is the need for those curated jars to be visible in the final tarball,make hadoop client set of curated jars available in a distribution tarball
If there are known failures  test patch will bail out as soon as it sees them  This causes the precommit builds to potentially not find real issues with a patch  because the tests that would fail might come after a known failure  We should add  fn to just the mvn test command in test patch to get the full list of failures,test patch should run tests with  fn to avoid masking test failures
Currently when the ZK session expires  it results in a fatal error being sent to the application callback  This is not the best behavior    for example  in the case of HA  if ZK goes down  we would like the current state to be maintained  rather than causing either NN to abort  When the ZK clients are able to reconnect  they should sort out the correct leader based on the normal locking schemes,Improve ActiveStandbyElector s behavior when session expires
To keep the initial patches manageable  kerberos security is not currently supported in the ZKFC implementation  This JIRA is to support the following important pieces for security    integrate with ZK authentication  kerberos or password based    allow the user to configure ACLs for the relevant znodes   add keytab configuration and login to the ZKFC daemons   ensure that the RPCs made by the health monitor and failover controller properly authenticate to the target daemons,Security support for ZK Failover controller
HADOOP      introduces a configure flag to prevent potential status inconsistency between zkfc and namenode  by making auto and manual failover mutually exclusive  However  as described in       section of design doc at HDFS       we should allow manual and auto failover co exist  by    adding some rpc interfaces at zkfc   manual failover shall be triggered by haadmin  and handled by zkfc if auto failover is enabled,Auto HA  Allow manual failover to be invoked from zkfc
This is to track changes for restoring security in      branch,Restore security in Hadoop      branch
As of now   hadoop streaming uses old Hadoop M R API  This JIRA ports it to the new M R API,Port StreamInputFormat to new Map Reduce API
Because of reasons listed here  http   findbugs sourceforge net bugDescriptions html SE COMPARATOR SHOULD BE SERIALIZABLE comparators should be serializable  To make deserialization work  it is required that all superclasses have no arg constructor  http   findbugs sourceforge net bugDescriptions html SE NO SUITABLE CONSTRUCTOR Simply add no arg constructor to WritableComparator,WritableComparator must implement no arg constructor
Currently FileUtil unTar   spawns tar utility to do the work  Tar may not be present on all platforms by default eg  Windows  So changing this to use JAVA API s would help make it more cross platform  FileUtil unZip   uses the same approach,Change untar to use Java API on Windows instead of spawning tar process
Quantcast has released QFS      http   quantcast github com qfs   a C   distributed filesystem based on Kosmos File System KFS   QFS comes with various feature  performance  and stability improvements over KFS  A hadoop  fs  shim needs be added to support QFS through  qfs     URIs,Need to add fs shim to use QFS
This JIRA is to make handling of improperly constructed file URIs for Windows local paths more rigorous  e g  reject  file    c   Windows  Valid file URI syntax explained at http   blogs msdn com b ie archive            file uris in windows aspx  Also see https   issues apache org jira browse HADOOP,Reject invalid Windows URIs
The given example is if datanode disk definitions but should be applicable to all configuration where a list of disks are provided  I have defined multiple local disks defined for a datanode  dfs data dir  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn  data    dfs dn true When one of those disks breaks and is unmounted then the mountpoint  such as  data    in this example  becomes a regular directory which doesn t have the valid permissions and possible directory structure Hadoop is expecting  When this situation happens  the datanode fails to restart because of this while actually we have enough disks in an OK state to proceed  The only way around this is to alter the configuration and omit that specific disk configuration  To my opinion  It would be more practical to let Hadoop daemons start when at least   disks partition in the provided list is in a usable state  This prevents having to roll out custom configurations for systems which have temporarily a disk  and therefor directory layout  missing  This might also be configurable that at least X partitions out of he available ones are in OK state,Allow daemon startup when at least    or configurable  disk is in an OK state
Currently different IPC servers in Hadoop use the same config variables names starting with  ipc server   This makes it difficult and confusing to maintain configuration for different IPC servers,Support per server IPC configuration
Once   JarFinder getJar     is invoked by a client app  it would be really useful to destroy the generated JAR after the JVM is destroyed by setting   tempJar deleteOnExit      In order to preserve backwards compatibility a configuration setting could be implemented  e g    test build dir purge on exit,JarFinder getJar should delete the jar file upon destruction of the JVM
This JIRA is to define commands for Hadoop token  The scope of this task is highlighted as following    Token init  authenticate and request an identity token  then persist the token in token cache for later reuse    Token display  show the existing token with its info and attributes in the token cache    Token revoke  revoke a token so that the token will no longer be valid and cannot be used later    Token renew  extend the lifecycle of a token before it s expired,Hadoop Token Command
Add Python API for mllib fpm PrefixSpan,Add Python API for PrefixSpan
The stat functions are defined in http   spark apache org docs latest api scala index html org apache spark sql DataFrameStatFunctions  Currently only crosstab   is supported  Functions to be supported include  corr  cov  freqItems,Add support for DataFrameStatFunctions in SparkR
currently in SparkR  collect   on a DataFrame collects the data within the DataFrame into a local data frame  R users are used to using data frame  However  collect   currently can t collect data of nested types from a DataFrame because     The serializer in JVM backend does not support nested types     collect   in R side assumes each column is of simple atomic type that can be combinded into a atomic vector,Improve the implementation of collect   on DataFrame in SparkR
make MultilayerPerceptronClassifier layers and weights public,make MultilayerPerceptronClassifier layers and weights public
ML Evaluator currently requires that metrics be maximized  bigger is better   That is counterintuitive for some metrics  Currently  we hackily negate some metrics in RegressionEvaluator  which is weird  Instead  we should    Return the metric as expected  e g    rmse  should return RMSE  not its negation     Provide an indicator of whether the metric should be maximized or minimized  Model selection algorithms can use the indicator as needed,ML Evaluator should indicate if metric should be maximized or minimized
Right now  we have QualifiedTableName  TableIdentifier  and Seq String  to represent table identifiers  We should only have one form and looks TableIdentifier is the best one because it provides methods to get table name  database name  return unquoted string  and return quoted string  There will be TODOs having  SPARK        in it  Those places need to be updated,Consolidate different forms of table identifiers
Add a column function on a DataFrame like  ifelse  in R to SparkR  I guess we could implement it with a combination with   when   and   otherwise    h   Example If   df x       is TRUE  then return    otherwise return,Add  ifelse  Column function to SparkR
It is convenient to implement data source API for LIBSVM format to have a better integration with DataFrames and ML pipeline API     This JIRA covers the following     Read LIBSVM data as a DataFrame with two columns  label  Double and features  Vector     Accept  numFeatures  as an option     The implementation should live under  org apache spark ml source libsvm,Implement SQL data source API for reading LIBSVM data
In codegen  we didn t consider nullability of expressions  Once considering this  we can avoid lots of null check  reduce the size of generated code  also improve performance   Before that  we should double check the correctness of nullablity of all expressions and schema  or we will hit NPE or wrong results,Consider nullability of expression in codegen
In ML pipelines  each transformer estimator appends new columns to the input DataFrame  For example  it might produce DataFrames like the following columns  a  b  c  d  where a is from raw input  b   udf b a   c   udf c b   and d   udf d c   Some UDFs could be expensive  However  if we materialize c and d  udf b  and udf c are triggered twice  i e   value c is not re used  It would be nice to detect this pattern and re use intermediate values,Optimize sequential projections
Python s   since  is defined under  pyspark sql   It would be nice to move it under  pyspark  to be shared by all components,Move  since annotator to pyspark to be shared by all components
They don t bring much value since we now have better unit test coverage for hash joins  This will also help reduce the test time,Remove HashJoinCompatibilitySuite
Currently the method join right  DataFrame  usingColumns  Seq String   only supports inner join  It is more convenient to have it support other join types,Support to specify join type when calling join with usingColumns
This was recently released  and it has many improvements  especially the following   quote  Python side  IDEs and interactive interpreters such as IPython can now get help text autocompletion for Java classes  objects  and members  This makes Py J an ideal tool to explore complex Java APIs  e g   the Eclipse API   Thanks to  jonahkichwacoders  quote  Normally we wrap all the APIs in spark  but for the ones that aren t  this would make it easier to offroad by using the java proxy objects,Upgrade pyspark to use py j
Currently  There will be ConvertToSafe for PythonUDF  that s not needed actually,PythonUDF could process UnsafeRow
SPARK      need to generate random data which follow Weibull distribution,Add WeibullGenerator for RandomDataGenerator
We implemented dspr with sparse vector support in  RowMatrix   This method is also used in WeightedLeastSquares and other places  It would be useful to move it to  linalg BLAS,move RowMatrix dspr to BLAS
Really simple request  to upgrade fastutil to     x  The current version      x  has some minor API s in the Object xxOpenHashMap structures which is used in many places in Spark  and has been marked deprecated  Plus there is a conflict with another library we are using  saddle    http   saddle github io   which uses a newer version of fastutil  I d be happy to send a PR  I guess a bigger question is do you want to keep using fastutil  SPARK      but Spark uses more than just hashmaps  so that probably requires another discussion,Remove Fastutil
The name  weights  becomes confusing as we are supporting weighted instanced  As discussed in https   github com apache spark pull       we want to deprecate  weights  and use  coefficients  instead    Deprecate but do not remove  weights     Only make changes under  spark ml,deprecate weights and use coefficients instead in ML models
The BlockMatrix multiply sends each block to all the corresponding columns of the right BlockMatrix  even though there might not be any corresponding block to multiply with  Some optimizations we can perform are    Simulate the multiplication on the driver  and figure out which blocks actually need to be shuffled   Send the block once to a partition  and join inside the partition rather than sending multiple copies to the same partition,Decrease communication in BlockMatrix multiply and increase performance
Hive already supports this according to https   issues apache org jira browse HIVE        Currently Spark sql still supports only primitive types,collect list   and collect set   should accept struct types as argument
As of https   issues apache org jira browse SPARK       we no longer need to use our custom SCP based mechanism for archiving Jenkins logs on the master machine  this has been superseded by the use of a Jenkins plugin which archives the logs and provides public viewing of them  We should remove the legacy log syncing code  since this is a blocker to disabling Worker    Master SSH on Jenkins,Remove legacy SCP based Jenkins log archiving code
Bagel has been deprecated and we haven t done any changes to it  There is no need to run those tests,Remove Bagel test suites
After SPARK        we should add Python API for AFTSurvivalRegression,Python API for AFTSurvivalRegression
The following method in   LogisticRegressionModel   is marked as   private    which prevents users from creating a summary on any given data set  Check  here https   github com feynmanliang spark blob d   fa c   e f  b  a           d  cd     mllib src main scala org apache spark ml regression LinearRegression scala L         This method is definitely necessary to test model performance  By the way  the name   evaluate   is already pretty good for me    mengxr  Could you check this   Thx,Make Logistic  Linear Regression Model evaluate   method public
The idea is that most of the logic of calling Python actually has nothing to do with RDD  it is really just communicating with a socket    there is nothing distributed about it   and it is only currently depending on RDD because it was written this way  If we extract that functionality out  we can apply it to area of the code that doesn t depend on RDDs  and also make it easier to test,Refactor PythonRDD to decouple iterator computation from PythonRDD
Refactoring  Instance  case class out from LOR and LIR  and also cleaning up some code,Refactoring  Instance  out from LOR and LIR  and also cleaning up some code
SPARK      uses network module to implement RPC  However  there are some configurations named with  spark shuffle  prefix in the network module  We should refactor them and make sure the user can control them in shuffle and RPC separately,Separate configs between shuffle and RPC
Currently  we try to support multiple sessions in SQL within a Spark Context  but it s broken and not complete  We should isolate these for each session      current database of Hive    SQLConf    UDF UDAF UDTF    temporary table For added jar and cached tables  they should be accessible for all sessions,Improve session management for SQL
There is support for message handler in Direct Kafka Stream  which allows arbitrary T to be the output of the stream instead of Array Byte   This is a very useful function  therefore should exist in Kinesis as well,Add MessageHandler to KinesisUtils createStream similar to Direct Kafka
typeId is not needed in columnar cache  it s confusing to having them,Remove typeId in columnar cache
Matches more closely with ImperativeAggregate,Rename ExpressionAggregate    DeclarativeAggregate
The serialize   will be called by actualSize   and append    we should use UnsafeProjection before unrolling,Avoid the serialization multiple times during unrolling of complex types
The new netty RPC still behaves too much like akka  it requires both client  e g  an executor  and server  e g  the driver  to listen for incoming connections  That is not necessary  since sockets are full duplex and RPCs should be able to flow either way on any connection  Also  because the semantics of the netty based RPC don t exactly match akka  you get weird issues like SPARK        Supporting a client only mode also reduces the number of ports Spark apps need to use,Netty based RPC env should support a  client only  mode
The Transformer version of KMeansModel does not currently have methods to computeCost or get the centers of the cluster centroids  If there could be a way to get this either by exposing the parentModel or by adding these method it would make things easier,Add computeCost and clusterCenters to KMeansModel in spark ml package
We should add a method analogous to spark mllib clustering KMeansModel computeCost to spark ml clustering KMeansModel  This will be a temp fix until we have proper evaluators defined for clustering,Add computeCost to KMeansModel in spark ml
We should share the SQLTab across sessions,SQLTab should be shared by across sessions
For some use cases  it will be useful to explicitly ban creating multiple root SQLContexts HiveContexts  At here root SQLContext means the first SQLContext that gets created,Introduce a mechanism to ban creating new root SQLContexts in a JVM
In the spirit of SPARK     we should clean up the use of SPARK HOME and  if possible  remove it entirely  We need to look through what this is used for  One use was allowing applications to run different versions of Spark in standalone mode  For instance  someone could submit an application with a custom SPARK HOME and the Worker would launch an Executor using a different path for Spark  This use case is not widely used and maybe should just be removed  The existing constructors that take SPARK HOME for this purpose should be deprecated and we should explain that SPARK HOME is no longer used for this purpose  If there are other legitimate reasons for SPARK HOME  we can keep it around    we need to audit the uses of it,Clean up and clarify use of SPARK HOME
For a variety of reasons  we tagged a bunch of internal classes in the execution package in SQL as DeveloperApi,Remove DeveloperApi annotation from private classes
Also SQLContext newSession,Add getOrCreate for SparkContext SQLContext for Python
We use KCL       in the current master  KCL       added integration with Kinesis Producer Library  KPL  and support auto de aggregation  It would be great to upgrade KCL to the latest stable version  Note that the latest version is       and       restored compatibility with dynamodb streams kinesis adapter  which was broken in        See https   github com awslabs amazon kinesis client release notes    tdas    brkyvz  Please recommend a version for upgrade,Upgrade Kinesis Client Library to the latest stable version
Right now  we stack a new URLClassLoader when a user add a jar through SQL s add jar command  This approach can introduce issues caused by the ordering of added jars when a class of a jar depends on another class of another jar  For example     In this case  when we lookup class B  we will not be able to find class A because Jar  is the parent of Jar,Use a single URLClassLoader for jars added through SQL s  ADD JAR  command
As part of the work to implement SPARK        it would be nice to have the network library efficiently stream data over a connection  Currently all it has is the shuffle data protocol  which is not very efficient for large files  it requires the whole file to be buffered on the receiver side before the receiver can do anything  For large files  that comes at a huge cost in memory  You can chunk large files but that requires the client to ask for each chunk separately  Instead  a similar approach but allowing the data to be processed as it arrives would be a lot more efficient  and make it easier to implement the file server in the referenced bug,Support streaming data using network library
Update the tachyon client dependency from       to        There are no new dependencies added or Spark facing APIs changed,Upgrade Tachyon dependency to
,Mark all Stage ResultStage ShuffleMapStage internal state as private
This is the first cut implementation of  trackStateByKey   new improvement state management method in Spark Streaming  See the epic jira for more details   https   issues apache org jira browse SPARK,Implement trackStateByKey for improved state management
We should add text to DataFrameReader and DataFrameWriter,Python API for text data source
Currently the Write Ahead Log in Spark Streaming flushes data as writes need to be made  S  does not support flushing of data  data is written once the stream is actually closed  In case of failure  the data for the last minute  default rolling interval  will not be properly written  Therefore we need a flag to close the stream after the write  so that we achieve read after write consistency,Flag to close Write Ahead Log after writing
runs  introduces extra complexity and overhead in MLlib s k means implementation  I haven t seen much usage with  runs  not equal to      We can deprecate this method in      and remove or void it in      It helps us simplify the implementation,Deprecate  runs  in k means
We use scala collection mutable BitSet in BroadcastNestedLoopJoin now  We should use Spark s BitSet,Use Spark BitSet in BroadcastNestedLoopJoin
In order to lay the groundwork for proper off heap memory support in SQL   Tungsten  we need to extend our MemoryManager to perform bookkeeping for off heap memory,Add support for off heap memory to MemoryManager
DISTRIBUTE BY allows the user to control the partitioning and ordering of a data set which can be very useful for some applications,Add a DataFrame API that provides functionality similar to HiveQL s DISTRIBUTE BY
Since SPARK       is resolved  MapPartitionWithPrepare is not needed anymore,Remove PrepareRDD
Add Java friendly API for StreamingListener,Add JavaStreamingListener
,Add R API for stddev variance
These two classes should be public  since they are used in public code,Make DataFrameHolder and DatasetHolder public
We should remove methods for variance  stddev  skewness,GroupedData should only keep common first order statistics
Since we have   bytes as number of records in the beginning of a page  then the address can not be zero  so we do not need the bitset,Remove Bitset in BytesToBytesMap
Umbrella ticket to walk through all newly introduced APIs to make sure they are consistent,SQL API audit for Spark
DataFrame has an internal implicit conversion that turns a LogicalPlan into a DataFrame  This has been fairly confusing to a few new contributors  Since it doesn t buy us much  we should just remove that implicit conversion,Remove the internal implicit conversion from LogicalPlan to DataFrame
,Remove implicit conversion from Expression to Column
,Remove the internal implicit conversion from Expression to Column in functions scala
We deprecated  runs  in Spark      SPARK         In      we can either remove  runs  or make it no effect  with warning messages   So we can simplify the implementation  I prefer the latter for better binary compatibility,Make  runs  no effect in k means
Implement Python API for bisecting k means,Python API for bisecting k means
We don t sufficiently test the  path work well,Remove the option to turn off unsafe and codegen
,Remove OpenHashSet for the old aggregate
If it returns Text  we can reuse this in Spark SQL to provide a WholeTextFile data source and directly convert the Text into UTF String without extra string decoding and encoding,WholeTextFileRDD should return Text rather than String
,Add Java API for trackStateByKey
Random Forests have feature importance  but GBT do not  It would be great if we can add feature importance to GBT as well  Perhaps the code in Random Forests can be refactored to apply to both types of ensembles  See https   issues apache org jira browse SPARK,Feature Importance for GBT
RowEncoder doesn t support UserDefinedType now  We should add the support for it. Database application is important to focus in this issue,Add UserDefinedType support to RowEncoder
,consolidate  ExpressionEncoder tuple  and  Encoders tuple
We want to support JSON serialization of vectors in order to support SPARK,JSON serialization of Vectors
Collection functions documented at http   spark apache org docs latest api scala index html org apache spark sql functions  are size    explode    array contains   and sort array    size    explode   are already implemented  array contains   and sort array   are to be implemented,Implement collection functions in SparkR
